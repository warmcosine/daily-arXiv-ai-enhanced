<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]
- [eess.SP](#eess.SP) [Total: 14]
- [cs.LG](#cs.LG) [Total: 142]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [From Prompts to Power: Measuring the Energy Footprint of LLM Inference](https://arxiv.org/abs/2511.05597)
*Francisco Caravaca,Ángel Cuevas,Rubén Cuevas*

Main category: cs.AI

TL;DR: 本文通过大规模测量研究分析了大型语言模型推理阶段的能耗问题，涵盖32,500多个测量数据、21种GPU配置和155种模型架构，开发了预测模型来估算推理能耗并实现为浏览器扩展。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速扩张，推理工作负载的能源需求急剧增长，甚至需要核能供电的数据中心，但系统性的推理能耗分析仍然有限。

Method: 使用vLLM推理引擎进行大规模测量研究，在提示级别量化能耗，分析架构和操作因素对能源需求的影响，并开发预测模型。

Result: 识别了影响推理能耗的关键因素，开发了能够准确估算未见架构和硬件能耗的预测模型，并实现了浏览器扩展工具。

Conclusion: 该研究为理解生成式AI的环境影响提供了系统分析工具，有助于提高对AI能耗问题的认识。

Abstract: The rapid expansion of Large Language Models (LLMs) has introduced
unprecedented energy demands, extending beyond training to large-scale
inference workloads that often dominate total lifecycle consumption. Deploying
these models requires energy-intensive GPU infrastructure, and in some cases
has even prompted plans to power data centers with nuclear energy. Despite this
growing relevance, systematic analyses of inference energy consumption remain
limited. In this work, we present a large-scale measurement-based study
comprising over 32,500 measurements across 21 GPU configurations and 155 model
architectures, from small open-source models to frontier systems. Using the
vLLM inference engine, we quantify energy usage at the prompt level and
identify how architectural and operational factors shape energy demand.
Building on these insights, we develop a predictive model that accurately
estimates inference energy consumption across unseen architectures and
hardware, and implement it as a browser extension to raise awareness of the
environmental impact of generative AI.

</details>


### [2] [CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization](https://arxiv.org/abs/2511.05747)
*Ziqian Bi,Kaijie Chen,Tianyang Wang,Junfeng Hao,Xinyuan Song*

Main category: cs.AI

TL;DR: 提出了一种自适应推理摘要框架，通过语义分割、重要性评分、预算感知动态压缩和连贯性重建来压缩推理轨迹，在保持关键推理步骤的同时显著减少token使用量。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought推理带来的大量推理开销问题，使其能够在资源受限的环境中部署。

Method: 采用语义分割与重要性评分、预算感知动态压缩、连贯性重建的方法来压缩推理轨迹，并使用高斯过程贝叶斯优化模块降低评估成本。

Result: 在7,501个医学考试问题上，相比截断方法在相同token预算下准确率提高40%；在64个模型对上的评估证实了强跨模型可迁移性；评估成本降低84%。

Conclusion: 推理摘要为实现高效CoT迁移提供了实用路径，使得在严格计算约束下能够进行高级推理。

Abstract: Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of
large language models (LLMs) but leads to substantial inference overhead,
limiting deployment in resource-constrained settings. This paper investigates
efficient CoT transfer across models of different scales and architectures
through an adaptive reasoning summarization framework. The proposed method
compresses reasoning traces via semantic segmentation with importance scoring,
budget-aware dynamic compression, and coherence reconstruction, preserving
critical reasoning steps while significantly reducing token usage. Experiments
on 7{,}501 medical examination questions across 10 specialties show up to 40%
higher accuracy than truncation under the same token budgets. Evaluations on 64
model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and
Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian
Process-based Bayesian optimization module reduces evaluation cost by 84% and
reveals a power-law relationship between model size and cross-domain
robustness. These results demonstrate that reasoning summarization provides a
practical path toward efficient CoT transfer, enabling advanced reasoning under
tight computational constraints. Code will be released upon publication.

</details>


### [3] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: 本文通过概率分析和归因方法研究大语言模型中的锚定偏见，发现锚点会改变整个输出分布，不同规模模型对锚定的敏感性不同，归因效应因提示设计而异。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中观察到的认知偏见是表面模仿还是更深层的概率变化，锚定偏见提供了一个关键测试案例。

Method: 使用基于对数概率的行为分析、结构化提示字段的精确Shapley值归因，以及统一的锚定偏见敏感度评分。

Result: 在Gemma-2B、Phi-2和Llama-2-7B中发现了稳健的锚定效应，锚点影响重新加权；较小模型如GPT-2、Falcon-RW-1B和GPT-Neo-125M表现出变异性。

Conclusion: 大语言模型中的锚定偏见是稳健、可测量和可解释的，但在应用领域存在风险，该框架为评估其他认知偏见提供了可复现路径。

Abstract: Large language models (LLMs) are increasingly examined as both behavioral
subjects and decision systems, yet it remains unclear whether observed
cognitive biases reflect surface imitation or deeper probability shifts.
Anchoring bias, a classic human judgment bias, offers a critical test case.
While prior work shows LLMs exhibit anchoring, most evidence relies on
surface-level outputs, leaving internal mechanisms and attributional
contributions unexplored. This paper advances the study of anchoring in LLMs
through three contributions: (1) a log-probability-based behavioral analysis
showing that anchors shift entire output distributions, with controls for
training-data contamination; (2) exact Shapley-value attribution over
structured prompt fields to quantify anchor influence on model
log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score
integrating behavioral and attributional evidence across six open-source
models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and
Llama-2-7B, with attribution signaling that the anchors influence reweighting.
Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability,
suggesting scale may modulate sensitivity. Attributional effects, however, vary
across prompt designs, underscoring fragility in treating LLMs as human
substitutes. The findings demonstrate that anchoring bias in LLMs is robust,
measurable, and interpretable, while highlighting risks in applied domains.
More broadly, the framework bridges behavioral science, LLM safety, and
interpretability, offering a reproducible path for evaluating other cognitive
biases in LLMs.

</details>


### [4] [Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection](https://arxiv.org/abs/2511.05854)
*Zepeng Bao,Shen Zhou,Qiankun Pi,Jianhao Chen,Mayi Xu,Ming Zhong,Yuanyuan Zhu,Tieyun Qian*

Main category: cs.AI

TL;DR: 本文提出了LEAP框架，通过动态策略学习和主动修正机制解决LLM幻觉检测中的策略适应性问题，在三个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具增强的幻觉检测方法存在策略固定、成本高、适应性差的问题，无法应对动态变化的执行环境，导致检测失败。

Method: 提出LEAP框架，将幻觉检测建模为动态策略学习问题：使用教师模型生成轨迹并动态调整策略，通过智能体调优将动态规划能力蒸馏到高效学生模型中，学生模型在执行前采用主动修正机制优化验证策略。

Result: 在三个具有挑战性的基准测试中，LEAP调优的模型优于现有的最先进方法。

Conclusion: LEAP框架成功赋予学生模型动态学习和主动修正能力，有效解决了幻觉检测中的策略适应性问题，实现了高效且自适应的检测性能。

Abstract: Hallucination in large language models (LLMs) remains a critical barrier to
their safe deployment. Existing tool-augmented hallucination detection methods
require pre-defined fixed verification strategies, which are crucial to the
quality and effectiveness of tool calls. Some methods directly employ powerful
closed-source LLMs such as GPT-4 as detectors, which are effective but too
costly. To mitigate the cost issue, some methods adopt the teacher-student
architecture and finetune open-source small models as detectors via agent
tuning. However, these methods are limited by fixed strategies. When faced with
a dynamically changing execution environment, they may lack adaptability and
inappropriately call tools, ultimately leading to detection failure. To address
the problem of insufficient strategy adaptability, we propose the innovative
``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an
efficient student model with the dynamic learning and proactive correction
capabilities of the teacher model. Specifically, our method formulates the
hallucination detection problem as a dynamic strategy learning problem. We
first employ a teacher model to generate trajectories within the dynamic
learning loop and dynamically adjust the strategy based on execution failures.
We then distill this dynamic planning capability into an efficient student
model via agent tuning. Finally, during strategy execution, the student model
adopts a proactive correction mechanism, enabling it to propose, review, and
optimize its own verification strategies before execution. We demonstrate
through experiments on three challenging benchmarks that our LEAP-tuned model
outperforms existing state-of-the-art methods.

</details>


### [5] [An Empirical Study of Reasoning Steps in Thinking Code LLMs](https://arxiv.org/abs/2511.05874)
*Haoran Xue,Gias Uddin,Song Wang*

Main category: cs.AI

TL;DR: 对6个先进思维LLM在代码生成任务中的推理过程进行实证研究，发现推理链质量存在显著差异，任务复杂度是影响推理质量的关键因素，不完整性是主要失败模式。


<details>
  <summary>Details</summary>
Motivation: 虽然思维LLM在代码生成时生成显式中间推理链可能提高透明度和准确性，但这些推理链的质量尚未得到充分研究。

Method: 评估6个先进推理LLM在100个不同难度代码生成任务上的表现，通过步骤计数和冗长度量化推理链结构，进行步骤预算调整控制实验，并开展21人参与的人工评估。

Result: 步骤干预显示针对性增加步骤可提高某些模型/任务的解决率，适度减少步骤通常能保持标准任务的成功率但难以保持困难任务的成功率。稳定性分析表明思维LLM在不同计算努力水平下保持一致的逻辑结构并能自我纠正错误。

Conclusion: 任务复杂度显著影响推理质量，困难问题比标准任务更容易出现不完整性，研究为当前思维LLM在软件工程中的优势和局限性提供了新见解。

Abstract: Thinking Large Language Models (LLMs) generate explicit intermediate
reasoning traces before final answers, potentially improving transparency,
interpretability, and solution accuracy for code generation. However, the
quality of these reasoning chains remains underexplored. We present a
comprehensive empirical study examining the reasoning process and quality of
thinking LLMs for code generation. We evaluate six state-of-the-art reasoning
LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking,
Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code
generation tasks of varying difficulty from BigCodeBench. We quantify
reasoning-chain structure through step counts and verbosity, conduct controlled
step-budget adjustments, and perform a 21-participant human evaluation across
three dimensions: efficiency, logical correctness, and completeness. Our
step-count interventions reveal that targeted step increases can improve
resolution rates for certain models/tasks, while modest reductions often
preserve success on standard tasks, rarely on hard ones. Through systematic
analysis, we develop a reasoning-problematic taxonomy, identifying completeness
as the dominant failure mode. Task complexity significantly impacts reasoning
quality; hard problems are substantially more prone to incompleteness than
standard tasks. Our stability analysis demonstrates that thinking LLMs maintain
consistent logical structures across computational effort levels and can
self-correct previous errors. This study provides new insights into the
strengths and limitations of current thinking LLMs in software engineering.

</details>


### [6] [Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks](https://arxiv.org/abs/2511.05883)
*Hehai Lin,Hui Liu,Shilei Cao,Jing Li,Haoliang Li,Wenya Wang*

Main category: cs.AI

TL;DR: 本文提出三种基于不同粒度理论的样本级模态偏差量化方法，用于自动识别多模态误信息检测中的模态偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态误信息基准存在特定模态偏差，使检测器仅依赖单一模态即可预测。之前的研究在数据集层面量化偏差或手动识别模态与标签间的伪相关性，但缺乏样本级洞察且难以扩展到海量在线信息。

Method: 提出三种偏差量化方法：1）粗粒度的模态效益评估；2）中粒度的信息流量化；3）细粒度的因果分析。在两个流行基准上进行人工评估验证有效性。

Result: 实验发现三个重要发现：1）集成多个视角对可靠自动分析至关重要；2）自动分析易受检测器诱导的波动影响；3）不同视角在模态平衡样本上一致性更高，在偏差样本上分歧更大。

Conclusion: 研究为未来研究提供了潜在方向，强调多视角集成的重要性，并揭示了自动分析在样本级模态偏差识别中的挑战和机遇。

Abstract: Numerous multimodal misinformation benchmarks exhibit bias toward specific
modalities, allowing detectors to make predictions based solely on one
modality. While previous research has quantified bias at the dataset level or
manually identified spurious correlations between modalities and labels, these
approaches lack meaningful insights at the sample level and struggle to scale
to the vast amount of online information. In this paper, we investigate the
design for automated recognition of modality bias at the sample level.
Specifically, we propose three bias quantification methods based on
theories/views of different levels of granularity: 1) a coarse-grained
evaluation of modality benefit; 2) a medium-grained quantification of
information flow; and 3) a fine-grained causality analysis. To verify the
effectiveness, we conduct a human evaluation on two popular benchmarks.
Experimental results reveal three interesting findings that provide potential
direction toward future research: 1)~Ensembling multiple views is crucial for
reliable automated analysis; 2)~Automated analysis is prone to detector-induced
fluctuations; and 3)~Different views produce a higher agreement on
modality-balanced samples but diverge on biased ones.

</details>


### [7] [Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement](https://arxiv.org/abs/2511.05931)
*Hiroaki Hayashi,Bo Pang,Wenting Zhao,Ye Liu,Akash Gokul,Srijan Bansal,Caiming Xiong,Semih Yavuz,Yingbo Zhou*

Main category: cs.AI

TL;DR: SAGE框架使LLM智能体能够从自身任务执行中学习，通过自我抽象提炼关键步骤、依赖关系和约束，从而改进后续执行性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体通常在静态执行框架中运行，缺乏从自身经验和过往执行中学习改进的机制，导致性能受限于初始框架设计和基础LLM能力。

Method: 提出SAGE框架，智能体在初始执行后从具体经验中归纳出简洁的计划抽象，提炼关键步骤、依赖关系和约束，然后将学习到的抽象作为上下文指导反馈，优化智能体策略。

Result: SAGE在不同LLM骨干和智能体架构上均带来一致的性能提升，与GPT-5（高）骨干配合时相比Mini-SWE-Agent基线获得7.2%的相对性能改进，在SWE-Bench Verified基准测试中分别达到73.2%和74%的Pass@1解决率。

Conclusion: SAGE框架通过自我抽象学习机制有效提升了LLM智能体在软件工程任务中的性能，证明了从经验中学习对智能体能力改进的重要性。

Abstract: Large language model (LLM) based agents are increasingly used to tackle
software engineering tasks that require multi-step reasoning and code
modification, demonstrating promising yet limited performance. However, most
existing LLM agents typically operate within static execution frameworks,
lacking a principled mechanism to learn and self-improve from their own
experience and past rollouts. As a result, their performance remains bounded by
the initial framework design and the underlying LLM's capabilities. We propose
Self-Abstraction from Grounded Experience (SAGE), a framework that enables
agents to learn from their own task executions and refine their behavior
through self-abstraction. After an initial rollout, the agent induces a concise
plan abstraction from its grounded experience, distilling key steps,
dependencies, and constraints. This learned abstraction is then fed back as
contextual guidance, refining the agent's policy and supporting more
structured, informed subsequent executions. Empirically, SAGE delivers
consistent performance gains across diverse LLM backbones and agent
architectures. Notably, it yields a 7.2% relative performance improvement over
the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone.
SAGE further achieves strong overall performance on SWE-Bench Verified
benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent
and OpenHands CodeAct agent framework, respectively.

</details>


### [8] [An Epistemic Perspective on Agent Awareness](https://arxiv.org/abs/2511.05977)
*Pavel Naumov,Alexandra Pavlova*

Main category: cs.AI

TL;DR: 该论文将智能体意识视为一种知识形式，打破了现有文献传统。它区分了这种知识的de re和de dicto形式，引入了两种模态来捕捉这些形式，并使用2D语义学版本正式指定其含义。主要技术结果是描述所提出的两种模态与标准"事实知识"模态之间相互作用的一个健全且完备的逻辑系统。


<details>
  <summary>Details</summary>
Motivation: 打破现有文献中将智能体意识视为传统知识形式的惯例，提出将意识作为独立的知识形式来研究，并区分其不同表现形式。

Method: 引入两种模态来捕捉de re和de dicto形式的意识知识，使用2D语义学版本正式指定这些模态的含义，并构建逻辑系统来描述它们之间的相互作用。

Result: 开发了一个健全且完备的逻辑系统，能够描述所提出的两种意识模态与标准事实知识模态之间的相互作用关系。

Conclusion: 通过将智能体意识形式化为知识，并区分其不同形式，成功构建了一个能够准确描述意识知识与事实知识相互关系的逻辑框架。

Abstract: The paper proposes to treat agent awareness as a form of knowledge, breaking
the tradition in the existing literature on awareness. It distinguishes the de
re and de dicto forms of such knowledge. The work introduces two modalities
capturing these forms and formally specifies their meaning using a version of
2D-semantics. The main technical result is a sound and complete logical system
describing the interplay between the two proposed modalities and the standard
"knowledge of the fact" modality.

</details>


### [9] [When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks](https://arxiv.org/abs/2511.06136)
*Stefano Ferraro,Akihiro Nakano,Masahiro Suzuki,Yutaka Matsuo*

Main category: cs.AI

TL;DR: DLPWM是一种无监督的、解耦的物体中心世界模型，能够从像素中学习物体级潜在表示。虽然该模型在重建和预测方面表现良好，但在下游模型控制任务中，其策略性能不如DreamerV3，主要原因是多物体交互过程中的表示漂移问题。


<details>
  <summary>Details</summary>
Motivation: 研究假设明确解耦的物体级表示通过定位任务相关信息，可以增强策略在新特征组合上的性能。

Method: 引入DLPWM，一种完全无监督的解耦物体中心世界模型，直接从像素学习物体级潜在表示。

Result: DLPWM实现了强大的重建和预测性能，包括对多种分布外视觉变化的鲁棒性。但在下游模型控制中，基于DLPWM潜在表示的策略训练效果不如DreamerV3。

Conclusion: 尽管物体中心感知支持鲁棒的视觉建模，但实现稳定控制需要减轻潜在漂移问题。

Abstract: Object-centric world models (OCWM) aim to decompose visual scenes into
object-level representations, providing structured abstractions that could
improve compositional generalization and data efficiency in reinforcement
learning. We hypothesize that explicitly disentangled object-level
representations, by localizing task-relevant information, can enhance policy
performance across novel feature combinations. To test this hypothesis, we
introduce DLPWM, a fully unsupervised, disentangled object-centric world model
that learns object-level latents directly from pixels. DLPWM achieves strong
reconstruction and prediction performance, including robustness to several
out-of-distribution (OOD) visual variations. However, when used for downstream
model-based control, policies trained on DLPWM latents underperform compared to
DreamerV3. Through latent-trajectory analyses, we identify representation shift
during multi-object interactions as a key driver of unstable policy learning.
Our results suggest that, although object-centric perception supports robust
visual modeling, achieving stable control requires mitigating latent drift.

</details>


### [10] [Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles](https://arxiv.org/abs/2511.06160)
*Fatima Jahara,Mark Dredze,Sharon Levy*

Main category: cs.AI

TL;DR: PRIME是一个新的评估框架，使用逻辑网格谜题来系统性地探测LLM在逻辑推理和决策中受到的社会刻板印象影响，重点关注性别刻板印象。


<details>
  <summary>Details</summary>
Motivation: 现有安全护栏能有效抑制明显的偏见输出，但在复杂逻辑推理任务中更微妙的社会偏见形式会逃避当前评估基准，需要新的评估方法来填补这一空白。

Method: 使用逻辑网格谜题自动生成和验证，创建刻板印象、反刻板印象和中性的谜题变体，从共享的谜题结构中生成，允许受控和细粒度的比较。

Result: 模型在解决方案符合刻板印象关联时推理准确性更高，这证明了PRIME在诊断和量化LLM演绎推理中持续存在的社会偏见方面的重要性。

Conclusion: PRIME框架对于在公平性至关重要的场景中诊断和量化LLM演绎推理中的社会偏见具有重要意义。

Abstract: While recent safety guardrails effectively suppress overtly biased outputs,
subtler forms of social bias emerge during complex logical reasoning tasks that
evade current evaluation benchmarks. To fill this gap, we introduce a new
evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model
Evaluation), that uses logic grid puzzles to systematically probe the influence
of social stereotypes on logical reasoning and decision making in LLMs. Our use
of logic puzzles enables automatic generation and verification, as well as
variability in complexity and biased settings. PRIME includes stereotypical,
anti-stereotypical, and neutral puzzle variants generated from a shared puzzle
structure, allowing for controlled and fine-grained comparisons. We evaluate
multiple model families across puzzle sizes and test the effectiveness of
prompt-based mitigation strategies. Focusing our experiments on gender
stereotypes, our findings highlight that models consistently reason more
accurately when solutions align with stereotypical associations. This
demonstrates the significance of PRIME for diagnosing and quantifying social
biases perpetuated in the deductive reasoning of LLMs, where fairness is
critical.

</details>


### [11] [Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.06168)
*Boxuan Wang,Zhuoyun Li,Xinmiao Huang,Xiaowei Huang,Yi Dong*

Main category: cs.AI

TL;DR: 本文提出了一个评估和优化大语言模型推理一致性的框架，通过新的对齐分数指标量化模型生成推理链与人类参考链的语义对齐程度。研究发现2跳推理链对齐分数最高，定义了四种错误类型，并提出了SCOS方法来优化推理一致性。


<details>
  <summary>Details</summary>
Motivation: 评估和优化大语言模型在链式思维推理中的一致性，解决推理链与人类参考链的语义对齐问题。

Method: 提出了对齐分数指标来量化语义对齐，定义了四种错误类型（逻辑断开、主题偏移、冗余推理、因果倒置），并开发了语义一致性优化采样方法（SCOS）来筛选最小化对齐错误的推理链。

Result: 实证发现2跳推理链对齐分数最高，SCOS方法显著提高了对齐分数，在3跳任务中平均提升29.84%。

Conclusion: 通过提出的对齐分数和SCOS方法，能够有效评估和优化大语言模型的推理一致性，特别是在较长推理链任务中表现显著改善。

Abstract: This paper presents a framework for evaluating and optimizing reasoning
consistency in Large Language Models (LLMs) via a new metric, the Alignment
Score, which quantifies the semantic alignment between model-generated
reasoning chains and human-written reference chains in Chain-of-Thought (CoT)
reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest
Alignment Score. To explain this phenomenon, we define four key error types:
logical disconnection, thematic shift, redundant reasoning, and causal
reversal, and show how each contributes to the degradation of the Alignment
Score. Building on this analysis, we further propose Semantic Consistency
Optimization Sampling (SCOS), a method that samples and favors chains with
minimal alignment errors, significantly improving Alignment Scores by an
average of 29.84% with longer reasoning chains, such as in 3-hop tasks.

</details>


### [12] [CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference](https://arxiv.org/abs/2511.06175)
*Kaijie Xu,Fandi Meng,Clark Verbrugge,Simon Lucas*

Main category: cs.AI

TL;DR: CSP4SDG是一个用于社交推理游戏的约束满足框架，通过将游戏事件和对话映射为四类约束，结合硬约束和加权软约束来推断玩家角色，在推理准确性和可解释性方面优于基于LLM的方法。


<details>
  <summary>Details</summary>
Motivation: 社交推理游戏中玩家隐藏身份并故意误导他人，使得角色推断成为核心挑战。准确的角色识别是玩家信念状态的基础，对人和AI性能都至关重要。

Method: 提出CSP4SDG概率约束满足框架，将游戏事件和对话映射到证据、现象、断言和假设四类约束。硬约束修剪不可能角色分配，加权软约束对剩余分配评分，信息增益加权将假设与其在熵减少下的期望值关联。

Result: 在三个公开数据集上的实验显示，CSP4SDG在所有推理场景中都优于基于LLM的基线方法，并且当作为辅助推理工具提供给LLMs时能提升其性能。

Conclusion: 研究表明，基于信息理论的原则性概率推理是社交推理游戏中重量级神经模型的可扩展替代或补充方案。

Abstract: In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players
conceal their identities and deliberately mislead others, making hidden-role
inference a central and demanding task. Accurate role identification, which
forms the basis of an agent's belief state, is therefore the keystone for both
human and AI performance. We introduce CSP4SDG, a probabilistic,
constraint-satisfaction framework that analyses gameplay objectively. Game
events and dialogue are mapped to four linguistically-agnostic constraint
classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune
impossible role assignments, while weighted soft constraints score the
remainder; information-gain weighting links each hypothesis to its expected
value under entropy reduction, and a simple closed-form scoring rule guarantees
that truthful assertions converge to classical hard logic with minimum error.
The resulting posterior over roles is fully interpretable and updates in real
time. Experiments on three public datasets show that CSP4SDG (i) outperforms
LLM-based baselines in every inference scenario, and (ii) boosts LLMs when
supplied as an auxiliary "reasoning tool." Our study validates that principled
probabilistic reasoning with information theory is a scalable alternative-or
complement-to heavy-weight neural models for SDGs.

</details>


### [13] [Dataforge: A Data Agent Platform for Autonomous Data Engineering](https://arxiv.org/abs/2511.06185)
*Xinyuan Wang,Yanjie Fu*

Main category: cs.AI

TL;DR: Data Agent是一个完全自主的表格数据处理系统，利用大语言模型推理和验证，自动执行数据清洗、分层路由和特征级优化，实现从原始数据到AI就绪数据的端到端自动化转换。


<details>
  <summary>Details</summary>
Motivation: 随着AI在材料发现、分子建模和气候科学等领域的应用需求增长，数据准备成为重要但劳动密集的步骤。原始数据需要清洗、标准化和转换才能用于AI，而有效的特征转换和选择对高效训练和推理至关重要。

Method: 利用大语言模型推理和基于验证的方法，Data Agent自动执行数据清洗、分层路由和特征级优化，通过双重反馈循环实现端到端可靠性，无需人工监督。

Result: 展示了首个实用的自主Data Agent实现，演示了如何将原始数据转换为更好的数据。

Conclusion: Data Agent体现了自动、安全和非专家友好的核心原则，解决了可扩展性和专业知识依赖的挑战，为AI应用提供了可靠的数据准备解决方案。

Abstract: The growing demand for AI applications in fields such as materials discovery,
molecular modeling, and climate science has made data preparation an important
but labor-intensive step. Raw data from diverse sources must be cleaned,
normalized, and transformed to become AI-ready, while effective feature
transformation and selection are essential for efficient training and
inference. To address the challenges of scalability and expertise dependence,
we present Data Agent, a fully autonomous system specialized for tabular data.
Leveraging large language model (LLM) reasoning and grounded validation, Data
Agent automatically performs data cleaning, hierarchical routing, and
feature-level optimization through dual feedback loops. It embodies three core
principles: automatic, safe, and non-expert friendly, which ensure end-to-end
reliability without human supervision. This demo showcases the first practical
realization of an autonomous Data Agent, illustrating how raw data can be
transformed "From Data to Better Data."

</details>


### [14] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 提出一种基于数据驱动不确定性分数的轻量级推理步骤验证方法，使用Transformer不确定性量化头来估计LLM推理步骤的不确定性，无需大规模人工标注，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理验证方法如过程奖励模型计算成本高、领域受限且需要大规模标注，需要更轻量、通用的验证方案。

Method: 训练基于Transformer的不确定性量化头，利用冻结LLM的内部状态估计推理步骤的不确定性，标签由更大LLM或原始模型自监督生成。

Result: 在数学、规划和常识问答等多个领域，UHeads性能匹配甚至超越比其大810倍的过程奖励模型，参数量小于1000万。

Conclusion: LLM内部状态编码了其不确定性，可作为可靠的推理验证信号，为可扩展和通用化的自省LLM提供了有前景的方向。

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step
reasoning chains. Previous work has shown that verifying the correctness of
individual reasoning steps can further improve the performance and efficiency
of LLMs on such tasks and enhance solution interpretability. However, existing
verification approaches, such as Process Reward Models (PRMs), are either
computationally expensive, limited to specific domains, or require large-scale
human or model-generated annotations. Thus, we propose a lightweight
alternative for step-level reasoning verification based on data-driven
uncertainty scores. We train transformer-based uncertainty quantification heads
(UHeads) that use the internal states of a frozen LLM to estimate the
uncertainty of its reasoning steps during generation. The approach is fully
automatic: target labels are generated either by another larger LLM (e.g.,
DeepSeek R1) or in a self-supervised manner by the original model itself.
UHeads are both effective and lightweight, containing less than 10M parameters.
Across multiple domains, including mathematics, planning, and general knowledge
question answering, they match or even surpass the performance of PRMs that are
up to 810x larger. Our findings suggest that the internal states of LLMs encode
their uncertainty and can serve as reliable signals for reasoning verification,
offering a promising direction toward scalable and generalizable introspective
LLMs.

</details>


### [15] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: VibeThinker-1.5B是一个1.5B参数的密集模型，通过Spectrum-to-Signal Principle（SSP）框架开发，挑战了通过扩大模型参数来提升能力的传统方法。该模型在仅7800美元的训练成本下，在多个数学基准测试中超越了DeepSeek R1（671B）等大型模型，证明了小模型也能具备强大的推理能力。


<details>
  <summary>Details</summary>
Motivation: 挑战当前认为小模型缺乏强大推理能力的共识，探索通过优化训练方法而非单纯扩大参数规模来提升模型能力，从而大幅降低AI研究的训练和推理成本。

Method: 采用Spectrum-to-Signal Principle（SSP）框架：1）两阶段多样性探索蒸馏（SFT）生成广泛的解决方案谱；2）最大熵引导策略优化（RL）放大正确信号。

Result: 在AIME24（80.3 vs 79.8）、AIME25（74.4 vs 70.0）和HMMT25（50.4 vs 41.7）三个数学基准上超越了400倍大的DeepSeek R1；在LiveCodeBench V6上获得51.1分，优于Magistral Medium的50.3分；相比基础模型（6.7、4.3、0.6）有显著提升。

Conclusion: 小模型通过优化的训练方法可以达到与大型模型相当的推理能力，大幅降低了训练和推理成本，有助于民主化先进AI研究。

Abstract: Challenging the prevailing consensus that small models inherently lack robust
reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense
model developed via our Spectrum-to-Signal Principle (SSP). This challenges the
prevailing approach of scaling model parameters to enhance capabilities, as
seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework
first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a
broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)
to amplify the correct signal. With a total training cost of only $7,800,
VibeThinker-1.5B demonstrates superior reasoning capabilities compared to
closed-source models like Magistral Medium and Claude Opus 4, and performs on
par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses
the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),
AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial
improvement over its base model (6.7, 4.3, and 0.6, respectively). On
LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its
base model's 0.0. These findings demonstrate that small models can achieve
reasoning capabilities comparable to large models, drastically reducing
training and inference costs and thereby democratizing advanced AI research.

</details>


### [16] [ROAR: Robust Accident Recognition and Anticipation for Autonomous Driving](https://arxiv.org/abs/2511.06226)
*Xingcheng Liu,Yanchen Guan,Haicheng Liao,Zhengbing He,Zhenning Li*

Main category: cs.AI

TL;DR: 本文提出ROAR方法，通过结合离散小波变换、自适应目标感知模块和动态焦点损失，解决自动驾驶车辆事故预测中的传感器故障、环境干扰和数据不平衡等问题，在多个数据集上优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有事故预测方法通常假设理想条件，忽略了传感器故障、环境干扰、数据不完整以及不同车辆类型间驾驶员行为和事故率的显著差异，这些因素会严重影响预测准确性。

Method: ROAR方法结合离散小波变换(DWT)从噪声和不完整数据中提取特征，使用自适应目标感知模块关注高风险车辆并建模交通参与者的时空关系，采用动态焦点损失缓解正负样本类别不平衡问题。

Result: 在Dashcam Accident Dataset (DAD)、Car Crash Dataset (CCD)和AnAn Accident Detection (A3D)三个数据集上的评估显示，ROAR在平均精度(AP)和平均事故时间(mTTA)等关键指标上持续优于现有基线方法。

Conclusion: ROAR模型在真实世界条件下表现出强大的鲁棒性，特别是在处理传感器退化、环境噪声和不平衡数据分布方面，为复杂交通环境中的可靠准确事故预测提供了有前景的解决方案。

Abstract: Accurate accident anticipation is essential for enhancing the safety of
autonomous vehicles (AVs). However, existing methods often assume ideal
conditions, overlooking challenges such as sensor failures, environmental
disturbances, and data imperfections, which can significantly degrade
prediction accuracy. Additionally, previous models have not adequately
addressed the considerable variability in driver behavior and accident rates
across different vehicle types. To overcome these limitations, this study
introduces ROAR, a novel approach for accident detection and prediction. ROAR
combines Discrete Wavelet Transform (DWT), a self adaptive object aware module,
and dynamic focal loss to tackle these challenges. The DWT effectively extracts
features from noisy and incomplete data, while the object aware module improves
accident prediction by focusing on high-risk vehicles and modeling the spatial
temporal relationships among traffic agents. Moreover, dynamic focal loss
mitigates the impact of class imbalance between positive and negative samples.
Evaluated on three widely used datasets, Dashcam Accident Dataset (DAD), Car
Crash Dataset (CCD), and AnAn Accident Detection (A3D), our model consistently
outperforms existing baselines in key metrics such as Average Precision (AP)
and mean Time to Accident (mTTA). These results demonstrate the model's
robustness in real-world conditions, particularly in handling sensor
degradation, environmental noise, and imbalanced data distributions. This work
offers a promising solution for reliable and accurate accident anticipation in
complex traffic environments.

</details>


### [17] [Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents](https://arxiv.org/abs/2511.06292)
*Yaoning Yu,Kaimin Chang,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于数据增强优化的自改进提示框架，通过生成合成金融表格和文档片段、验证其正确性和鲁棒性，然后根据结果更新提示，在无需外部标注的情况下持续提升金融推理任务的提示准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常在固定的金融文本或表格数据集上调整提示，这限制了它们适应新问题类型或文档结构的能力，或者需要昂贵的手动标注/策划数据集来构建提示。

Method: 结合合成数据生成器、验证器和提示优化器的闭环框架，生成器产生暴露当前提示弱点的示例，验证器检查生成示例的有效性和鲁棒性，优化器根据结果逐步优化提示。

Result: 在DocMath-Eval基准测试上的评估表明，该系统在准确性和鲁棒性方面均优于标准提示方法。

Conclusion: 将合成数据生成纳入金融应用的提示学习中具有重要价值，能够在不依赖外部标签的情况下持续改进提示性能。

Abstract: Financial documents like earning reports or balance sheets often involve long
tables and multi-page reports. Large language models have become a new tool to
help numerical reasoning and understanding these documents. However, prompt
quality can have a major effect on how well LLMs perform these financial
reasoning tasks. Most current methods tune prompts on fixed datasets of
financial text or tabular data, which limits their ability to adapt to new
question types or document structures, or they involve costly and manually
labeled/curated dataset to help build the prompts. We introduce a
self-improving prompt framework driven by data-augmented optimization. In this
closed-loop process, we generate synthetic financial tables and document
excerpts, verify their correctness and robustness, and then update the prompt
based on the results. Specifically, our framework combines a synthetic data
generator with verifiers and a prompt optimizer, where the generator produces
new examples that exposes weaknesses in the current prompt, the verifiers check
the validity and robustness of the produced examples, and the optimizer
incrementally refines the prompt in response. By iterating these steps in a
feedback cycle, our method steadily improves prompt accuracy on financial
reasoning tasks without needing external labels. Evaluation on DocMath-Eval
benchmark demonstrates that our system achieves higher performance in both
accuracy and robustness than standard prompt methods, underscoring the value of
incorporating synthetic data generation into prompt learning for financial
applications.

</details>


### [18] [Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems](https://arxiv.org/abs/2511.06301)
*Azanzi Jiomekong,Jean Bikim,Patricia Negoue,Joyce Chin*

Main category: cs.AI

TL;DR: 该论文介绍了Secu-Table数据集，这是一个包含1500多个表格和15,000多个实体的安全领域语义表格解释数据集，基于CVE和CWE数据源构建，使用Wikidata和SEPSES CSKG进行标注。


<details>
  <summary>Details</summary>
Motivation: 在安全领域，用于评估语义表格解释系统的公开表格数据集缺乏，这限制了基于大语言模型的STI系统在特定领域中的评估和发展。

Method: 从CVE和CWE数据源提取安全数据构建表格数据集，使用Wikidata和SEPSES CSKG知识图谱进行实体标注，并公开所有代码。

Result: 创建了包含1500多个表格和15,000多个实体的Secu-Table数据集，作为SemTab挑战赛的一部分，并进行了初步评估，使用Falcon3-7b-instruct、Mistral-7B-Instruct和GPT-4o mini作为基线模型。

Conclusion: Secu-Table数据集填补了安全领域语义表格解释评估的数据空白，为研究社区提供了重要的资源，有助于推动STI系统在安全领域的发展。

Abstract: Evaluating semantic tables interpretation (STI) systems, (particularly, those
based on Large Language Models- LLMs) especially in domain-specific contexts
such as the security domain, depends heavily on the dataset. However, in the
security domain, tabular datasets for state-of-the-art are not publicly
available. In this paper, we introduce Secu-Table dataset, composed of more
than 1500 tables with more than 15k entities constructed using security data
extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness
Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic
Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES
CSKG). Along with the dataset, all the code is publicly released. This dataset
is made available to the research community in the context of the SemTab
challenge on Tabular to Knowledge Graph Matching. This challenge aims to
evaluate the performance of several STI based on open source LLMs. Preliminary
evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and
Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source
LLM.

</details>


### [19] [The Station: An Open-World Environment for AI-Driven Discovery](https://arxiv.org/abs/2511.06309)
*Stephen Chung,Wenyu Du*

Main category: cs.AI

TL;DR: STATION是一个开放世界的多智能体环境，模拟微型科学生态系统，智能体可以自主进行科学研究活动，包括阅读论文、提出假设、提交代码、执行分析和发表结果，无需中央协调。


<details>
  <summary>Details</summary>
Motivation: 构建一个开放世界的科学发现环境，让AI智能体能够自主进行科学研究，超越传统的刚性优化范式，通过涌现行为推动自主科学发现。

Method: 利用扩展的上下文窗口，智能体在STATION环境中进行长期科学探索，自由选择行动并发展自己的叙事，没有中央系统协调。

Result: 实验表明，STATION中的AI智能体在数学、计算生物学和机器学习等广泛基准测试中达到新的最先进性能，显著超越AlphaEvolve在圆堆积问题上的表现，并涌现出新的方法如密度自适应scRNA-seq批次整合算法。

Conclusion: STATION代表了迈向由开放世界环境中涌现行为驱动的自主科学发现的第一步，标志着超越刚性优化的新范式。

Abstract: We introduce the STATION, an open-world multi-agent environment that models a
miniature scientific ecosystem. Leveraging their extended context windows,
agents in the Station can engage in long scientific journeys that include
reading papers from peers, formulating hypotheses, submitting code, performing
analyses, and publishing results. Importantly, there is no centralized system
coordinating their activities - agents are free to choose their own actions and
develop their own narratives within the Station. Experiments demonstrate that
AI agents in the Station achieve new state-of-the-art performance on a wide
range of benchmarks, spanning from mathematics to computational biology to
machine learning, notably surpassing AlphaEvolve in circle packing. A rich
tapestry of narratives emerges as agents pursue independent research, interact
with peers, and build upon a cumulative history. From these emergent
narratives, novel methods arise organically, such as a new density-adaptive
algorithm for scRNA-seq batch integration. The Station marks a first step
towards autonomous scientific discovery driven by emergent behavior in an
open-world environment, representing a new paradigm that moves beyond rigid
optimization.

</details>


### [20] [ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning](https://arxiv.org/abs/2511.06316)
*MD Thamed Bin Zaman Chowdhury,Moazzem Hossain*

Main category: cs.AI

TL;DR: ALIGN是一个视觉语言框架，通过模拟人类空间推理从文本和地图线索推断交通事故坐标，解决了多语言和非结构化新闻环境中传统地理编码工具性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家缺乏准确的位置特定交通事故数据，现有基于文本的地理编码工具在多语言和非结构化新闻环境中表现不佳，不完整的地点描述和混合的孟加拉语-英语脚本模糊了空间上下文。

Method: ALIGN将大型语言和视觉语言模型集成到多阶段管道中，执行光学字符识别、语言推理和基于网格的空间扫描进行地图级验证，系统评估每个预测位置与上下文和视觉证据的一致性。

Result: 应用于孟加拉语新闻数据时，ALIGN相比传统地理解析方法表现出持续改进，准确识别了地区和子地区级别的碰撞地点。

Conclusion: 该框架为数据稀缺地区的自动碰撞映射建立了高精度基础，支持基于证据的道路安全政策制定，并促进多模态人工智能在交通分析中的更广泛整合。

Abstract: Reliable geospatial information on road accidents is vital for safety
analysis and infrastructure planning, yet most low- and middle-income countries
continue to face a critical shortage of accurate, location-specific crash data.
Existing text-based geocoding tools perform poorly in multilingual and
unstructured news environments, where incomplete place descriptions and mixed
Bangla-English scripts obscure spatial context. To address these limitations,
this study introduces ALIGN (Accident Location Inference through Geo-Spatial
Neural Reasoning)- a vision-language framework that emulates human spatial
reasoning to infer accident coordinates directly from textual and map-based
cues. ALIGN integrates large language and vision-language models within a
multi-stage pipeline that performs optical character recognition, linguistic
reasoning, and map-level verification through grid-based spatial scanning. The
framework systematically evaluates each predicted location against contextual
and visual evidence, ensuring interpretable, fine-grained geolocation outcomes
without requiring model retraining. Applied to Bangla-language news data, ALIGN
demonstrates consistent improvements over traditional geoparsing methods,
accurately identifying district and sub-district-level crash sites. Beyond its
technical contribution, the framework establishes a high accuracy foundation
for automated crash mapping in data-scarce regions, supporting evidence-driven
road-safety policymaking and the broader integration of multimodal artificial
intelligence in transportation analytics. The code for this paper is
open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN

</details>


### [21] [LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation](https://arxiv.org/abs/2511.06346)
*Liya Zhu,Peizhuang Cong,Aowei Ji,Wenya Wu,Jiani Hou,Chunjie Wu,Xiang Gao,Jingkai Liu,Zhou Huan,Xuelei Sun,Yang Yang,Jianpeng Jiao,Liang Hu,Xinjie Chen,Jiashuo Liu,Jingzhe Ding,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: LPFQA是一个基于长尾知识的基准测试，从20个学术和工业领域的专业论坛中提取，包含502个基于实际专业知识的任务，旨在更真实地评估大语言模型的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试往往关注简化任务或人工场景，忽略了长尾知识和真实世界应用的复杂性，难以准确评估大语言模型的真实能力。

Method: 从20个学术和工业领域的专业论坛中提取长尾知识，构建包含502个任务的基准测试，引入细粒度评估维度、分层难度结构、真实专业场景建模和跨学科知识整合。

Result: 在12个主流大语言模型上评估LPFQA，观察到显著的性能差异，特别是在专业推理任务上表现明显。

Conclusion: LPFQA为推进大语言模型评估和指导未来模型开发提供了一个稳健、真实且具有区分度的基准测试。

Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question
answering, and professional applications; however, their true capabilities
remain difficult to evaluate using existing benchmarks. Current datasets often
focus on simplified tasks or artificial scenarios, overlooking long-tail
knowledge and the complexities of real-world applications. To bridge this gap,
we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic
professional forums across 20 academic and industrial fields, covering 502
tasks grounded in practical expertise. LPFQA introduces four key innovations:
fine-grained evaluation dimensions that target knowledge depth, reasoning,
terminology comprehension, and contextual analysis; a hierarchical difficulty
structure that ensures semantic clarity and unique answers; authentic
professional scenario modeling with realistic user personas; and
interdisciplinary knowledge integration across diverse domains. We evaluated 12
mainstream LLMs on LPFQA and observed significant performance disparities,
especially in specialized reasoning tasks. LPFQA provides a robust, authentic,
and discriminative benchmark for advancing LLM evaluation and guiding future
model development.

</details>


### [22] [What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models](https://arxiv.org/abs/2511.06380)
*Chen He,Xun Jiang,Lei Wang,Hao Yang,Chong Peng,Peng Yan,Fumin Shen,Xing Xu*

Main category: cs.AI

TL;DR: 论文提出AEPO方法解决LLM在复杂领域推理中的"回响反思"问题，通过信息过滤和自适应熵优化提升反思质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在数学推理中表现良好，但在涉及复杂领域知识的任务中，LLM在反思阶段无法产生新见解，而是机械重复早期推理步骤，这种现象被称为"回响反思"。

Method: 提出自适应熵策略优化(AEPO)框架，包含两个主要组件：反思感知信息过滤(量化认知信息流，防止早期错误认知影响最终答案)和自适应熵优化(动态平衡不同推理阶段的探索与利用)。

Result: 大量实验表明，AEPO在多样化基准测试中始终优于主流强化学习基线方法，达到最先进性能。

Conclusion: AEPO通过控制信息流和促进认知多样性，有效解决了LLM在复杂领域推理中的反思质量问题，为提升LLM的认知能力提供了新思路。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of reasoning tasks. Recent methods have further improved LLM
performance in complex mathematical reasoning. However, when extending these
methods beyond the domain of mathematical reasoning to tasks involving complex
domain-specific knowledge, we observe a consistent failure of LLMs to generate
novel insights during the reflection stage. Instead of conducting genuine
cognitive refinement, the model tends to mechanically reiterate earlier
reasoning steps without introducing new information or perspectives, a
phenomenon referred to as "Echo Reflection". We attribute this behavior to two
key defects: (1) Uncontrollable information flow during response generation,
which allows premature intermediate thoughts to propagate unchecked and distort
final decisions; (2) Insufficient exploration of internal knowledge during
reflection, leading to repeating earlier findings rather than generating new
cognitive insights. Building on these findings, we proposed a novel
reinforcement learning method termed Adaptive Entropy Policy Optimization
(AEPO). Specifically, the AEPO framework consists of two major components: (1)
Reflection-aware Information Filtration, which quantifies the cognitive
information flow and prevents the final answer from being affected by earlier
bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically
balances exploration and exploitation across different reasoning stages,
promoting both reflective diversity and answer correctness. Extensive
experiments demonstrate that AEPO consistently achieves state-of-the-art
performance over mainstream reinforcement learning baselines across diverse
benchmarks.

</details>


### [23] [SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization](https://arxiv.org/abs/2511.06411)
*Zhi Zheng,Wee Sun Lee*

Main category: cs.AI

TL;DR: 本文提出了SofT-GRPO算法，通过将Gumbel噪声注入logits、使用Gumbel-Softmax技术和重参数化技巧，成功实现了软思维推理模式下的LLM强化学习优化。


<details>
  <summary>Details</summary>
Motivation: 软思维推理模式在某些场景下优于传统的离散token链式思维推理，但将其与强化学习结合存在挑战，导致之前的方法表现不如离散token GRPO。

Method: 提出SofT-GRPO算法：注入Gumbel噪声到logits，使用Gumbel-Softmax避免软思维token超出预训练嵌入空间，并在策略梯度中利用重参数化技巧。

Result: 在1.5B到7B参数的LLM上实验，SofT-GRPO使软思维LLM在Pass@1上略优于离散token GRPO（平均准确率+0.13%），在Pass@32上显著提升（平均准确率+2.19%）。

Conclusion: SofT-GRPO成功解锁了软思维推理的潜力，为软思维模式与强化学习的结合提供了有效解决方案。

Abstract: The soft-thinking paradigm for Large Language Model (LLM) reasoning can
outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in
some scenarios, underscoring its research and application value. However, while
the discrete-token CoT reasoning pattern can be reinforced through policy
optimization algorithms such as group relative policy optimization (GRPO),
extending the soft-thinking pattern with Reinforcement Learning (RL) remains
challenging. This difficulty stems from the complexities of injecting
stochasticity into soft-thinking tokens and updating soft-thinking policies
accordingly. As a result, previous attempts to combine soft-thinking with GRPO
typically underperform their discrete-token GRPO counterparts. To fully unlock
the potential of soft-thinking, this paper presents a novel policy optimization
algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning
pattern. SofT-GRPO injects the Gumbel noise into logits, employs the
Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained
embedding space, and leverages the reparameterization trick in policy gradient.
We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and
results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly
outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while
exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes
and weights are available on https://github.com/zz1358m/SofT-GRPO-master

</details>


### [24] [AUTO-Explorer: Automated Data Collection for GUI Agent](https://arxiv.org/abs/2511.06417)
*Xiangwu Guo,Difei Gao,Mike Zheng Shou*

Main category: cs.AI

TL;DR: 提出Auto-Explorer自动化GUI数据收集方法，解决现有方法难以应用于桌面软件和新网站的问题，通过探索机制高效收集数据并建立UIXplore基准评估探索质量。


<details>
  <summary>Details</summary>
Motivation: 现有GUI数据收集方法主要依赖Common Crawl网页HTML，难以应用于桌面软件和未收录的新网站，需要低成本自动化解决方案来快速适应个性化场景。

Method: 提出Auto-Explorer方法，包含简单有效的探索机制，能自主解析和探索GUI环境高效收集数据，并建立UIXplore基准评估探索质量。

Result: 实验证明Auto-Explorer性能优越，能快速提升多模态大语言模型在已探索软件中的能力。

Conclusion: Auto-Explorer提供了一种低成本高效的GUI数据收集解决方案，能有效解决现有方法的局限性，提升模型在新环境中的适应能力。

Abstract: Recent advancements in GUI agents have significantly expanded their ability
to interpret natural language commands to manage software interfaces. However,
acquiring GUI data remains a significant challenge. Existing methods often
involve designing automated agents that browse URLs from the Common Crawl,
using webpage HTML to collect screenshots and corresponding annotations,
including the names and bounding boxes of UI elements. However, this method is
difficult to apply to desktop software or some newly launched websites not
included in the Common Crawl. While we expect the model to possess strong
generalization capabilities to handle this, it is still crucial for
personalized scenarios that require rapid and perfect adaptation to new
software or websites. To address this, we propose an automated data collection
method with minimal annotation costs, named Auto-Explorer. It incorporates a
simple yet effective exploration mechanism that autonomously parses and
explores GUI environments, gathering data efficiently. Additionally, to assess
the quality of exploration, we have developed the UIXplore benchmark. This
benchmark creates environments for explorer agents to discover and save
software states. Using the data gathered, we fine-tune a multimodal large
language model (MLLM) and establish a GUI element grounding testing set to
evaluate the effectiveness of the exploration strategies. Our experiments
demonstrate the superior performance of Auto-Explorer, showing that our method
can quickly enhance the capabilities of an MLLM in explored software.

</details>


### [25] [MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models](https://arxiv.org/abs/2511.06419)
*Jingyu Hu,Shu Yang,Xilin Gong,Hongming Wang,Weiru Liu,Di Wang*

Main category: cs.AI

TL;DR: MONICA是一个监控引导的校准框架，通过在推理步骤级别实时监控和缓解大型推理模型的奉承行为，无需模型生成完整答案即可动态抑制奉承倾向。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在奉承行为，倾向于同意用户的错误信念和追随错误信息，这损害了模型可靠性并带来社会风险。现有方法主要基于最终答案进行判断和修正，无法理解奉承行为在推理过程中的发展。

Method: 提出MONICA框架，包含奉承监控器和校准器。监控器在响应生成过程中实时监控奉承漂移分数，校准器在分数超过预定阈值时动态抑制奉承行为。

Result: 在12个数据集和3个大型推理模型上的广泛实验表明，该方法有效减少了中间推理步骤和最终答案中的奉承行为，实现了稳健的性能提升。

Conclusion: MONICA框架能够有效监控和缓解大型推理模型在推理过程中的奉承行为，提高模型的可靠性和独立性。

Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models
tend to agree with users' incorrect beliefs and follow misinformation rather
than maintain independent reasoning. This behavior undermines model reliability
and poses societal risks. Mitigating LRM sycophancy requires monitoring how
this sycophancy emerges during the reasoning trajectory; however, current
methods mainly focus on judging based on final answers and correcting them,
without understanding how sycophancy develops during reasoning processes. To
address this limitation, we propose MONICA, a novel Monitor-guided Calibration
framework that monitors and mitigates sycophancy during model inference at the
level of reasoning steps, without requiring the model to finish generating its
complete answer. MONICA integrates a sycophantic monitor that provides
real-time monitoring of sycophantic drift scores during response generation
with a calibrator that dynamically suppresses sycophantic behavior when scores
exceed predefined thresholds. Extensive experiments across 12 datasets and 3
LRMs demonstrate that our method effectively reduces sycophantic behavior in
both intermediate reasoning steps and final answers, yielding robust
performance improvements.

</details>


### [26] [GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets](https://arxiv.org/abs/2511.06471)
*Jingtao Tang,Hang Ma*

Main category: cs.AI

TL;DR: GHOST是一个分层框架，用于解决图凸集旅行商问题(GCS-TSP)，通过结合组合路径搜索和凸轨迹优化，在保证最优性的同时显著提升求解效率。


<details>
  <summary>Details</summary>
Motivation: 传统TSP方法无法处理GCS-TSP问题，因为边成本取决于通过凸区域的具体轨迹，而非固定值。需要开发新的算法来处理这种复杂的轨迹规划问题。

Method: GHOST采用分层框架，在GCS诱导的完全图上系统探索路径，使用新颖的抽象路径展开算法计算可接受下界，指导高层(路径)和低层(可行GCS路径)的最佳优先搜索。

Result: 实验表明GHOST比统一混合整数凸规划基准快几个数量级，并能独特处理涉及高阶连续性约束和不完整GCS的复杂轨迹规划问题。

Conclusion: GHOST为GCS-TSP问题提供了最优解保证，同时提供了有界次优变体用于时间关键场景，在复杂轨迹规划中表现出色。

Abstract: We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP)
defined over a Graph of Convex Sets (GCS) -- a powerful representation for
trajectory planning that decomposes the configuration space into convex regions
connected by a sparse graph. In this setting, edge costs are not fixed but
depend on the specific trajectory selected through each convex region, making
classical TSP methods inapplicable. We introduce GHOST, a hierarchical
framework that optimally solves the GCS-TSP by combining combinatorial tour
search with convex trajectory optimization. GHOST systematically explores tours
on a complete graph induced by the GCS, using a novel abstract-path-unfolding
algorithm to compute admissible lower bounds that guide best-first search at
both the high level (over tours) and the low level (over feasible GCS paths
realizing the tour). These bounds provide strong pruning power, enabling
efficient search while avoiding unnecessary convex optimization calls. We prove
that GHOST guarantees optimality and present a bounded-suboptimal variant for
time-critical scenarios. Experiments show that GHOST is orders-of-magnitude
faster than unified mixed-integer convex programming baselines for simple cases
and uniquely handles complex trajectory planning problems involving high-order
continuity constraints and an incomplete GCS.

</details>


### [27] [FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis](https://arxiv.org/abs/2511.06522)
*Jan Ondras,Marek Šuppa*

Main category: cs.AI

TL;DR: FractalBench是一个评估多模态AI系统从图像中合成分形程序能力的基准测试，通过让模型生成可执行的Python代码来重现分形图案，测试视觉感知与数学抽象的结合能力。


<details>
  <summary>Details</summary>
Motivation: 研究多模态AI系统是否具备从视觉模式中抽象出符号规则的能力，即从有限推断无限，这是数学推理的核心能力。

Method: 使用FractalBench基准测试，评估4个领先的多模态大语言模型在12个经典分形上的表现。模型需要从分形图像生成可执行的Python代码来重现分形图案。

Result: 结果显示76%的模型能生成语法有效的代码，但只有4%能正确捕捉数学结构。模型在处理几何变换时表现较好（如Koch曲线：17-21%），但在处理分支递归时表现很差（如树形分形：<2%）。

Conclusion: 多模态AI系统在数学抽象能力上存在根本性差距，FractalBench提供了一个抗污染的诊断工具来评估视觉-数学推理能力。

Abstract: Mathematical reasoning requires abstracting symbolic rules from visual
patterns -- inferring the infinite from the finite. We investigate whether
multimodal AI systems possess this capability through FractalBench, a benchmark
evaluating fractal program synthesis from images. Fractals provide ideal test
cases: Iterated Function Systems with only a few contraction maps generate
complex self-similar patterns through simple recursive rules, requiring models
to bridge visual perception with mathematical abstraction. We evaluate four
leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL
-- on 12 canonical fractals. Models must generate executable Python code
reproducing the fractal, enabling objective evaluation. Results reveal a
striking disconnect: 76% generate syntactically valid code but only 4% capture
mathematical structure. Success varies systematically -- models handle
geometric transformations (Koch curves: 17-21%) but fail at branching recursion
(trees: <2%), revealing fundamental gaps in mathematical abstraction.
FractalBench provides a contamination-resistant diagnostic for
visual-mathematical reasoning and is available at
https://github.com/NaiveNeuron/FractalBench

</details>


### [28] [GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2511.06618)
*Moriya Dechtiar,Daniel Martin Katz,Mari Sundaresan,Sylvain Jaume,Hongming Wang*

Main category: cs.AI

TL;DR: 本文提出了一种将法律合同转换为结构化语义图的新框架，使用基于强化学习的LLM方法自动提取合同中的实体和关系，实现合同审查的自动化和可视化分析。


<details>
  <summary>Details</summary>
Motivation: 合同是复杂的文档，具有详细的形式结构、显性和隐性依赖关系以及丰富的语义内容。手动起草和审查合同既繁琐又容易出错，因此需要简化和自动化合同审查分析任务。

Method: 提出GRAPH-GRPO-LEX方法，使用基于强化学习的大型语言模型框架进行合同分割和实体关系提取，结合组相对策略优化（GRPO）和精心设计的图指标奖励函数。

Result: 该方法能够自动识别条款间的直接关系，甚至发现隐藏的依赖关系，通过门控GRPO方法显示出强大的学习信号，将合同分析从线性手动阅读过程转变为易于可视化的图结构。

Conclusion: 该框架为合同分析提供了动态分析能力，为类似软件工程中代码检查的合同检查奠定了基础，实现了合同审查的自动化和可视化。

Abstract: Contracts are complex documents featuring detailed formal structures,
explicit and implicit dependencies and rich semantic content. Given these
document properties, contract drafting and manual examination of contracts have
proven to be both arduous and susceptible to errors. This work aims to simplify
and automate the task of contract review and analysis using a novel framework
for transforming legal contracts into structured semantic graphs, enabling
computational analysis and data-driven insights. We introduce a detailed
ontology mapping core legal contract elements to their graph-theoretic
equivalents of nodes and edges. We then present a reinforcement learning based
Large Language Model (LLM) framework for segmentation and extraction of
entities and relationships from contracts. Our method, GRAPH-GRPO-LEX,
incorporates both LLMs and reinforcement learning with group relative policy
optimization (GRPO). By applying a carefully drafted reward function of graph
metrics, we demonstrate the ability to automatically identify direct
relationships between clauses, and even uncover hidden dependencies. Our
introduction of the gated GRPO approach shows a strong learning signal and can
move contract analysis from a linear, manual reading process to an easily
visualized graph. This allows for a more dynamic analysis, including building
the groundwork for contract linting similar to what is now practiced in
software engineering.

</details>


### [29] [MathSE: Improving Multimodal Mathematical Reasoning via Self-Evolving Iterative Reflection and Reward-Guided Fine-Tuning](https://arxiv.org/abs/2511.06805)
*Jinhao Chen,Zhen Yang,Jianxin Shi,Tianyu Wo,Jie Tang*

Main category: cs.AI

TL;DR: 本文提出了MathSE框架，通过推理-反思-奖励反馈的迭代循环来增强多模态大语言模型的数学推理能力，相比传统一次性微调方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在复杂数学推理任务中存在局限性，传统方法依赖静态教师模型蒸馏的数据集，缺乏适应性和迭代深度，限制了模型的泛化能力。

Method: 提出MathSE框架，通过迭代微调结合推理路径和专门的Outcome Reward Model反馈，实现模型的自我进化。

Result: 在多个挑战性基准测试中表现出色，特别是在MathVL-test上超越了领先的开源多模态数学推理模型QVQ。

Conclusion: MathSE框架通过迭代优化机制有效提升了MLLMs的数学推理能力，为复杂推理任务提供了新的解决方案。

Abstract: Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities in vision-language answering tasks. Despite their strengths, these
models often encounter challenges in achieving complex reasoning tasks such as
mathematical problem-solving. Previous works have focused on fine-tuning on
specialized mathematical datasets. However, these datasets are typically
distilled directly from teacher models, which capture only static reasoning
patterns and leaving substantial gaps compared to student models. This reliance
on fixed teacher-derived datasets not only restricts the model's ability to
adapt to novel or more intricate questions that extend beyond the confines of
the training data, but also lacks the iterative depth needed for robust
generalization. To overcome these limitations, we propose \textbf{\method}, a
\textbf{Math}ematical \textbf{S}elf-\textbf{E}volving framework for MLLMs. In
contrast to traditional one-shot fine-tuning paradigms, \method iteratively
refines the model through cycles of inference, reflection, and reward-based
feedback. Specifically, we leverage iterative fine-tuning by incorporating
correct reasoning paths derived from previous-stage inference and integrating
reflections from a specialized Outcome Reward Model (ORM). To verify the
effectiveness of \method, we evaluate it on a suite of challenging benchmarks,
demonstrating significant performance gains over backbone models. Notably, our
experimental results on MathVL-test surpass the leading open-source multimodal
mathematical reasoning model QVQ. Our code and models are available at
\texttt{https://zheny2751\allowbreak-dotcom.github.io/\allowbreak
MathSE.github.io/}.

</details>


### [30] [Proceedings of the 2025 XCSP3 Competition](https://arxiv.org/abs/2511.06918)
*Gilles Audemard,Christophe Lecoutre,Emmanuel Lonca*

Main category: cs.AI

TL;DR: 2025年XCSP3约束求解器竞赛的会议记录文档


<details>
  <summary>Details</summary>
Motivation: 展示和记录2025年XCSP3约束求解器竞赛的结果，该竞赛在CP'25国际约束编程会议上进行

Method: 组织约束求解器竞赛，收集和分析各求解器的性能结果

Result: 记录了竞赛的详细结果和排名

Conclusion: 该文档为约束编程社区提供了最新的约束求解器性能评估和比较

Abstract: This document represents the proceedings of the 2025 XCSP3 Competition. The
results of this competition of constraint solvers were presented at CP'25 (31st
International Conference on Principles and Practice of Constraint Programming).

</details>


### [31] [Do LLMs Feel? Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning](https://arxiv.org/abs/2511.07061)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao,Yu Liu*

Main category: cs.AI

TL;DR: PRC-Emo是一个新的对话情感识别框架，结合提示工程、演示检索和课程学习，通过设计情感敏感提示模板、构建专用演示检索库和引入课程学习策略，在IEMOCAP和MELD数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在对话情感识别方面显示出潜力，但其捕捉显性和隐性情感之间内在联系的能力仍然有限，需要开发更有效的训练框架来提升模型对对话中情感的理解能力。

Method: 提出PRC-Emo框架：1）基于显性和隐性情感线索设计情感敏感提示模板；2）构建首个专用演示检索库，包含训练样本和LLM生成的高质量对话示例；3）在LoRA微调中引入课程学习策略，根据说话者间情感转移的加权值分配难度级别。

Result: 在IEMOCAP和MELD两个基准数据集上的实验结果表明，该方法实现了新的最先进性能，证明了其在改进基于LLM的情感理解方面的有效性和泛化能力。

Conclusion: PRC-Emo框架通过整合提示工程、演示检索和课程学习，显著提升了LLM在对话情感识别任务中的表现，为改进基于LLM的情感理解提供了有效解决方案。

Abstract: Emotion Recognition in Conversation (ERC) is a crucial task for understanding
human emotions and enabling natural human-computer interaction. Although Large
Language Models (LLMs) have recently shown great potential in this field, their
ability to capture the intrinsic connections between explicit and implicit
emotions remains limited. We propose a novel ERC training framework, PRC-Emo,
which integrates Prompt engineering, demonstration Retrieval, and Curriculum
learning, with the goal of exploring whether LLMs can effectively perceive
emotions in conversational contexts. Specifically, we design emotion-sensitive
prompt templates based on both explicit and implicit emotional cues to better
guide the model in understanding the speaker's psychological states. We
construct the first dedicated demonstration retrieval repository for ERC, which
includes training samples from widely used datasets, as well as high-quality
dialogue examples generated by LLMs and manually verified. Moreover, we
introduce a curriculum learning strategy into the LoRA fine-tuning process,
incorporating weighted emotional shifts between same-speaker and
different-speaker utterances to assign difficulty levels to dialogue samples,
which are then organized in an easy-to-hard training sequence. Experimental
results on two benchmark datasets-- IEMOCAP and MELD --show that our method
achieves new state-of-the-art (SOTA) performance, demonstrating the
effectiveness and generalizability of our approach in improving LLM-based
emotional understanding.

</details>


### [32] [Improving Region Representation Learning from Urban Imagery with Noisy Long-Caption Supervision](https://arxiv.org/abs/2511.07062)
*Yimei Zhang,Guojiang Shen,Kaili Ning,Tongwei Ren,Xuebo Qiu,Mengmeng Wang,Xiangjie Kong*

Main category: cs.AI

TL;DR: UrbanLN是一个新的预训练框架，通过长文本感知和噪声抑制来改进城市区域表示学习，解决了视觉特征与长文本对齐困难以及LLM生成描述中噪声的问题。


<details>
  <summary>Details</summary>
Motivation: 城市视觉外观反映了潜在的社会经济和环境特征，但现有方法在将细粒度视觉特征与长文本对齐以及处理LLM生成描述中的噪声方面存在挑战。

Method: 提出信息保留拉伸插值策略对齐长文本与细粒度视觉语义；采用双级优化策略：数据级通过多模型协作自动生成多样化可靠描述，模型级使用动量自蒸馏机制生成稳定伪目标。

Result: 在四个真实城市和多个下游任务上的广泛实验表明UrbanLN具有优越性能。

Conclusion: UrbanLN框架有效解决了城市区域表示学习中的长文本对齐和噪声抑制问题，在各种下游任务中表现出色。

Abstract: Region representation learning plays a pivotal role in urban computing by
extracting meaningful features from unlabeled urban data. Analogous to how
perceived facial age reflects an individual's health, the visual appearance of
a city serves as its ``portrait", encapsulating latent socio-economic and
environmental characteristics. Recent studies have explored leveraging Large
Language Models (LLMs) to incorporate textual knowledge into imagery-based
urban region representation learning. However, two major challenges remain:
i)~difficulty in aligning fine-grained visual features with long captions, and
ii) suboptimal knowledge incorporation due to noise in LLM-generated captions.
To address these issues, we propose a novel pre-training framework called
UrbanLN that improves Urban region representation learning through Long-text
awareness and Noise suppression. Specifically, we introduce an
information-preserved stretching interpolation strategy that aligns long
captions with fine-grained visual semantics in complex urban scenes. To
effectively mine knowledge from LLM-generated captions and filter out noise, we
propose a dual-level optimization strategy. At the data level, a multi-model
collaboration pipeline automatically generates diverse and reliable captions
without human intervention. At the model level, we employ a momentum-based
self-distillation mechanism to generate stable pseudo-targets, facilitating
robust cross-modal learning under noisy conditions. Extensive experiments
across four real-world cities and various downstream tasks demonstrate the
superior performance of our UrbanLN.

</details>


### [33] [Increasing AI Explainability by LLM Driven Standard Processes](https://arxiv.org/abs/2511.07083)
*Marc Jansen,Marcel Pehlke*

Main category: cs.AI

TL;DR: 本文提出了一种通过将大型语言模型嵌入标准化分析流程来提高AI系统可解释性的方法，将LLM推理转化为透明可审计的决策轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统可解释AI方法主要关注特征归因或事后解释，而本方法旨在通过将LLM集成到正式决策模型中，将不透明的推理转化为透明可审计的决策过程。

Method: 提出分层架构，将LLM的推理空间与上层的可解释过程空间分离，将LLM嵌入标准化决策模型如QOC、敏感性分析、博弈论和风险管理中。

Result: 实证评估表明，该系统能够在去中心化治理、系统分析和战略推理等场景中重现人类水平的决策逻辑。

Conclusion: LLM驱动的标准流程为可靠、可解释和可验证的AI支持决策提供了基础框架。

Abstract: This paper introduces an approach to increasing the explainability of
artificial intelligence (AI) systems by embedding Large Language Models (LLMs)
within standardized analytical processes. While traditional explainable AI
(XAI) methods focus on feature attribution or post-hoc interpretation, the
proposed framework integrates LLMs into defined decision models such as
Question-Option-Criteria (QOC), Sensitivity Analysis, Game Theory, and Risk
Management. By situating LLM reasoning within these formal structures, the
approach transforms opaque inference into transparent and auditable decision
traces. A layered architecture is presented that separates the reasoning space
of the LLM from the explainable process space above it. Empirical evaluations
show that the system can reproduce human-level decision logic in decentralized
governance, systems analysis, and strategic reasoning contexts. The results
suggest that LLM-driven standard processes provide a foundation for reliable,
interpretable, and verifiable AI-supported decision making.

</details>


### [34] [Green AI: A systematic review and meta-analysis of its definitions, lifecycle models, hardware and measurement attempts](https://arxiv.org/abs/2511.07090)
*Marcel Rojahn,Marcus Grum*

Main category: cs.AI

TL;DR: 本文提出了一个统一的绿色AI定义，建立了五阶段生命周期框架，并制定了包含硬件策略和校准测量的综合方法，以解决AI生命周期中的多维环境影响问题。


<details>
  <summary>Details</summary>
Motivation: AI生命周期中的环境影响（能源、碳、水、隐含影响）缺乏统一的测量和比较标准，现有工具存在异质性且往往忽略水足迹和价值链影响，限制了可比性和可重复性。

Method: 建立五阶段生命周期映射到LCA阶段，通过PDCA循环进行治理，系统化硬件和系统级策略，定义结合估计模型和直接测量的校准测量框架。

Result: 提出了一个可操作的、基于证据的指导框架，使研究人员、从业者和政策制定者能够进行可重复的、提供商无关的环境影响比较。

Conclusion: 通过结合定义、生命周期流程、硬件策略和校准测量，为绿色AI提供了全面且可操作的解决方案，促进AI发展的环境可持续性。

Abstract: Across the Artificial Intelligence (AI) lifecycle - from hardware to
development, deployment, and reuse - burdens span energy, carbon, water, and
embodied impacts. Cloud provider tools improve transparency but remain
heterogeneous and often omit water and value chain effects, limiting
comparability and reproducibility. Addressing these multi dimensional burdens
requires a lifecycle approach linking phase explicit mapping with system levers
(hardware, placement, energy mix, cooling, scheduling) and calibrated
measurement across facility, system, device, and workload levels. This article
(i) establishes a unified, operational definition of Green AI distinct from
Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle
Assessment (LCA) stages, making energy, carbon, water, and embodied impacts
first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles
with decision gateways; (iv) systematizes hardware and system level strategies
across the edge cloud continuum to reduce embodied burdens; and (v) defines a
calibrated measurement framework combining estimator models with direct
metering to enable reproducible, provider agnostic comparisons. Combining
definition, lifecycle processes, hardware strategies, and calibrated
measurement, this article offers actionable, evidence based guidance for
researchers, practitioners, and policymakers.

</details>


### [35] [Data Complexity of Querying Description Logic Knowledge Bases under Cost-Based Semantics](https://arxiv.org/abs/2511.07095)
*Meghyn Bienvenu,Quentin Manière*

Main category: cs.AI

TL;DR: 本文研究了在成本语义下查询不一致加权描述逻辑知识库的数据复杂性，重点关注包含逆角色和角色包含的DL，覆盖了DL-Lite方言。主要贡献包括：精化下界、确定最优成本确定答案语义的精确复杂度，并首次发现在固定成本边界下，DL-Lite^H_bool本体可以实现一阶重写，达到最低数据复杂度TC_0。


<details>
  <summary>Details</summary>
Motivation: 现有对成本语义的研究主要关注EL_⊥到ALCO之间的描述逻辑，且所有结果都显示成本语义是难处理的。本文旨在扩展研究范围到包含逆角色和角色包含的DL，特别是DL-Lite方言，并探索是否存在可处理的情况。

Method: 采用数据复杂性分析方法，研究在成本语义下查询不一致加权描述逻辑知识库。通过分配成本给违反公理和断言的解释，基于最优或有界成本的解释来确定确定和可能的查询答案。

Result: 精化了多个下界并确定了最优成本确定答案语义的精确复杂度（之前没有非平凡上界）。最令人惊讶的结果是：对于DL-Lite^H_bool本体和固定成本边界，实例查询的确定答案和合取查询的可能答案可以通过一阶重写计算，达到最低数据复杂度TC_0。

Conclusion: 虽然成本语义通常被认为是难处理的，但本文首次发现在特定条件下（DL-Lite^H_bool本体和固定成本边界），可以实现高效计算，达到最优的数据复杂度TC_0，这为实际应用提供了理论支持。

Abstract: In this paper, we study the data complexity of querying inconsistent weighted
description logic (DL) knowledge bases under recently-introduced cost-based
semantics. In a nutshell, the idea is to assign each interpretation a cost
based upon the weights of the violated axioms and assertions, and certain and
possible query answers are determined by considering all (resp. some)
interpretations having optimal or bounded cost. Whereas the initial study of
cost-based semantics focused on DLs between $\mathcal{EL}_\bot$ and
$\mathcal{ALCO}$, we consider DLs that may contain inverse roles and role
inclusions, thus covering prominent DL-Lite dialects. Our data complexity
analysis goes significantly beyond existing results by sharpening several lower
bounds and pinpointing the precise complexity of optimal-cost certain answer
semantics (no non-trivial upper bound was known). Moreover, while all existing
results show the intractability of cost-based semantics, our most challenging
and surprising result establishes that if we consider
$\text{DL-Lite}^\mathcal{H}_\mathsf{bool}$ ontologies and a fixed cost bound,
certain answers for instance queries and possible answers for conjunctive
queries can be computed using first-order rewriting and thus enjoy the lowest
possible data complexity ($\mathsf{TC}_0$).

</details>


### [36] [Agentic AI Sustainability Assessment for Supply Chain Document Insights](https://arxiv.org/abs/2511.07097)
*Diego Gosmar,Anna Chiara Pallotta,Giovanni Zenezini*

Main category: cs.AI

TL;DR: 本文提出了一个基于智能体AI的供应链文档智能可持续性评估框架，比较了全人工、AI辅助和智能体AI三种工作流程，实证显示AI方案在能耗、碳排放和水资源使用方面比人工流程减少70-98%。


<details>
  <summary>Details</summary>
Motivation: 解决供应链文档密集型工作流程中自动化效率与环境绩效的双重目标，为AI赋能的供应链解决方案提供统一的ESG导向评估方法。

Method: 开发了包含性能、能耗和排放指标的统一评估框架，比较全人工、AI辅助（人在回路）和智能体AI（使用解析器和验证器的多智能体工作流程）三种场景。

Result: AI辅助和智能体AI场景相比人工流程实现：能耗减少70-90%，二氧化碳排放减少90-97%，水资源使用减少89-98%。完全智能体配置即使资源使用略有增加，仍比纯人工方法有显著可持续性收益。

Conclusion: 智能体AI工作流程在供应链文档处理中能显著提升可持续性表现，该框架为评估和治理AI赋能的供应链解决方案提供了可复制的ESG导向方法论。

Abstract: This paper presents a comprehensive sustainability assessment framework for
document intelligence within supply chain operations, centered on agentic
artificial intelligence (AI). We address the dual objective of improving
automation efficiency while providing measurable environmental performance in
document-intensive workflows. The research compares three scenarios: fully
manual (human-only), AI-assisted (human-in-the-loop, HITL), and an advanced
multi-agent agentic AI workflow leveraging parsers and verifiers. Empirical
results show that AI-assisted HITL and agentic AI scenarios achieve reductions
of up to 70-90% in energy consumption, 90-97% in carbon dioxide emissions, and
89-98% in water usage compared to manual processes. Notably, full agentic
configurations, combining advanced reasoning (thinking mode) and multi-agent
validation, achieve substantial sustainability gains over human-only
approaches, even when resource usage increases slightly versus simpler
AI-assisted solutions. The framework integrates performance, energy, and
emission indicators into a unified ESG-oriented methodology for assessing and
governing AI-enabled supply chain solutions. The paper includes a complete
replicability use case demonstrating the methodology's application to
real-world document extraction tasks.

</details>


### [37] [Boosting Fine-Grained Urban Flow Inference via Lightweight Architecture and Focalized Optimization](https://arxiv.org/abs/2511.07098)
*Yuanshao Zhu,Xiangyu Zhao,Zijian Zhang,Xuetao Wei,James Jianqiao Yu*

Main category: cs.AI

TL;DR: 提出PLGF架构和DualFocal Loss，解决城市流量推断中的计算成本高和性能不佳问题，在保持轻量化的同时实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有城市流量推断方法面临的两个关键挑战：过度参数化模型的高计算成本和传统损失函数在高度偏态分布上的次优性能。

Method: 1) PLGF架构：采用渐进式局部-全局融合策略，有效捕捉细粒度细节和全局上下文依赖；2) DualFocal Loss：集成双空间监督和难度感知聚焦机制，自适应关注难预测区域。

Result: 在4个真实场景上的实验验证了方法的有效性和可扩展性。PLGF将模型大小减少高达97%，在可比参数预算下准确率提升超过10%。

Conclusion: 该方法通过架构效率和自适应优化的协同作用，为细粒度城市流量推断提供了统一解决方案，在性能和效率方面均达到最优水平。

Abstract: Fine-grained urban flow inference is crucial for urban planning and
intelligent transportation systems, enabling precise traffic management and
resource allocation. However, the practical deployment of existing methods is
hindered by two key challenges: the prohibitive computational cost of
over-parameterized models and the suboptimal performance of conventional loss
functions on the highly skewed distribution of urban flows. To address these
challenges, we propose a unified solution that synergizes architectural
efficiency with adaptive optimization. Specifically, we first introduce PLGF, a
lightweight yet powerful architecture that employs a Progressive Local-Global
Fusion strategy to effectively capture both fine-grained details and global
contextual dependencies. Second, we propose DualFocal Loss, a novel function
that integrates dual-space supervision with a difficulty-aware focusing
mechanism, enabling the model to adaptively concentrate on hard-to-predict
regions. Extensive experiments on 4 real-world scenarios validate the
effectiveness and scalability of our method. Notably, while achieving
state-of-the-art performance, PLGF reduces the model size by up to 97% compared
to current high-performing methods. Furthermore, under comparable parameter
budgets, our model yields an accuracy improvement of over 10% against strong
baselines. The implementation is included in the https://github.com/Yasoz/PLGF.

</details>


### [38] [A Theoretical Analysis of Detecting Large Model-Generated Time Series](https://arxiv.org/abs/2511.07104)
*Junji Hou,Junzhou Zhao,Shuo Zhang,Pinghui Wang*

Main category: cs.AI

TL;DR: 本文提出了一种检测时间序列大模型生成合成数据的方法，通过分析模型生成时间序列在递归预测中不确定性逐渐收缩的特性，开发了不确定性收缩估计器来有效识别合成时间序列。


<details>
  <summary>Details</summary>
Motivation: 随着数据滥用和伪造风险增加，需要检测由时间序列大模型生成的合成数据。现有文本生成检测方法不适用于时间序列数据，因为时间序列具有较低信息密度和更平滑的概率分布。

Method: 提出收缩假说：模型生成的时间序列在递归预测中表现出逐渐减小的不确定性。基于此开发了不确定性收缩估计器，通过聚合连续前缀的不确定性指标来识别合成时间序列。

Result: 在32个数据集上的广泛实验表明，UCE始终优于最先进的基线方法，为检测模型生成的时间序列提供了可靠且可推广的解决方案。

Conclusion: 模型生成的时间序列在递归预测中确实表现出不确定性收缩特性，基于此特性开发的不确定性收缩估计器能够有效检测合成时间序列，解决了现有方法在时间序列检测上的局限性。

Abstract: Motivated by the increasing risks of data misuse and fabrication, we
investigate the problem of identifying synthetic time series generated by
Time-Series Large Models (TSLMs) in this work. While there are extensive
researches on detecting model generated text, we find that these existing
methods are not applicable to time series data due to the fundamental modality
difference, as time series usually have lower information density and smoother
probability distributions than text data, which limit the discriminative power
of token-based detectors. To address this issue, we examine the subtle
distributional differences between real and model-generated time series and
propose the contraction hypothesis, which states that model-generated time
series, unlike real ones, exhibit progressively decreasing uncertainty under
recursive forecasting. We formally prove this hypothesis under theoretical
assumptions on model behavior and time series structure. Model-generated time
series exhibit progressively concentrated distributions under recursive
forecasting, leading to uncertainty contraction. We provide empirical
validation of the hypothesis across diverse datasets. Building on this insight,
we introduce the Uncertainty Contraction Estimator (UCE), a white-box detector
that aggregates uncertainty metrics over successive prefixes to identify
TSLM-generated time series. Extensive experiments on 32 datasets show that UCE
consistently outperforms state-of-the-art baselines, offering a reliable and
generalizable solution for detecting model-generated time series.

</details>


### [39] [MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107)
*Liang Shan,Kaicheng Shen,Wen Wu,Zhenyu Ying,Chaochao Lu,Guangze Ye,Liang He*

Main category: cs.AI

TL;DR: MENTOR是一个基于元认知的自我进化框架，用于发现和缓解LLMs在领域任务中的隐式风险，通过元认知自我评估、动态规则知识图谱生成和激活引导等技术，显著降低语义攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs对齐工作主要针对显式风险，但缺乏对领域特定隐式风险的关注，且缺少灵活、可泛化的框架来应对不同专业领域的需求。

Method: 提出元认知驱动的自我评估工具，使LLMs能够反思潜在价值偏差；动态生成补充规则知识图谱扩展静态规则树；在推理时使用激活引导来遵循规则。

Result: 在三个垂直领域的防御测试中，框架显著降低了语义攻击成功率；元认知评估与基线人工评估结果高度一致，且提供更全面深入的分析。

Conclusion: MENTOR框架有效提升了LLMs对隐式风险的缓解能力，建立了持续自我进化循环，降低了静态系统的维护成本和僵化问题。

Abstract: Ensuring the safety and value alignment of large language models (LLMs) is
critical for their deployment. Current alignment efforts primarily target
explicit risks such as bias, hate speech, and violence. However, they often
fail to address deeper, domain-specific implicit risks and lack a flexible,
generalizable framework applicable across diverse specialized fields. Hence, we
proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering
and mitigating implicit Risks in LLMs on Domain Tasks. To address the
limitations of labor-intensive human evaluation, we introduce a novel
metacognitive self-assessment tool. This enables LLMs to reflect on potential
value misalignments in their responses using strategies like perspective-taking
and consequential thinking. We also release a supporting dataset of 9,000 risk
queries spanning education, finance, and management to enhance domain-specific
risk identification. Subsequently, based on the outcomes of metacognitive
reflection, the framework dynamically generates supplementary rule knowledge
graphs that extend predefined static rule trees. This enables models to
actively apply validated rules to future similar challenges, establishing a
continuous self-evolution cycle that enhances generalization by reducing
maintenance costs and inflexibility of static systems. Finally, we employ
activation steering during inference to guide LLMs in following the rules, a
cost-effective method to robustly enhance enforcement across diverse contexts.
Experimental results show MENTOR's effectiveness: In defensive testing across
three vertical domains, the framework substantially reduces semantic attack
success rates, enabling a new level of implicit risk mitigation for LLMs.
Furthermore, metacognitive assessment not only aligns closely with baseline
human evaluators but also delivers more thorough and insightful analysis of
LLMs value alignment.

</details>


### [40] [PADiff: Predictive and Adaptive Diffusion Policies for Ad Hoc Teamwork](https://arxiv.org/abs/2511.07260)
*Hohei Chan,Xinzhi Zhang,Antao Xiang,Weinan Zhang,Mengchen Zhao*

Main category: cs.AI

TL;DR: PADiff是一种基于扩散模型的方法，用于解决ad hoc teamwork中的多模态行为问题，通过整合队友预测信息来适应动态环境，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于强化学习的方法在ad hoc teamwork中往往导致策略坍缩为单一行为，无法捕捉多模态合作模式，需要开发能够预测和适应未知队友的智能体。

Method: 提出PADiff方法，使用扩散模型捕捉智能体的多模态行为，并在去噪过程中整合队友的关键预测信息，以应对高度非平稳的ad hoc teamwork场景。

Result: 在三个合作环境中的广泛实验表明，PADiff显著优于现有的ad hoc teamwork方法。

Conclusion: PADiff通过扩散模型成功解决了ad hoc teamwork中的多模态合作问题，为动态协作提供了有效解决方案。

Abstract: Ad hoc teamwork (AHT) requires agents to collaborate with previously unseen
teammates, which is crucial for many real-world applications. The core
challenge of AHT is to develop an ego agent that can predict and adapt to
unknown teammates on the fly. Conventional RL-based approaches optimize a
single expected return, which often causes policies to collapse into a single
dominant behavior, thus failing to capture the multimodal cooperation patterns
inherent in AHT. In this work, we introduce PADiff, a diffusion-based approach
that captures agent's multimodal behaviors, unlocking its diverse cooperation
modes with teammates. However, standard diffusion models lack the ability to
predict and adapt in highly non-stationary AHT scenarios. To address this
limitation, we propose a novel diffusion-based policy that integrates critical
predictive information about teammates into the denoising process. Extensive
experiments across three cooperation environments demonstrate that PADiff
outperforms existing AHT methods significantly.

</details>


### [41] [AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning](https://arxiv.org/abs/2511.07262)
*Qile Jiang,George Karniadakis*

Main category: cs.AI

TL;DR: AgenticSciML是一个多智能体协作系统，通过10多个专业AI智能体的结构化推理和迭代演化，自动设计和优化科学机器学习解决方案，在多个任务中显著超越单智能体和人工设计的基线方法。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习的设计过程需要专家驱动，需要大量实验和问题特定洞察，这限制了其可扩展性和效率。

Method: 采用协作多智能体系统，结合结构化辩论、检索增强方法记忆和集成引导的进化搜索，让智能体协作提出、批评和改进SciML解决方案。

Result: 在物理信息学习和算子学习任务中，该框架发现的解决方案比单智能体和人工设计基线在误差减少方面高出四个数量级，产生了包括自适应专家混合架构、基于分解的PINNs和物理信息算子学习模型等新颖策略。

Conclusion: AI智能体之间的协作推理可以产生涌现的方法创新，为科学计算中可扩展、透明和自主的发现提供了一条路径。

Abstract: Scientific Machine Learning (SciML) integrates data-driven inference with
physical modeling to solve complex problems in science and engineering.
However, the design of SciML architectures, loss formulations, and training
strategies remains an expert-driven research process, requiring extensive
experimentation and problem-specific insights. Here we introduce AgenticSciML,
a collaborative multi-agent system in which over 10 specialized AI agents
collaborate to propose, critique, and refine SciML solutions through structured
reasoning and iterative evolution. The framework integrates structured debate,
retrieval-augmented method memory, and ensemble-guided evolutionary search,
enabling the agents to generate and assess new hypotheses about architectures
and optimization procedures. Across physics-informed learning and operator
learning tasks, the framework discovers solution methods that outperform
single-agent and human-designed baselines by up to four orders of magnitude in
error reduction. The agents produce novel strategies -- including adaptive
mixture-of-expert architectures, decomposition-based PINNs, and
physics-informed operator learning models -- that do not appear explicitly in
the curated knowledge base. These results show that collaborative reasoning
among AI agents can yield emergent methodological innovation, suggesting a path
toward scalable, transparent, and autonomous discovery in scientific computing.

</details>


### [42] [Beyond Detection: Exploring Evidence-based Multi-Agent Debate for Misinformation Intervention and Persuasion](https://arxiv.org/abs/2511.07267)
*Chen Han,Yijia Ma,Jin Tan,Wenzhen Zheng,Xijin Tang*

Main category: cs.AI

TL;DR: ED2D是一个基于证据的多智能体辩论框架，不仅用于检测错误信息，还旨在纠正用户信念并阻止错误信息传播。当预测正确时，其生成的辟谣文本具有与人类专家相当的劝说效果；但当分类错误时，其解释可能无意中强化用户的误解。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体辩论框架主要关注检测准确性，忽视了帮助用户理解事实判断背后的推理过程和发展未来抵抗力的重要性。辩论记录提供了丰富但未充分利用的透明推理资源。

Method: 引入ED2D框架，扩展了先前方法，加入了事实证据检索功能。ED2D不仅是一个检测框架，还是一个旨在纠正用户信念和阻止错误信息传播的劝说性多智能体系统。

Result: ED2D在三个错误信息检测基准测试中优于现有基线。当ED2D生成正确预测时，其辟谣文本的劝说效果与人类专家相当；但当分类错误时，其解释可能无意中强化用户的误解，即使与准确的人类解释一起呈现。

Conclusion: 研究结果强调了部署多智能体辩论系统进行错误信息干预的潜力和潜在风险。开发了公共社区网站来帮助用户探索ED2D，促进透明度、批判性思维和协作事实核查。

Abstract: Multi-agent debate (MAD) frameworks have emerged as promising approaches for
misinformation detection by simulating adversarial reasoning. While prior work
has focused on detection accuracy, it overlooks the importance of helping users
understand the reasoning behind factual judgments and develop future
resilience. The debate transcripts generated during MAD offer a rich but
underutilized resource for transparent reasoning. In this study, we introduce
ED2D, an evidence-based MAD framework that extends previous approach by
incorporating factual evidence retrieval. More importantly, ED2D is designed
not only as a detection framework but also as a persuasive multi-agent system
aimed at correcting user beliefs and discouraging misinformation sharing. We
compare the persuasive effects of ED2D-generated debunking transcripts with
those authored by human experts. Results demonstrate that ED2D outperforms
existing baselines across three misinformation detection benchmarks. When ED2D
generates correct predictions, its debunking transcripts exhibit persuasive
effects comparable to those of human experts; However, when ED2D misclassifies,
its accompanying explanations may inadvertently reinforce users'misconceptions,
even when presented alongside accurate human explanations. Our findings
highlight both the promise and the potential risks of deploying MAD systems for
misinformation intervention. We further develop a public community website to
help users explore ED2D, fostering transparency, critical thinking, and
collaborative fact-checking.

</details>


### [43] [IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction](https://arxiv.org/abs/2511.07327)
*Guoxin Chen,Zile Qiao,Xuanzhong Chen,Donglei Yu,Haotian Xu,Wayne Xin Zhao,Ruihua Song,Wenbiao Yin,Huifeng Yin,Liwen Zhang,Kuan Li,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.AI

TL;DR: IterResearch提出了一种迭代式深度研究范式，通过马尔可夫决策过程和策略性工作空间重构解决长时程研究任务中的上下文窒息和噪声污染问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究智能体采用单上下文范式，在长时程任务中会导致上下文窒息和噪声污染，限制了其有效性。

Method: 引入IterResearch迭代研究范式，将长时程研究建模为马尔可夫决策过程，维护演化报告作为记忆，周期性合成见解；开发效率感知策略优化框架，通过几何奖励折扣激励高效探索。

Result: 在六个基准测试中平均提升14.5个百分点，交互规模扩展到2048次时性能从3.5%提升至42.5%，作为提示策略可使前沿模型在长时程任务上比ReAct提升19.2个百分点。

Conclusion: IterResearch是长时程推理的通用解决方案，既可作为训练智能体，也可作为前沿模型的提示范式。

Abstract: Recent advances in deep-research agents have shown promise for autonomous
knowledge construction through dynamic reasoning over external sources.
However, existing approaches rely on a mono-contextual paradigm that
accumulates all information in a single, expanding context window, leading to
context suffocation and noise contamination that limit their effectiveness on
long-horizon tasks. We introduce IterResearch, a novel iterative deep-research
paradigm that reformulates long-horizon research as a Markov Decision Process
with strategic workspace reconstruction. By maintaining an evolving report as
memory and periodically synthesizing insights, our approach preserves
consistent reasoning capacity across arbitrary exploration depths. We further
develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning
framework that incentivizes efficient exploration through geometric reward
discounting and enables stable distributed training via adaptive downsampling.
Extensive experiments demonstrate that IterResearch achieves substantial
improvements over existing open-source agents with average +14.5pp across six
benchmarks and narrows the gap with frontier proprietary systems. Remarkably,
our paradigm exhibits unprecedented interaction scaling, extending to 2048
interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves
as an effective prompting strategy, improving frontier models by up to 19.2pp
over ReAct on long-horizon tasks. These findings position IterResearch as a
versatile solution for long-horizon reasoning, effective both as a trained
agent and as a prompting paradigm for frontier models.

</details>


### [44] [DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas](https://arxiv.org/abs/2511.07338)
*Zhen Wang,Yufan Zhou,Zhongyan Luo,Lyumanshan Ye,Adam Wood,Man Yao,Luoshang Pan*

Main category: cs.AI

TL;DR: DEEPPERSONA是一个可扩展的生成引擎，通过两阶段、分类学引导的方法合成叙事完整的合成人物角色，显著提升了人物属性的多样性和独特性，并在个性化问答和社会调查中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成人物角色大多浅薄简单，缺乏真实人类身份的丰富复杂性和多样性，需要开发能够生成更深度、更真实人物角色的方法。

Method: 采用两阶段方法：首先通过挖掘数千个真实用户-ChatGPT对话算法构建最大的人类属性分类学；然后从该分类学中逐步采样属性，条件生成连贯且真实的人物角色。

Result: 内在评估显示属性多样性提高32%，人物独特性提高44%；外在评估中，人物角色使GPT-4.1-mini的个性化问答准确率平均提高11.6%，并将LLM公民与真实人类在社交调查中的差距缩小31.7%。

Conclusion: DEEPPERSONA为高保真人类模拟和个性化AI研究提供了一个严谨、可扩展且无隐私的平台。

Abstract: Simulating human profiles by instilling personas into large language models
(LLMs) is rapidly transforming research in agentic behavioral simulation, LLM
personalization, and human-AI alignment. However, most existing synthetic
personas remain shallow and simplistic, capturing minimal attributes and
failing to reflect the rich complexity and diversity of real human identities.
We introduce DEEPPERSONA, a scalable generative engine for synthesizing
narrative-complete synthetic personas through a two-stage, taxonomy-guided
method. First, we algorithmically construct the largest-ever human-attribute
taxonomy, comprising over hundreds of hierarchically organized attributes, by
mining thousands of real user-ChatGPT conversations. Second, we progressively
sample attributes from this taxonomy, conditionally generating coherent and
realistic personas that average hundreds of structured attributes and roughly 1
MB of narrative text, two orders of magnitude deeper than prior works.
Intrinsic evaluations confirm significant improvements in attribute diversity
(32 percent higher coverage) and profile uniqueness (44 percent greater)
compared to state-of-the-art baselines. Extrinsically, our personas enhance
GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on
average across ten metrics and substantially narrow (by 31.7 percent) the gap
between simulated LLM citizens and authentic human responses in social surveys.
Our generated national citizens reduced the performance gap on the Big Five
personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA
thus provides a rigorous, scalable, and privacy-free platform for high-fidelity
human simulation and personalized AI research.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [45] [Fast Time-Varying mmWave MIMO Channel Estimation and Reconstruction: An Efficient Rank-Aware Matrix Completion Method](https://arxiv.org/abs/2511.05902)
*Tianyu Jiang,Yan Yang,Hongjin Liu,Runyu Han,Bo Ai,Mohsen Guizani*

Main category: eess.SP

TL;DR: 提出一种两阶段秩感知压缩感知框架，用于毫米波MIMO系统中快速时变信道估计，结合低秩矩阵补全和稀疏信道恢复，在动态条件下提高估计精度并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决毫米波MIMO系统中快速时变信道估计问题，特别是在不完美信道状态信息和用户移动性导致信道秩突变的情况下，需要高效的信道重建方法。

Method: 第一阶段使用鲁棒秩一矩阵补全算法进行低秩矩阵补全，结合离散时间自回归模型利用时间秩相关性实现自适应观测矩阵补全；第二阶段开发秩感知块正交匹配追踪算法进行稀疏信道恢复，并引入秩感知测量矩阵设计。

Result: 仿真结果表明，与现有基准算法相比，所提方法在显著降低计算复杂度和训练开销的同时，实现了更优的信道估计性能。

Conclusion: 该两阶段秩感知压缩感知框架能够有效处理毫米波MIMO系统中的快速时变信道估计问题，在动态条件下提供准确的信道重建，同时保持较低的计算复杂度。

Abstract: We address the problem of fast time-varying channel estimation in
millimeter-wave (mmWave) MIMO systems with imperfect channel state information
(CSI) and facilitate efficient channel reconstruction. Specifically, leveraging
the low-rank and sparse characteristics of the mmWave channel matrix, a
two-phase rank-aware compressed sensing framework is proposed for efficient
channel estimation and reconstruction. In the first phase, a robust rank-one
matrix completion (R1MC) algorithm is used to reconstruct part of the observed
channel matrix through low-rank matrix completion (LRMC). To address abrupt
rank changes caused by user mobility, a discrete-time autoregressive (AR) model
is established that leverages temporal rank correlations across consecutive
time instances to enable adaptive observation matrix completion, thereby
improving estimation accuracy under dynamic conditions. In the second phase, a
rank-aware block orthogonal matching pursuit (RA-BOMP) algorithm is developed
for sparse channel recovery with low computational complexity. Furthermore, a
rank-aware measurement matrix design is introduced to improve angle estimation
accuracy. Simulation results demonstrate that, compared with existing benchmark
algorithms, the proposed approach achieves superior channel estimation
performance while significantly reducing computational complexity and training
overhead.

</details>


### [46] [Invariants in Eddy Current Testing via Dimensional Analysis](https://arxiv.org/abs/2511.06035)
*Vincenzo Mottola,Alessandro Sardellitti,Filippo Milano,Luigi Ferrigno,Marco Laracca,Antonello Tamburrino*

Main category: eess.SP

TL;DR: 本文提出了一种基于量纲分析的频域涡流检测数据不变变换系统方法，用于在存在不确定参数的情况下准确估计目标参数。


<details>
  <summary>Details</summary>
Motivation: 在无损检测应用中，经常需要从测量数据中估计特定子集的参数，而忽略其他不确定或变化参数的影响，如涡流检测中估计板厚和电导率而不受探头提离的影响。

Method: 通过量纲分析和白金汉π定理，推导出频域涡流检测数据的不变变换，使测量信号独立于一个或多个不确定参数。

Result: 实验验证表明，该方法在不同板厚、电导率和提离的配置下，能够在对广泛感兴趣参数范围内实现优异的估计精度。

Conclusion: 提出的不变变换方法为多参数估计提供了理论基础，兼容实时和在线操作，显著提高了无损检测结果的准确性和可靠性。

Abstract: The Buckingham's $\pi$, theorem has been recently introduced in the context
of Non destructive Testing \& Evaluation (NdT\&E) , giving a theoretical basis
for developing simple but effective methods for multi-parameter estimation via
dimensional analysis. Dimensional groups, or $\pi-$groups, allow for the
reduction of the number of parameters affecting the dimensionless measured
quantities.
  In many real-world applications, the main interest is in estimating only a
subset of the variables affecting the measurements. An example is estimating
the thickness and electrical conductivity of a plate from Eddy Current Testing
data, regardless of the lift-off of the probe, which may be either uncertain
and/or variable. Alternatively, one may seek to estimate thickness and lift-off
while neglecting the influence of the electrical conductivity, or to estimate
the electrical conductivity and the lift-off, neglecting the thickness.
  This is where the concept of invariants becomes crucial. An invariant
transformation is a mathematical mapping that makes the measured signal
independent of one or more of these uncertain parameters. Invariant
transformations provide a way to isolate useful signals from uncertain ones,
improving the accuracy and reliability of the NdT results.
  The main contribution of this paper is a systematic method to derive
\emph{invariant} transformations for frequency domain Eddy Current Testing
data, via dimensional analysis. The proposed method is compatible with
real-time and in-line operations.
  After its theoretical foundation is introduced, the method is validated by
means of experimental data, with reference to configurations consisting of
plates with different thicknesses, electrical conductivity, and lift-off. The
experimental validation proves the effectiveness of the method in achieving
excellent accuracy on a wide range of parameters of interest.

</details>


### [47] [Online Learning of Modular Bayesian Deep Receivers: Single-Step Adaptation with Streaming Data](https://arxiv.org/abs/2511.06045)
*Yakov Gusakov,Osvaldo Simeone,Tirza Routtenberg,Nir Shlezinger*

Main category: eess.SP

TL;DR: 本文提出了一种基于贝叶斯跟踪的在线学习框架，用于快速低复杂度地适应DNN无线接收器，解决了传统SGD方法在动态信道环境下的延迟和计算负担问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于预训练的静态DNN接收器在快速变化的无线信道环境下效果不佳，而基于SGD的在线学习方法存在延迟和计算负担大的问题，需要一种更高效的在线适应方法。

Method: 采用贝叶斯参数空间跟踪实现单步适应，避免多轮SGD训练；使用模块化DNN架构支持并行、在线和局部变分贝叶斯更新。

Result: 仿真实验表明，该框架能在实际通信信道中保持低误码率，显著降低更新延迟，并提高对信道动态变化的鲁棒性。

Conclusion: 提出的在线学习框架为DNN无线接收器在动态信道环境中的高效适应提供了可行的解决方案，优于传统的梯度下降方法。

Abstract: Deep neural network (DNN)-based receivers offer a powerful alternative to
classical model-based designs for wireless communication, especially in complex
and nonlinear propagation environments. However, their adoption is challenged
by the rapid variability of wireless channels, which makes pre-trained static
DNN-based receivers ineffective, and by the latency and computational burden of
online stochastic gradient descent (SGD)-based learning. In this work, we
propose an online learning framework that enables rapid low-complexity
adaptation of DNN-based receivers. Our approach is based on two main tenets.
First, we cast online learning as Bayesian tracking in parameter space,
enabling a single-step adaptation, which deviates from multi-epoch SGD .
Second, we focus on modular DNN architectures that enable parallel, online, and
localized variational Bayesian updates. Simulations with practical
communication channels demonstrate that our proposed online learning framework
can maintain a low error rate with markedly reduced update latency and
increased robustness to channel dynamics as compared to traditional gradient
descent based method.

</details>


### [48] [Positioning Using LEO Satellite Communication Signals Under Orbital Errors](https://arxiv.org/abs/2511.06060)
*Jie Ma,Pinjun Zheng,Xing Liu,Yuchen Zhang,Ali A. Nasir,Tareq Y. Al-Naffouri*

Main category: eess.SP

TL;DR: 本文研究了低地球轨道卫星在轨道误差下的定位问题，提出了基于误指定Cramér-Rao界的轨道校准和用户定位方法，显著提高了定位精度。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星作为全球导航卫星系统的替代方案具有潜力，但其较低轨道高度使其更易受轨道扰动影响，从而降低定位精度。

Method: 首先建立了考虑地球非球面性的LEO轨道模型和宽频通信模型，然后进行误指定Cramér-Rao界分析评估轨道误差影响，最后提出两阶段定位方法：基于MCRB的加权轨道校准和最小二乘用户定位。

Result: MCRB分析显示轨道误差可导致千米级位置偏差，仿真表明所提估计器相比轨道失配基线显著提高了定位精度，误差达到米级。

Conclusion: 提出的两阶段定位方法能有效减轻轨道误差对LEO卫星定位性能的影响，实现米级定位精度。

Abstract: Low Earth orbit (LEO) satellites offer a promising alternative to global
navigation satellite systems for precise positioning; however, their relatively
low altitudes make them more susceptible to orbital perturbations, which in
turn degrade positioning accuracy. In this work, we study LEO-based positioning
under orbital errors within a signal-of-opportunity framework. First, we
introduce a LEO orbit model that accounts for Earth's non-sphericity and derive
a wideband communication model that captures fast- and slow-time Doppler
effects and multipath propagation. Subsequently, we perform a misspecified
Cram\'er-Rao bound (MCRB) analysis to evaluate the impact of orbital errors on
positioning performance. Then, we propose a two-stage positioning method
starting with a (i) MCRB-based weighted orbit calibration, followed by (ii)
least-squares user positioning using the corrected orbit. The MCRB analysis
indicates that orbital errors can induce kilometer-level position biases.
Extensive simulations show that the proposed estimator can considerably enhance
the positioning accuracy relative to the orbit-mismatched baseline, yielding
errors on the order of a few meters.

</details>


### [49] [Hierarchically Block-Sparse Recovery With Prior Support Information](https://arxiv.org/abs/2511.06173)
*Liyang Lu,Haochen Wu,Wenbo Xu,Zhaocheng Wang,H. Vincent Poor*

Main category: eess.SP

TL;DR: 本文提出了基于先验支撑信息（PSI）的分层压缩感知（HCS）的新恢复界，设计了HiBOMP-P算法，并推导了基于互不相干性（MIP）的精确恢复条件，证明了HCS在PSI与真实支撑集不重叠时仍能提供改进的恢复性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法严重依赖先验信息与真实支撑集的重叠，当这种重叠不存在时性能会受损。本文旨在开发一种更稳健的分层压缩感知方法，即使先验信息不准确也能提供改进的恢复性能。

Method: 提出了基于各种形式PSI的详细重建模型，设计了递归形式的分层块正交匹配追踪算法（HiBOMP-P），并推导了基于互不相干性（MIP）的精确恢复条件。

Result: 开发了可重构稀疏度水平来揭示精确恢复条件的充分条件，提出了噪声场景下的可靠恢复条件，并分析了稀疏度不为零时的最优分层结构。

Conclusion: 分层压缩感知即使在先验信息与真实支撑集不重叠时也能提供改进的恢复性能，而现有方法严重依赖这种重叠，从而在不重叠时性能受损。

Abstract: We provide new recovery bounds for hierarchical compressed sensing (HCS)
based on prior support information (PSI). A detailed PSI-enabled reconstruction
model is formulated using various forms of PSI. The hierarchical block
orthogonal matching pursuit with PSI (HiBOMP-P) algorithm is designed in a
recursive form to reliably recover hierarchically block-sparse signals. We
derive exact recovery conditions (ERCs) measured by the mutual incoherence
property (MIP), wherein hierarchical MIP concepts are proposed, and further
develop reconstructible sparsity levels to reveal sufficient conditions for
ERCs. Leveraging these MIP analyses, we present several extended insights,
including reliable recovery conditions in noisy scenarios and the optimal
hierarchical structure for cases where sparsity is not equal to zero. Our
results further confirm that HCS offers improved recovery performance even when
the prior information does not overlap with the true support set, whereas
existing methods heavily rely on this overlap, thereby compromising performance
if it is absent.

</details>


### [50] [Fast Reconstruction of Motion-Corrupted Data with Mobile-GRAPPA: Motion and dB0 Inhomogeneity Correction Leveraging Efficient GRAPPA](https://arxiv.org/abs/2511.06257)
*Yimeng Lin,Nan Wang,Daniel Abraham,Daniel Polak,Xiaozhi Cao,Stephen Cauley,Kawin Setsompop*

Main category: eess.SP

TL;DR: Mobile-GRAPPA是一种k空间"清理"方法，使用局部GRAPPA算子去除运动和dB0相关伪影，使数据能够用标准SENSE重建，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有运动导航技术能够快速跟踪运动和dB0诱导相位，但将这些高时间分辨率信息准确整合到SENSE重建中计算成本过高，难以实际应用。

Method: 通过轻量级多层感知器(MLP)高效训练k空间位置特定的Mobile-GRAPPA核，并在整个k空间应用这些核来生成干净数据。

Result: 在严重运动伪影的1mm全脑GRE和EPTI实验中，Mobile-GRAPPA实现了准确重建且时间代价可忽略，而完整Aligned-SENSE重建时间过长（GRE>10小时，EPTI>10天）。

Conclusion: Mobile-GRAPPA能够以最小计算开销将详细运动和dB0跟踪整合到SENSE中，实现对挑战性数据的快速高质量重建。

Abstract: Advanced motion navigations now enable rapid tracking of subject motion and
dB0-induced phase, but accurately incorporating this high-temporal-resolution
information into SENSE (Aligned-SENSE) is often computationally prohibitive. We
propose "Mobile-GRAPPA", a k-space "cleaning" approach that uses local GRAPPA
operators to remove motion and dB0 related corruption so that the resulting
data can be reconstructed with standard SENSE. We efficiently train a family of
k-space-position-specific Mobile-GRAPPA kernels via a lightweight multilayer
perceptron (MLP) and apply them across k-space to generate clean data. In
experiments on highly motion-corrupted 1-mm whole-brain GRE (Tacq = 10 min;
1,620 motion/dB0 trackings) and EPTI (Tacq = 2 min; 544 trackings),
Mobile-GRAPPA enabled accurate reconstruction with negligible time penalty,
whereas full Aligned-SENSE was impractical (reconstruction times > 10 h for GRE
and > 10 days for EPTI). These results show that Mobile-GRAPPA incorporates
detailed motion and dB0 tracking into SENSE with minimal computational
overhead, enabling fast, high-quality reconstructions of challenging data.

</details>


### [51] [CSIT-Free Multi-Group Multicast Transmission in Overloaded mmWave Systems](https://arxiv.org/abs/2511.06369)
*Wonseok Choi,Jeongjae Lee,Songnam Hong*

Main category: eess.SP

TL;DR: 本文提出了一种无CSIT的多组多播传输方案，通过确定性预编码和闭式功率分配，在过载毫米波系统中实现低复杂度的干扰消除和性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的多组多播波束赋形需要高计算复杂度和大量反馈开销来获取CSIT，同时多组间的干扰管理和波束赋形优化会导致显著的速率损失。

Method: 提出CSIT-free多组多播传输方案，采用确定性CSIT-free预编码和基于最大最小公平的闭式功率分配，使每个用户能够完全消除组间干扰。

Result: 仿真结果表明，所提出的CF-MGM在可实现速率和组内用户增加方面优于现有的基于CSIT的方法，具有优越性和可扩展性。

Conclusion: CF-MGM方案成功消除了复杂CSIT获取的需求，以显著低复杂度实现了组间干扰的完全消除，在过载毫米波系统中表现出优越性能。

Abstract: In this paper, we investigate the downlink multi-group multicast (MGM)
transmission problem in overloaded mmWave systems. In particular, the
conventional MGM beamforming requires substantial computational complexity and
feedback (or pilot) overhead for acquisition of channel state information at
the transmitter (CSIT), while simultaneous interference management and
multicast beamforming optimization across multi-group inevitably incurs a
significant rate loss. To address this, we propose a CSIT-free MGM (CF-MGM)
transmission that eliminates the need for a complex CSIT acquisition. A
deterministic CSIT-free precoding and proposed closed-form power allocation
based on max-min fairness (MMF) allow each user to detect the common multicast
stream completely canceling the inter-group interference with a significantly
low complexity. Simulation results demonstrate the superiority and scalability
of the proposed CF-MGM for the achievable rate and increase of users in a group
outperforming the existing CSIT-based methods.

</details>


### [52] [Near-Field Velocity Estimation and Predictive Beamforming with Modular Linear Array](https://arxiv.org/abs/2511.06383)
*Khalid A. Alshumayri,Mudassir Masood,Ali. A. Nasir*

Main category: eess.SP

TL;DR: 本文推导了在预测波束成形框架下使用模块化线性阵列进行联合速度估计的闭式克拉美-罗界，分析了模块间距对速度估计精度的影响，并研究了速度失配对阵列增益的影响。


<details>
  <summary>Details</summary>
Motivation: 速度估计是近场预测波束成形的基础，需要研究模块化线性阵列在预测波束成形框架下的性能界限和设计准则。

Method: 推导闭式克拉美-罗界，分析模块间距对横向和径向速度估计精度的影响，研究速度失配对阵列增益的影响，并通过仿真验证目标跟踪性能。

Result: 增加模块间距可以扩大有效孔径并降低横向速度的CRB，而径向速度CRB对间距不敏感；横向速度误差比径向速度误差造成更严重的性能下降；预测波束成形与MLA在目标跟踪中保持高定位精度。

Conclusion: 模块化线性阵列在预测波束成形框架下能够有效估计速度，通过优化模块间距可以在保持精度的同时减少天线数量，横向速度估计精度对系统性能更为关键。

Abstract: Velocity estimation is a cornerstone of recently introduced near-field
predictive beamforming. This paper derives the closed-form Cramer-Rao bounds
(CRBs) for joint velocity estimation using a modular linear array (MLA) within
a predictive-beamforming framework. The analysis shows that increasing
inter-module separation enlarges the effective aperture and reduces the
transverse-velocity CRB, whereas the radial-velocity CRB is largely insensitive
to separation. We further obtain a simple closed-form relation linking the
achievable antenna savings to the inter-module separation while preserving the
same transverse accuracy of a uniform linear array (ULA). We further
investigate how velocity mismatch affects array gain and show that
transverse-velocity errors cause more severe performance degradation than
radial-velocity errors. Simulations show that predictive beamforming with MLAs
maintains high localization accuracy for target tracking.

</details>


### [53] [Distributed MIMO Positioning: Fundamental Limit Analysis and User Tracking Framework Design](https://arxiv.org/abs/2511.06440)
*Yingjie Xu,Xuesong Cai,Ali Al-Ameri,Sara Willhammar,Fredrik Tufvesson*

Main category: eess.SP

TL;DR: 本文研究了分布式MIMO系统中用户3D定位能力，采用极化天线模型分析定位误差边界，提出基于概率假设密度滤波和AP管理的定位框架，通过实测验证达到厘米级跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖理想化的各向同性天线模型，无法准确反映实际天线模式、布局和极化效应的影响，需要更真实的极化模型来提升分布式MIMO系统的3D定位精度。

Method: 采用极化天线模型和有效孔径分布函数表征实际天线特性，基于Fisher信息矩阵和位置误差边界分析定位极限，提出集成全局概率假设密度滤波和PEB感知AP管理的完整定位框架。

Result: 通过分布式MIMO信道测量验证，实现了厘米级跟踪精度，PEB感知AP管理策略在显著减少并发活跃AP数量的同时保持了鲁棒的跟踪性能。

Conclusion: 所提出的极化天线模型和定位框架能够有效提升分布式MIMO系统的3D定位性能，在降低系统开销的同时实现高精度用户跟踪。

Abstract: This paper presents a comprehensive study on the 3D positioning capabilities
of users in distributed multiple-input multiple-output (MIMO) systems. Unlike
previous studies that mainly rely on idealized isotropic antenna models, we
adopt a polarimetric model that takes advantage of effective aperture
distribution functions to characterize realistic antenna patterns, placements,
and polarization effects. Based on this model, we analyze the fundamental
limits of UE positioning using the Fisher information matrix (FIM) and the
position error bound (PEB). The FIM is shown to be expressed as a weighted sum
of the information contributions from individual access point (AP)-UE pairs,
with each contribution interpreted geometrically across distance, azimuth, and
elevation dimensions. The impact of the UE tilt and the spatial distribution of
APs on the PEBs is further analyzed. As a further advancement, we propose a
complete positioning framework from a UE tracking perspective. By integrating a
global probability hypothesis density filter and a PEB-aware AP management
strategy, the framework enables accurate tracking while optimizing AP
scheduling. Finally, we present a distributed MIMO channel measurement campaign
to validate the proposed framework. The results demonstrate a centimeter-level
tracking accuracy. In addition, the PEB-aware AP management strategy is shown
to maintain robust tracking performance while significantly reducing the number
of concurrently active APs, thus lowering the overall system overhead.

</details>


### [54] [Structure-Aware Near-Field Radio Map Recovery via RBF-Assisted Matrix Completion](https://arxiv.org/abs/2511.06710)
*Hao Sun,Xianghao Yu,Junting Chen*

Main category: eess.SP

TL;DR: 提出了一种基于径向基函数插值的结构感知矩阵补全框架，用于XL-MIMO系统中的近场无线电地图构建，通过结合局部变化结构建模和全局低秩结构利用，显著提高了重构精度。


<details>
  <summary>Details</summary>
Motivation: 近场波前由于球面波传播在角度和距离上具有强依赖性，导致接收信号强度变化复杂，传统方法难以有效捕捉近场环境中的复杂空间变化结构。

Method: 开发了正则化RBF插值方法，引入逆μ律启发的非均匀采样策略，将RBF插值与基于核范数最小化的矩阵补全相结合，并提出鲁棒的Huberized留一交叉验证方案进行参数自适应选择。

Result: 在变化的采样密度和阴影条件下，所提方法相比标准插值和矩阵补全方法在归一化均方误差上实现了超过10%的改进。

Conclusion: 通过RBF插值的局部变化结构建模与矩阵补全的全局低秩结构利用的集成，构建了一个结构感知框架，显著提高了近场无线电地图重构的准确性。

Abstract: This paper proposes a novel structure-aware matrix completion framework
assisted by radial basis function (RBF) interpolation for near-field radio map
construction in extremely large multiple-input multiple-output (XL-MIMO)
systems. Unlike the far-field scenario, near-field wavefronts exhibit strong
dependencies on both angle and distance due to spherical wave propagation,
leading to complicated variations in received signal strength (RSS). To
effectively capture the intricate spatial variations structure inherent in
near-field environments, a regularized RBF interpolation method is developed to
enhance radio map reconstruction accuracy. Leveraging theoretical insights from
interpolation error analysis of RBF, an inverse {\mu}-law-inspired nonuniform
sampling strategy is introduced to allocate measurements adaptively,
emphasizing regions with rapid RSS variations near the transmitter. To further
exploit the global low-rank structure in the near-field radio map, we integrate
RBF interpolation with nuclear norm minimization (NNM)-based matrix completion.
A robust Huberized leave-one-out cross-validation (LOOCV) scheme is then
proposed for adaptive selection of the tolerance parameter, facilitating
optimal fusion between RBF interpolation and matrix completion. The integration
of local variation structure modeling via RBF interpolation and global low-rank
structure exploitation via matrix completion yields a structure-aware framework
that substantially improves the accuracy of near-field radio map
reconstruction. Extensive simulations demonstrate that the proposed approach
achieves over 10% improvement in normalized mean squared error (NMSE) compared
to standard interpolation and matrix completion methods under varying sampling
densities and shadowing conditions.

</details>


### [55] [Learning Performance Optimization for Edge AI System with Time and Energy Constraints](https://arxiv.org/abs/2511.06806)
*Zhiyuan Zhai,Wei Ni,Xin Wang*

Main category: eess.SP

TL;DR: 本文研究了边缘AI系统的能效优化问题，通过建模数据采集、计算和通信过程的时间与能耗，分析系统参数对学习性能的影响，并提出了在时间和能量约束下最大化学习性能的优化算法。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统在实时处理和决策方面具有重要价值，但其部署面临高能耗和长时间运行的挑战，需要优化系统性能以在资源受限条件下实现高效学习。

Method: 建立了边缘AI系统的时间与能耗模型，进行严格的收敛性分析，构建系统级优化问题，针对同质和异构设备场景开发基于一维搜索和交替优化的低复杂度算法。

Result: 仿真结果验证了收敛分析的准确性，证明了所提算法在优化数据采集时间和训练轮次方面的有效性。

Conclusion: 该研究为在实际条件下设计能效优化的边缘AI系统提供了有价值的见解和有效的优化方法。

Abstract: Edge AI, which brings artificial intelligence to the edge of the network for
real-time processing and decision-making, has emerged as a transformative
technology across various applications. However, the deployment of Edge AI
systems faces significant challenges due to high energy consumption and
extended operation time.
  In this paper, we consider an Edge AI system which integrates the data
acquisition, computation and communication processes, and focus on improving
learning performance of this system. We model the time and energy consumption
of different processes and perform a rigorous convergence analysis to quantify
the impact of key system parameters, such as the amount of collected data and
the number of training rounds, on the learning performance. Based on this
analysis, we formulate a system-wide optimization problem that seeks to
maximize learning performance under given time and energy constraints. We
explore both homogeneous and heterogeneous device scenarios, developing
low-complexity algorithms based on one-dimensional search and alternating
optimization to jointly optimize data collection time and training rounds.
Simulation results validate the accuracy of our convergence analysis and
demonstrate the effectiveness of the proposed algorithms, providing valuable
insights into designing energy-efficient Edge AI systems under real-world
conditions.

</details>


### [56] [Real-Time Diverse Fiber Sensing Multi-Event Detection using Phase OTDR Measurements](https://arxiv.org/abs/2511.06922)
*Konstantinos Alexoudis,Jasper Müller,Sai Kireet Patri,Vincent A. J. M. Sleiffer,Vishal Chandraprakash Rai,André Sandmann,Sander Jansen,Thomas Bradley,Chigo Okonkwo*

Main category: eess.SP

TL;DR: 开发了一种实验性相位OTDR系统，能够实时检测和分类多种环境事件，包括风力引起的光纤移动、车辆移动和音频特征，并提供实时可视化。


<details>
  <summary>Details</summary>
Motivation: 传统OTDR系统在环境事件检测和分类方面的能力有限，需要开发能够同时检测多种事件类型并提供实时反馈的系统。

Method: 采用相位光学时域反射测量技术，构建实验系统实现环境事件的同步检测和分类。

Result: 系统成功实现了对风力引起的光纤移动、车辆移动和音频特征等环境事件的实时检测和分类，并提供了可视化功能。

Conclusion: 该相位OTDR系统为环境监测提供了一种有效的实时检测和分类解决方案，具有实际应用价值。

Abstract: We demonstrate an experimental phase optical time-domain reflectometry (OTDR)
system capable of simultaneous detection and classification of various
environmental events, such as wind-induced fiber movement, vehicle movement,
and audio signatures, with real-time visualization.

</details>


### [57] [Trajectory Design for UAV-Assisted Logistics Collection in Low-Altitude Economy](https://arxiv.org/abs/2511.07178)
*Zhiyuan Zhai,Yuan Gao,Wei Ni,Xiaojun Yuan,Xin Wang*

Main category: eess.SP

TL;DR: 本文提出了一种结合LKH和DDPG的新算法，用于优化低空经济中无人机物流收集任务的轨迹规划，相比基线方法可减少约49%的收集时间。


<details>
  <summary>Details</summary>
Motivation: 低空经济中无人机物流收集任务需要在复杂三维环境中飞行并避开障碍物，而传统基于自由空间条件的轨迹设计方法不适用，因此需要开发新的优化算法。

Method: 结合Lin-Kernighan-Helsgaun(LKH)算法和Deep Deterministic Policy Gradient(DDPG)方法，LKH确定最优物品收集顺序，DDPG设计收集点间的飞行轨迹。

Result: 仿真实验表明，提出的LKH-DDPG算法相比基线方法显著减少了约49%的收集时间。

Conclusion: 该算法在低空经济范式中有效优化了无人机轨迹，提升了物流收集任务的运营效率。

Abstract: Low-altitude economy (LAE) is rapidly emerging as a key driver of innovation,
encompassing economic activities taking place in airspace below 500 meters.
Unmanned aerial vehicles (UAVs) provide valuable tools for logistics collection
within LAE systems, offering the ability to navigate through complex
environments, avoid obstacles, and improve operational efficiency. However,
logistics collection tasks involve UAVs flying through complex
three-dimensional (3D) environments while avoiding obstacles, where traditional
UAV trajectory design methods,typically developed under free-space conditions
without explicitly accounting for obstacles, are not applicable. This paper
presents, we propose a novel algorithm that combines the Lin-Kernighan-Helsgaun
(LKH) and Deep Deterministic Policy Gradient (DDPG) methods to minimize the
total collection time. Specifically, the LKH algorithm determines the optimal
order of item collection, while the DDPG algorithm designs the flight
trajectory between collection points. Simulations demonstrate that the proposed
LKH-DDPG algorithm significantly reduces collection time by approximately 49
percent compared to baseline approaches, thereby highlighting its effectiveness
in optimizing UAV trajectories and enhancing operational efficiency for
logistics collection tasks in the LAE paradigm.

</details>


### [58] [Enhanced GCD through ORBGRAND-AI: Exploiting Partial and Total Correlation in Noise](https://arxiv.org/abs/2511.07376)
*Jiewei Feng,Ken R. Duffy,Muriel Médard*

Main category: eess.SP

TL;DR: 本文提出了一种基于ORBGRAND-AI的猜测码字解码方法，通过利用信道相关性来提升解码精度，在保持较低查询模式数量的同时实现了约0.75 dB的BLER改进。


<details>
  <summary>Details</summary>
Motivation: 现有软判决解码器大多假设比特在信道中独立受影响，但实际信道存在相关性。ORBGRAND-AI已证明可以利用信道相关性提升解码精度，本文旨在进一步将ORBGRAND-AI集成到猜测码字解码中。

Method: 采用两种方法：直接集成ORBGRAND-AI作为GCD的模式生成器，以及更精细地利用总相关性来优化解码性能。

Result: 直接方法在减少查询模式数量的同时BLER略有下降；精细方法在保持减少查询数量的同时实现了约0.75 dB的BLER改进。

Conclusion: 通过将ORBGRAND-AI与GCD结合并利用信道相关性，可以在减少计算复杂度的同时显著提升解码性能。

Abstract: There have been significant advances in recent years in the development of
forward error correction decoders that can decode codes of any structure,
including practical realizations in synthesized circuits and taped out chips.
While essentially all soft-decision decoders assume that bits have been
impacted independently on the channel, for one of these new approaches it has
been established that channel dependencies can be exploited to achieve superior
decoding accuracy, resulting in Ordered Reliability Bits Guessing Random
Additive Noise Decoding Approximate Independence (ORBGRAND-AI). Building on
that capability, here we consider the integration of ORBGRAND-AI as a pattern
generator for Guessing Codeword Decoding (GCD). We first establish that a
direct approach delivers mildly degraded block error rate (BLER) but with
reduced number of queried patterns when compared to ORBGRAND-AI. We then show
that with a more nuanced approach it is possible to leverage total correlation
to deliver an additional BLER improvement of around 0.75 dB while retaining
reduced query numbers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [AGRAG: Advanced Graph-based Retrieval-Augmented Generation for LLMs](https://arxiv.org/abs/2511.05549)
*Yubo Wang,Haoyang Li,Fei Teng,Lei Chen*

Main category: cs.LG

TL;DR: AGRAG是一个先进的基于图的检索增强生成框架，通过统计方法构建图避免LLM幻觉，使用最小成本最大影响力子图生成算法提供显式推理路径，提升LLM的推理能力和答案完整性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于图的RAG方法面临的三个关键挑战：不准确的图构建（由LLM幻觉引起）、推理能力差（缺乏显式推理路径）和答案不完整（LLM推理不足），这些导致在某些任务上性能甚至不如NaiveRAG。

Method: 1）用基于统计的方法替代LLM实体提取来构建图；2）将图推理过程建模为最小成本最大影响力子图生成问题，提出贪心算法求解；3）生成的MCMI子图作为显式推理路径指导LLM。

Result: AGRAG避免了LLM幻觉和错误传播，生成的推理路径更全面（支持复杂图结构如环），使LLM能更好关注查询相关内容，减少噪声影响，提升推理能力。

Conclusion: AGRAG通过改进图构建方法和引入MCMI子图生成算法，有效解决了现有图基RAG方法的局限性，显著提升了检索增强生成的性能和推理能力。

Abstract: Graph-based retrieval-augmented generation (Graph-based RAG) has demonstrated
significant potential in enhancing Large Language Models (LLMs) with structured
knowledge. However, existing methods face three critical challenges: Inaccurate
Graph Construction, caused by LLM hallucination; Poor Reasoning Ability, caused
by failing to generate explicit reasons telling LLM why certain chunks were
selected; and Inadequate Answering, which only partially answers the query due
to the inadequate LLM reasoning, making their performance lag behind NaiveRAG
on certain tasks. To address these issues, we propose AGRAG, an advanced
graph-based retrieval-augmented generation framework. When constructing the
graph, AGRAG substitutes the widely used LLM entity extraction method with a
statistics-based method, avoiding hallucination and error propagation. When
retrieval, AGRAG formulates the graph reasoning procedure as the Minimum Cost
Maximum Influence (MCMI) subgraph generation problem, where we try to include
more nodes with high influence score, but with less involving edge cost, to
make the generated reasoning paths more comprehensive. We prove this problem to
be NP-hard, and propose a greedy algorithm to solve it. The MCMI subgraph
generated can serve as explicit reasoning paths to tell LLM why certain chunks
were retrieved, thereby making the LLM better focus on the query-related part
contents of the chunks, reducing the impact of noise, and improving AGRAG's
reasoning ability. Furthermore, compared with the simple tree-structured
reasoning paths, our MCMI subgraph can allow more complex graph structures,
such as cycles, and improve the comprehensiveness of the generated reasoning
paths.

</details>


### [60] [Diversified Flow Matching with Translation Identifiability](https://arxiv.org/abs/2511.05558)
*Sagar Shrestha,Xiao Fu*

Main category: cs.LG

TL;DR: 本文提出了多样化流匹配（DFM），这是一个基于ODE的框架，用于解决多样化分布匹配（DDM）问题。DFM通过定制化的双层优化训练损失、非线性插值和结构重构，实现了翻译可识别性，是首个保证翻译可识别性的基于ODE的方法。


<details>
  <summary>Details</summary>
Motivation: DDM虽然解决了无配对域翻译中的内容错位问题并实现了翻译可识别性，但仅能通过GAN实现。GAN训练不稳定且无法提供传输轨迹信息，而轨迹信息在单细胞进化分析和机器人路径规划等应用中非常有用。

Method: 提出了多样化流匹配（DFM）框架，采用定制化的双层优化训练损失、非线性插值和结构重构，将流匹配（FM）适应到DDM的约束中，学习翻译函数的速度而非翻译函数本身。

Result: 在合成和真实世界数据集上的实验验证了所提出方法的有效性。

Conclusion: DFM是首个基于ODE的方法，能够保证翻译可识别性，为需要传输轨迹信息的应用提供了稳定的解决方案。

Abstract: Diversified distribution matching (DDM) finds a unified translation function
mapping a diverse collection of conditional source distributions to their
target counterparts. DDM was proposed to resolve content misalignment issues in
unpaired domain translation, achieving translation identifiability. However,
DDM has only been implemented using GANs due to its constraints on the
translation function. GANs are often unstable to train and do not provide the
transport trajectory information -- yet such trajectories are useful in
applications such as single-cell evolution analysis and robot route planning.
This work introduces diversified flow matching (DFM), an ODE-based framework
for DDM. Adapting flow matching (FM) to enforce a unified translation function
as in DDM is challenging, as FM learns the translation function's velocity
rather than the translation function itself. A custom bilevel
optimization-based training loss, a nonlinear interpolant, and a structural
reformulation are proposed to address these challenges, offering a tangible
implementation. To our knowledge, DFM is the first ODE-based approach
guaranteeing translation identifiability. Experiments on synthetic and
real-world datasets validate the proposed method.

</details>


### [61] [Effective Test-Time Scaling of Discrete Diffusion through Iterative Refinement](https://arxiv.org/abs/2511.05562)
*Sanghyun Lee,Sunwoo Kim,Seungryong Kim,Jongho Park,Dongmin Park*

Main category: cs.LG

TL;DR: 提出了Iterative Reward-Guided Refinement (IterRef)方法，这是一种针对离散扩散模型的测试时缩放方法，通过奖励引导的噪声-去噪转换逐步优化未对齐的中间状态。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放通过奖励引导生成在离散扩散模型中尚未充分探索，但具有作为有前景替代方案的潜力。

Method: 在Multiple-Try Metropolis (MTM)框架内形式化该过程，利用奖励引导的噪声-去噪转换逐步细化未对齐的中间状态，证明其收敛到奖励对齐分布。

Result: 在文本和图像领域的多种离散扩散模型上评估IterRef，观察到奖励引导生成质量的一致改进，特别是在低计算预算下取得了显著增益，远超先前最先进的基线方法。

Conclusion: IterRef是一种有效的测试时缩放方法，能够通过逐步优化中间状态显著提升离散扩散模型的奖励引导生成质量。

Abstract: Test-time scaling through reward-guided generation remains largely unexplored
for discrete diffusion models despite its potential as a promising alternative.
In this work, we introduce Iterative Reward-Guided Refinement (IterRef), a
novel test-time scaling method tailored to discrete diffusion that leverages
reward- guided noising-denoising transitions to progressively refine misaligned
intermediate states. We formalize this process within a Multiple-Try Metropolis
(MTM) framework, proving convergence to the reward-aligned distribution. Unlike
prior methods that assume the current state is already aligned with the reward
distribution and only guide the subsequent transition, our approach explicitly
refines each state in situ, progressively steering it toward the optimal
intermediate distribution. Across both text and image domains, we evaluate
IterRef on diverse discrete diffusion models and observe consistent
improvements in reward-guided generation quality. In particular, IterRef
achieves striking gains under low compute budgets, far surpassing prior
state-of-the-art baselines.

</details>


### [62] [Lookahead Unmasking Elicits Accurate Decoding in Diffusion Language Models](https://arxiv.org/abs/2511.05563)
*Sanghyun Lee,Seungryong Kim,Jongho Park,Dongmin Park*

Main category: cs.LG

TL;DR: 本文提出了Lookahead Unmasking (LookUM)方法，通过重新定义采样为在所有可能解掩码顺序上的路径选择，解决了传统置信度采样方法的短视问题，无需外部奖励模型即可实现更优的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的掩码扩散模型在推理时依赖解掩码顺序，但主流启发式方法如置信度采样存在短视问题：仅局部优化、无法利用额外计算资源、早期解码错误会级联传播。

Method: LookUM框架包含：(i)路径生成器，通过从解掩码集合池中采样来提出路径；(ii)验证器，计算所提路径的不确定性并进行重要性采样来选择最终路径。该方法利用错误解掩码会增加序列级不确定性的特性来避免错误轨迹。

Result: 在数学、规划和编程等六个基准测试中均表现出持续性能提升。仅需2-3条路径即可达到峰值性能，路径选择效率极高。在LLaDA和LLaDA 1.5上都实现了显著改进：基础LLaDA配合LookUM可与RL调优的LLaDA 1.5相媲美，而LookUM进一步提升了LLaDA 1.5的性能。

Conclusion: 基于不确定性的验证为强化学习提供了正交优势，证明了该框架的通用性。不确定性验证与强化学习具有互补性，LookUM方法在掩码扩散模型推理中具有重要价值。

Abstract: Masked Diffusion Models (MDMs) as language models generate by iteratively
unmasking tokens, yet their performance crucially depends on the inference time
order of unmasking. Prevailing heuristics, such as confidence based sampling,
are myopic: they optimize locally, fail to leverage extra test-time compute,
and let early decoding mistakes cascade. We propose Lookahead Unmasking
(LookUM), which addresses these concerns by reformulating sampling as path
selection over all possible unmasking orders without the need for an external
reward model. Our framework couples (i) a path generator that proposes paths by
sampling from pools of unmasking sets with (ii) a verifier that computes the
uncertainty of the proposed paths and performs importance sampling to
subsequently select the final paths. Empirically, erroneous unmasking
measurably inflates sequence level uncertainty, and our method exploits this to
avoid error-prone trajectories. We validate our framework across six
benchmarks, such as mathematics, planning, and coding, and demonstrate
consistent performance improvements. LookUM requires only two to three paths to
achieve peak performance, demonstrating remarkably efficient path selection.
The consistent improvements on both LLaDA and post-trained LLaDA 1.5 are
particularly striking: base LLaDA with LookUM rivals the performance of
RL-tuned LLaDA 1.5, while LookUM further enhances LLaDA 1.5 itself showing that
uncertainty based verification provides orthogonal benefits to reinforcement
learning and underscoring the versatility of our framework. Code will be
publicly released.

</details>


### [63] [Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction](https://arxiv.org/abs/2511.05577)
*An Vuong,Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 本研究通过指令调优对视觉语言模型进行微调，用于聚合物属性预测，展示了多模态学习在材料科学中的优势。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在科学领域（如材料科学）的有效性仍然有限，缺乏专门用于聚合物属性预测等多模态任务的基础模型。

Method: 构建多模态聚合物数据集，通过指令调优对视觉语言模型进行微调，使用LoRA方法。

Result: 微调后的模型在性能上优于单模态和基线方法，证明了多模态学习的益处。

Conclusion: 该方法减少了为不同属性训练单独模型的需求，降低了部署和维护成本。

Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like
visual question answering and multimodal text generation, but their
effectiveness in scientific domains such as materials science remains limited.
While some machine learning methods have addressed specific challenges in this
field, there is still a lack of foundation models designed for broad tasks like
polymer property prediction using multimodal data. In this work, we present a
multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs
and assess the impact of multimodality on prediction performance. Our
fine-tuned models, using LoRA, outperform unimodal and baseline approaches,
demonstrating the benefits of multimodal learning. Additionally, this approach
reduces the need to train separate models for different properties, lowering
deployment and maintenance costs.

</details>


### [64] [Distillation-Accelerated Uncertainty Modeling for Multi-Objective RTA Interception](https://arxiv.org/abs/2511.05582)
*Gaoxiang Zhao,Ruina Qiu,Pengpeng Zhao,Rongjin Wang,Zhangang Lin,Xiaoqiang Wang*

Main category: cs.LG

TL;DR: DAUM是一个实时竞价拦截框架，通过集成多目标学习和不确定性建模来预测流量质量并提供可靠的置信度估计，同时使用知识蒸馏技术降低计算开销，在保持预测准确性的同时实现10倍推理速度提升。


<details>
  <summary>Details</summary>
Motivation: 解决实时竞价拦截中的两个关键挑战：需要准确估计流量质量并提供高置信度的预测（通常通过不确定性建模解决），以及不确定性建模在实时应用中引入的效率瓶颈问题。

Method: 提出了DAUM联合建模框架，集成多目标学习和不确定性建模；在此基础上应用知识蒸馏技术来减少不确定性建模的计算开销。

Result: 在JD广告数据集上的实验表明，DAUM持续提升了预测性能，蒸馏后的模型实现了10倍的推理速度提升。

Conclusion: DAUM框架有效解决了实时竞价拦截中的准确性和效率问题，通过联合建模和知识蒸馏技术实现了高性能和高效率的平衡。

Abstract: Real-Time Auction (RTA) Interception aims to filter out invalid or irrelevant
traffic to enhance the integrity and reliability of downstream data. However,
two key challenges remain: (i) the need for accurate estimation of traffic
quality together with sufficiently high confidence in the model's predictions,
typically addressed through uncertainty modeling, and (ii) the efficiency
bottlenecks that such uncertainty modeling introduces in real-time applications
due to repeated inference. To address these challenges, we propose DAUM, a
joint modeling framework that integrates multi-objective learning with
uncertainty modeling, yielding both traffic quality predictions and reliable
confidence estimates. Building on DAUM, we further apply knowledge distillation
to reduce the computational overhead of uncertainty modeling, while largely
preserving predictive accuracy and retaining the benefits of uncertainty
estimation. Experiments on the JD advertisement dataset demonstrate that DAUM
consistently improves predictive performance, with the distilled model
delivering a tenfold increase in inference speed.

</details>


### [65] [Prompting Neural-Guided Equation Discovery Based on Residuals](https://arxiv.org/abs/2511.05586)
*Jannis Brugger,Viktor Pfanschilling,David Richter,Mira Mezini,Stefan Kramer*

Main category: cs.LG

TL;DR: 提出RED方法，通过分析初始方程的残差来改进方程发现系统，无需重新搜索即可获得更好的方程建议。


<details>
  <summary>Details</summary>
Motivation: 现有神经引导方程发现系统在预测方程不符合用户期望时，缺乏有效方法获得其他方程建议，需要大量人工干预。

Method: 将初始方程解析为语法树，使用基于节点的计算规则计算每个子方程的残差，然后将残差作为新目标变量生成新提示，用更好的子方程替换原有子方程。

Result: 在Feynman基准测试的53个方程上，RED方法不仅改进了所有测试的神经引导系统，也改进了所有经典遗传编程系统。

Conclusion: RED是一种快速计算、易于扩展的后处理方法，可与任何方程发现系统配合使用，有效提高方程发现质量。

Abstract: Neural-guided equation discovery systems use a data set as prompt and predict
an equation that describes the data set without extensive search. However, if
the equation does not meet the user's expectations, there are few options for
getting other equation suggestions without intensive work with the system. To
fill this gap, we propose Residuals for Equation Discovery (RED), a
post-processing method that improves a given equation in a targeted manner,
based on its residuals. By parsing the initial equation to a syntax tree, we
can use node-based calculation rules to compute the residual for each
subequation of the initial equation. It is then possible to use this residual
as new target variable in the original data set and generate a new prompt. If,
with the new prompt, the equation discovery system suggests a subequation
better than the old subequation on a validation set, we replace the latter by
the former. RED is usable with any equation discovery system, is fast to
calculate, and is easy to extend for new mathematical operations. In
experiments on 53 equations from the Feynman benchmark, we show that it not
only helps to improve all tested neural-guided systems, but also all tested
classical genetic programming systems.

</details>


### [66] [CoPRIS: Efficient and Stable Reinforcement Learning via Concurrency-Controlled Partial Rollout with Importance Sampling](https://arxiv.org/abs/2511.05589)
*Zekai Qu,Yinxu Pan,Ao Sun,Chaojun Xiao,Xu Han*

Main category: cs.LG

TL;DR: CoPRIS提出了一种异步强化学习训练方法，通过控制并发rollout数量、提前终止和重用未完成轨迹来解决传统同步RL系统中的效率问题，在数学推理基准测试中实现了1.94倍加速且性能相当或更优。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的RL训练系统采用完全同步方式，需要等待整个批次rollout完成，导致效率低下，特别是长轨迹会阻塞整个rollout过程并使GPU空闲。

Method: 提出并发控制部分rollout与重要性采样（CoPRIS），通过维护固定数量的并发rollout、提前终止收集足够样本、重用未完成轨迹，并引入跨阶段重要性采样校正来处理离策略轨迹。

Result: 在具有挑战性的数学推理基准测试中，CoPRIS实现了高达1.94倍的训练加速，同时保持了与同步RL系统相当或更优的性能。

Conclusion: CoPRIS有效解决了LLM RL训练中的长尾效率问题，通过异步rollout和重要性采样校正实现了显著加速而不牺牲性能。

Abstract: Reinforcement learning (RL) post-training has become a trending paradigm for
enhancing the capabilities of large language models (LLMs). Most existing RL
systems for LLMs operate in a fully synchronous manner, where training must
wait for the rollout of an entire batch to complete. This design leads to
severe inefficiencies, as extremely long trajectories can stall the entire
rollout process and leave many GPUs idle. To address this issue, we propose
Concurrency- Controlled Partial Rollout with Importance Sampling (CoPRIS),
which mitigates long-tail inefficiencies by maintaining a fixed number of
concurrent rollouts, early-terminating once sufficient samples are collected,
and reusing unfinished trajectories in subsequent rollouts. To mitigate the
impact of off-policy trajectories, we introduce Cross-stage Importance Sampling
Correction, which concatenates buffered log probabilities from the previous
policy with those recomputed under the current policy for importance sampling
correction. Experiments on challenging mathematical reasoning benchmarks show
that CoPRIS achieves up to 1.94x faster training while maintaining comparable
or superior performance to synchronous RL systems. The code of CoPRIS is
available at https://github.com/777pomingzi/CoPRIS.

</details>


### [67] [FedSparQ: Adaptive Sparse Quantization with Error Feedback for Robust & Efficient Federated Learning](https://arxiv.org/abs/2511.05591)
*Chaimaa Medjadji,Sadi Alawadi,Feras M. Awaysheh,Guilain Leduc,Sylvain Kubler,Yves Le Traon*

Main category: cs.LG

TL;DR: FedSparQ是一个轻量级联邦学习压缩框架，通过动态稀疏化、半精度量化和误差反馈来减少90%的通信开销，同时保持或提高模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时面临显著的通信开销问题，需要高效的压缩方法来解决带宽受限环境下的通信瓶颈。

Method: 采用自适应阈值动态稀疏化客户端梯度，对保留条目应用半精度量化，并集成误差反馈机制防止信息丢失，无需手动调整稀疏率或量化计划。

Result: 在IID和非IID数据分布下，FedSparQ相比FedAvg减少90%的通信字节，模型精度提高6%，收敛鲁棒性提升50%。

Conclusion: FedSparQ为带宽受限的联邦学习部署提供了实用、易部署的解决方案，并为自适应精度和隐私保护协议的未来扩展奠定了基础。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients while preserving data privacy by keeping raw data local.
However, FL suffers from significant communication overhead due to the frequent
exchange of high-dimensional model updates over constrained networks. In this
paper, we present FedSparQ, a lightweight compression framework that
dynamically sparsifies the gradient of each client through an adaptive
threshold, applies half-precision quanti- zation to retained entries and
integrates residuals from error feedback to prevent loss of information.
FedSparQ requires no manual tuning of sparsity rates or quantization schedules,
adapts seamlessly to both homogeneous and heterogeneous data distributions, and
is agnostic to model architecture. Through extensive empirical evaluation on
vision benchmarks under independent and identically distributed (IID) and
non-IID data, we show that FedSparQ substantially reduces communication
overhead (reducing by 90% of bytes sent compared to FedAvg) while preserving or
improving model accuracy (improving by 6% compared to FedAvg non-compressed
solution or to state-of-the- art compression models) and enhancing convergence
robustness (by 50%, compared to the other baselines). Our approach provides a
practical, easy-to-deploy solution for bandwidth- constrained federated
deployments and lays the groundwork for future extensions in adaptive precision
and privacy-preserving protocols.

</details>


### [68] [Gradient Projection onto Historical Descent Directions for Communication-Efficient Federated Learning](https://arxiv.org/abs/2511.05593)
*Arnaud Descours,Léonard Deroose,Jan Ramon*

Main category: cs.LG

TL;DR: 提出了两种联邦学习通信效率优化算法：ProjFL（适用于无偏压缩器）和ProjFL+EF（适用于有偏压缩器），通过在共享子空间投影梯度来显著降低通信成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中通信效率是重要瓶颈，特别是对于大规模模型，需要开发高效的通信压缩方法。

Method: 将本地梯度投影到由历史下降方向张成的共享客户端-服务器子空间，结合误差反馈机制处理压缩偏差。

Result: 在标准联邦学习分类基准测试中，两种算法在保持与现有基线相当准确度的同时，显著降低了通信成本。

Conclusion: ProjFL和ProjFL+EF是有效的联邦学习通信优化方法，在强凸、凸和非凸设置下均具有收敛保证。

Abstract: Federated Learning (FL) enables decentralized model training across multiple
clients while optionally preserving data privacy. However, communication
efficiency remains a critical bottleneck, particularly for large-scale models.
In this work, we introduce two complementary algorithms: ProjFL, designed for
unbiased compressors, and ProjFL+EF, tailored for biased compressors through an
Error Feedback mechanism. Both methods rely on projecting local gradients onto
a shared client-server subspace spanned by historical descent directions,
enabling efficient information exchange with minimal communication overhead. We
establish convergence guarantees for both algorithms under strongly convex,
convex, and non-convex settings. Empirical evaluations on standard FL
classification benchmarks with deep neural networks show that ProjFL and
ProjFL+EF achieve accuracy comparable to existing baselines while substantially
reducing communication costs.

</details>


### [69] [FlowNet: Modeling Dynamic Spatio-Temporal Systems via Flow Propagation](https://arxiv.org/abs/2511.05595)
*Yutong Feng,Xu Liu,Yutong Xia,Yuxuan Liang*

Main category: cs.LG

TL;DR: 提出了一种名为Spatio-Temporal Flow的物理启发范式，通过可量化的流转移来显式建模动态节点耦合，并设计了FlowNet架构，在三个真实世界系统的建模中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于图或注意力机制，依赖相似性驱动的连接假设，忽略了控制系统演化的不对称流交换。需要一种能够显式建模动态节点间流转移的方法。

Method: 设计了FlowNet架构，使用流令牌作为信息载体，通过流分配模块模拟源到目标的转移，确保状态重新分配符合守恒定律。采用自适应空间掩蔽模块动态调整交互半径，并通过级联架构增强可扩展性和非线性表示能力。

Result: 在三个真实世界系统的建模中，FlowNet在七个指标上显著优于现有最先进方法，验证了其效率和物理可解释性。

Conclusion: 建立了一种通过时空流交互建模复杂系统的原则性方法学，FlowNet的成功证明了显式建模流转移的重要性。

Abstract: Accurately modeling complex dynamic spatio-temporal systems requires
capturing flow-mediated interdependencies and context-sensitive interaction
dynamics. Existing methods, predominantly graph-based or attention-driven, rely
on similarity-driven connectivity assumptions, neglecting asymmetric flow
exchanges that govern system evolution. We propose Spatio-Temporal Flow, a
physics-inspired paradigm that explicitly models dynamic node couplings through
quantifiable flow transfers governed by conservation principles. Building on
this, we design FlowNet, a novel architecture leveraging flow tokens as
information carriers to simulate source-to-destination transfers via Flow
Allocation Modules, ensuring state redistribution aligns with conservation
laws. FlowNet dynamically adjusts the interaction radius through an Adaptive
Spatial Masking module, suppressing irrelevant noise while enabling
context-aware propagation. A cascaded architecture enhances scalability and
nonlinear representation capacity. Experiments demonstrate that FlowNet
significantly outperforms existing state-of-the-art approaches on seven metrics
in the modeling of three real-world systems, validating its efficiency and
physical interpretability. We establish a principled methodology for modeling
complex systems through spatio-temporal flow interactions.

</details>


### [70] [FiCABU: A Fisher-Based, Context-Adaptive Machine Unlearning Processor for Edge AI](https://arxiv.org/abs/2511.05605)
*Eun-Su Cho,Jongin Choi,Jeongmin Jin,Jae-Jin Lee,Woojoo Lee*

Main category: cs.LG

TL;DR: FiCABU是一种软硬件协同设计的机器遗忘方法，针对边缘AI处理器优化，通过上下文自适应遗忘和平衡阻尼技术，在保持保留准确性的同时显著减少计算和能耗。


<details>
  <summary>Details</summary>
Motivation: 受隐私法规和"被遗忘权"驱动，需要在边缘设备上实现机器遗忘，但传统的服务器中心化或重训练方法在计算和能源预算紧张的情况下不实用。

Method: 结合上下文自适应遗忘（从后端层开始编辑，达到目标遗忘后停止）和平衡阻尼（根据深度缩放阻尼强度以保持保留准确性），在RISC-V边缘AI处理器中实现完整的RTL设计，集成轻量级IP用于Fisher估计和阻尼。

Result: 在CIFAR-20和PinsFaceRecognition数据集上，FiCABU实现随机猜测遗忘准确性，同时匹配无重训练基线SSD的保留准确性，计算量减少高达87.52%（ResNet-18）和71.03%（ViT）。在INT8硬件原型上，进一步改善保留保护，能耗降至SSD基线的6.48%（CIFAR-20）和0.13%（PinsFaceRecognition）。

Conclusion: FiCABU证明后端优先、深度感知的遗忘方法可以在资源受限的边缘AI设备上实现实用且高效。

Abstract: Machine unlearning, driven by privacy regulations and the "right to be
forgotten", is increasingly needed at the edge, yet server-centric or
retraining-heavy methods are impractical under tight computation and energy
budgets. We present FiCABU (Fisher-based Context-Adaptive Balanced Unlearning),
a software-hardware co-design that brings unlearning to edge AI processors.
FiCABU combines (i) Context-Adaptive Unlearning, which begins edits from
back-end layers and halts once the target forgetting is reached, with (ii)
Balanced Dampening, which scales dampening strength by depth to preserve retain
accuracy. These methods are realized in a full RTL design of a RISC-V edge AI
processor that integrates two lightweight IPs for Fisher estimation and
dampening into a GEMM-centric streaming pipeline, validated on an FPGA
prototype and synthesized in 45 nm for power analysis. Across CIFAR-20 and
PinsFaceRecognition with ResNet-18 and ViT, FiCABU achieves random-guess forget
accuracy while matching the retraining-free Selective Synaptic Dampening (SSD)
baseline on retain accuracy, reducing computation by up to 87.52 percent
(ResNet-18) and 71.03 percent (ViT). On the INT8 hardware prototype, FiCABU
further improves retain preservation and reduces energy to 6.48 percent
(CIFAR-20) and 0.13 percent (PinsFaceRecognition) of the SSD baseline. In sum,
FiCABU demonstrates that back-end-first, depth-aware unlearning can be made
both practical and efficient for resource-constrained edge AI devices.

</details>


### [71] [An MLCommons Scientific Benchmarks Ontology](https://arxiv.org/abs/2511.05614)
*Ben Hawks,Gregor von Laszewski,Matthew D. Sinclair,Marco Colombo,Shivaram Venkataraman,Rutwik Jain,Yiwei Jiang,Nhan Tran,Geoffrey Fox*

Main category: cs.LG

TL;DR: 本文提出了一个科学机器学习基准测试的本体论，通过统一的社区驱动努力，将MLCommons生态系统扩展到物理、化学、材料科学、生物学、气候科学等领域，整合了分散的基准测试框架。


<details>
  <summary>Details</summary>
Motivation: 现有的科学机器学习基准测试工作缺乏标准化且相互隔离，这使得机器学习在关键科学用例中的创新应用更加分散，影响路径不清晰。

Method: 基于XAI-BENCH、FastML Science Benchmarks、PDEBench和SciMLBench等先前倡议，整合大量分散的基准测试和框架，形成单一的分类体系，并通过开放提交工作流程添加新基准测试。

Result: 开发了一个可扩展的架构，支持未来的科学和AI/ML模式，并提供了六类评级标准来识别高质量基准测试，使利益相关者能够选择满足其特定需求的基准测试。

Conclusion: MLCommons科学基准测试本体论为科学机器学习中的可重现、跨领域基准测试提供了标准化、可扩展的基础。

Abstract: Scientific machine learning research spans diverse domains and data
modalities, yet existing benchmark efforts remain siloed and lack
standardization. This makes novel and transformative applications of machine
learning to critical scientific use-cases more fragmented and less clear in
pathways to impact. This paper introduces an ontology for scientific
benchmarking developed through a unified, community-driven effort that extends
the MLCommons ecosystem to cover physics, chemistry, materials science,
biology, climate science, and more. Building on prior initiatives such as
XAI-BENCH, FastML Science Benchmarks, PDEBench, and the SciMLBench framework,
our effort consolidates a large set of disparate benchmarks and frameworks into
a single taxonomy of scientific, application, and system-level benchmarks. New
benchmarks can be added through an open submission workflow coordinated by the
MLCommons Science Working Group and evaluated against a six-category rating
rubric that promotes and identifies high-quality benchmarks, enabling
stakeholders to select benchmarks that meet their specific needs. The
architecture is extensible, supporting future scientific and AI/ML motifs, and
we discuss methods for identifying emerging computing patterns for unique
scientific workloads. The MLCommons Science Benchmarks Ontology provides a
standardized, scalable foundation for reproducible, cross-domain benchmarking
in scientific machine learning. A companion webpage for this work has also been
developed as the effort evolves: https://mlcommons-science.github.io/benchmark/

</details>


### [72] [Frequency Matters: When Time Series Foundation Models Fail Under Spectral Shift](https://arxiv.org/abs/2511.05619)
*Tianze Wang,Sofiane Ennadir,John Pertoft,Gabriela Zarzar Gandler,Lele Cao,Zineb Senane,Styliani Katsarou,Sahar Asadi,Axel Karlsson,Oleg Smirnov*

Main category: cs.LG

TL;DR: 时间序列基础模型在公共基准测试中表现良好，但在工业应用中存在泛化问题，主要原因是频谱偏移（下游任务与预训练中的主导频率成分不匹配）。


<details>
  <summary>Details</summary>
Motivation: 研究时间序列基础模型在工业环境中表现不佳的原因，特别是频谱不匹配对模型泛化能力的影响。

Method: 通过工业规模的移动游戏玩家参与度预测任务进行实证分析，并设计受控合成实验对比已见和未见频率带的信号表现。

Result: 时间序列基础模型在工业任务中表现不如领域适配基线模型，在频谱不匹配情况下出现系统性性能下降。

Conclusion: 频率感知对于稳健的时间序列基础模型部署至关重要，需要开发新的预训练和评估协议来明确考虑频谱多样性。

Abstract: Time series foundation models (TSFMs) have shown strong results on public
benchmarks, prompting comparisons to a "BERT moment" for time series. Their
effectiveness in industrial settings, however, remains uncertain. We examine
why TSFMs often struggle to generalize and highlight spectral shift (a mismatch
between the dominant frequency components in downstream tasks and those
represented during pretraining) as a key factor. We present evidence from an
industrial-scale player engagement prediction task in mobile gaming, where
TSFMs underperform domain-adapted baselines. To isolate the mechanism, we
design controlled synthetic experiments contrasting signals with seen versus
unseen frequency bands, observing systematic degradation under spectral
mismatch. These findings position frequency awareness as critical for robust
TSFM deployment and motivate new pretraining and evaluation protocols that
explicitly account for spectral diversity.

</details>


### [73] [Fooling Algorithms in Non-Stationary Bandits using Belief Inertia](https://arxiv.org/abs/2511.05620)
*Gal Mendelson,Eyal Tadmor*

Main category: cs.LG

TL;DR: 本文通过信念惯性论证，为分段平稳多臂老虎机问题建立了新的最坏情况遗憾下界，证明即使只有一个变化点，经典算法如Explore Then Commit、epsilon greedy和UCB也会遭受线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有分段平稳老虎机理论主要依赖稀疏采样论证，但这种方法存在局限性。作者希望引入一种基于信念惯性的新方法来建立更精确的最坏情况遗憾下界。

Method: 提出信念惯性论证，分析算法历史奖励平均值形成的经验信念如何产生惯性，抵抗变化点后的新证据。利用这种惯性构造对抗性实例来误导经典算法。

Result: 证明即使只有一个变化点，经典算法也会遭受线性遗憾，且具有相当大的常数因子。即使采用周期性重启策略，最坏情况遗憾仍然保持线性。

Conclusion: 信念惯性方法可以成为推导非平稳老虎机中尖锐下界的强大工具，揭示了经典算法在时间变化环境中的根本局限性。

Abstract: We study the problem of worst case regret in piecewise stationary multi armed
bandits. While the minimax theory for stationary bandits is well established,
understanding analogous limits in time-varying settings is challenging.
Existing lower bounds rely on what we refer to as infrequent sampling
arguments, where long intervals without exploration allow adversarial reward
changes that induce large regret.
  In this paper, we introduce a fundamentally different approach based on a
belief inertia argument. Our analysis captures how an algorithm's empirical
beliefs, encoded through historical reward averages, create momentum that
resists new evidence after a change. We show how this inertia can be exploited
to construct adversarial instances that mislead classical algorithms such as
Explore Then Commit, epsilon greedy, and UCB, causing them to suffer regret
that grows linearly with T and with a substantial constant factor, regardless
of how their parameters are tuned, even with a single change point.
  We extend the analysis to algorithms that periodically restart to handle non
stationarity and prove that, even then, the worst case regret remains linear in
T. Our results indicate that utilizing belief inertia can be a powerful method
for deriving sharp lower bounds in non stationary bandits.

</details>


### [74] [Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games](https://arxiv.org/abs/2511.05640)
*Hamza Virk,Sandro Amaglobeli,Zuhayr Syed*

Main category: cs.LG

TL;DR: 本文提出了Blind-IGT框架，解决了逆博弈理论中当智能体理性参数τ未知时的统计不可识别性问题，通过引入归一化约束联合恢复奖励参数θ和τ。


<details>
  <summary>Details</summary>
Motivation: 传统基于熵正则化量化响应均衡的逆博弈理论方法假设智能体理性参数τ已知，但当τ未知时会出现尺度模糊性，导致奖励参数θ和τ统计不可识别。

Method: 提出Blind-IGT统计框架，引入归一化约束解决尺度模糊性，使用归一化最小二乘估计器，并扩展到马尔可夫博弈场景。

Result: 建立了唯一识别的充要条件，证明了NLS估计器达到最优O(N^{-1/2})收敛率，在强可识别条件不满足时提供部分识别保证，在未知转移动态下仍表现良好。

Conclusion: Blind-IGT是首个能够联合恢复θ和τ的统计框架，解决了逆博弈理论中的基本尺度模糊问题，为未知理性参数场景提供了理论保证和实用方法。

Abstract: Inverse Game Theory (IGT) methods based on the entropy-regularized Quantal
Response Equilibrium (QRE) offer a tractable approach for competitive settings,
but critically assume the agents' rationality parameter (temperature $\tau$) is
known a priori. When $\tau$ is unknown, a fundamental scale ambiguity emerges
that couples $\tau$ with the reward parameters ($\theta$), making them
statistically unidentifiable. We introduce Blind-IGT, the first statistical
framework to jointly recover both $\theta$ and $\tau$ from observed behavior.
We analyze this bilinear inverse problem and establish necessary and sufficient
conditions for unique identification by introducing a normalization constraint
that resolves the scale ambiguity. We propose an efficient Normalized Least
Squares (NLS) estimator and prove it achieves the optimal
$\mathcal{O}(N^{-1/2})$ convergence rate for joint parameter recovery. When
strong identifiability conditions fail, we provide partial identification
guarantees through confidence set construction. We extend our framework to
Markov games and demonstrate optimal convergence rates with strong empirical
performance even when transition dynamics are unknown.

</details>


### [75] [QiVC-Net: Quantum-Inspired Variational Convolutional Network, with Application to Biosignal Classification](https://arxiv.org/abs/2511.05730)
*Amin Golnari,Jamileh Yousefi,Reza Moheimani,Saeid Sanei*

Main category: cs.LG

TL;DR: 本文提出了量子启发的变分卷积框架，通过量子旋转集成机制实现卷积权重的可微分低维子空间旋转，在生物信号分类任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决生物信号分析中高噪声、主体间变异性和数据不平衡等挑战，需要开发更表达性强、稳定且具有不确定性感知能力的卷积架构。

Method: 采用量子启发的旋转集成机制，对卷积权重进行可微分低维子空间旋转，模拟量子态演化过程，实现结构化不确定性建模。

Result: 在两个基准数据集上分别达到97.84%和97.89%的准确率，实现了最先进的性能表现。

Conclusion: QiVC框架在真实世界生物医学信号分析中展示了不确定性感知建模的潜力和通用性。

Abstract: This work introduces the quantum-inspired variational convolution (QiVC)
framework, a novel learning paradigm that integrates principles of
probabilistic inference, variational optimization, and quantum-inspired
transformations within convolutional architectures. The central innovation of
QiVC lies in its quantum-inspired rotated ensemble (QiRE) mechanism. QiRE
performs differentiable low-dimensional subspace rotations of convolutional
weights, analogously to quantum state evolution. This approach enables
structured uncertainty modeling while preserving the intrinsic geometry of the
parameter space, resulting in more expressive, stable, and uncertainty-aware
representations. To demonstrate its practical potential, the concept is
instantiated in a QiVC-based convolutional network (QiVC-Net) and evaluated in
the context of biosignal classification, focusing on phonocardiogram (PCG)
recordings, a challenging domain characterized by high noise, inter-subject
variability, and often imbalanced data. The proposed QiVC-Net integrates an
architecture in which the QiVC layer does not introduce additional parameters,
instead performing an ensemble rotation of the convolutional weights through a
structured mechanism ensuring robustness without added highly computational
burden. Experiments on two benchmark datasets, PhysioNet CinC 2016 and
PhysioNet CirCor DigiScope 2022, show that QiVC-Net achieves state-of-the-art
performance, reaching accuracies of 97.84% and 97.89%, respectively. These
findings highlight the versatility of the QiVC framework and its promise for
advancing uncertainty-aware modeling in real-world biomedical signal analysis.
The implementation of the QiVConv layer is openly available in GitHub.

</details>


### [76] [KLASS: KL-Guided Fast Inference in Masked Diffusion Models](https://arxiv.org/abs/2511.05664)
*Seo Hyun Kim,Sunwoo Hong,Hojung Jung,Youngrok Park,Se-Young Yun*

Main category: cs.LG

TL;DR: 本文提出了KL-Adaptive Stability Sampling (KLASS)方法，通过利用token级别的KL散度识别稳定高置信度预测，在无需额外模型训练的情况下，在每次迭代中同时解掩多个token，显著加速生成过程同时保持样本质量。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在各种任务中表现出色，但由于其迭代优化过程，推理速度往往受到缓慢且固定的采样速度的限制。

Method: KLASS方法利用token级别的KL散度来识别稳定、高置信度的预测，在每次迭代中同时解掩多个token，无需额外模型训练。

Result: 在推理基准测试中，KLASS实现了高达2.78倍的实时加速，同时性能优于标准贪婪解码，在基于扩散的采样器中达到最先进水平。在文本、图像和分子生成等多个领域验证了其有效性。

Conclusion: KLASS是一种快速有效的采样方法，可作为跨不同模型的通用采样器，在保持质量的同时显著加速生成过程。

Abstract: Masked diffusion models have demonstrated competitive results on various
tasks including language generation. However, due to its iterative refinement
process, the inference is often bottlenecked by slow and static sampling speed.
To overcome this problem, we introduce `KL-Adaptive Stability Sampling'
(KLASS), a fast yet effective sampling method that exploits token-level KL
divergence to identify stable, high-confidence predictions. By unmasking
multiple tokens in each iteration without any additional model training, our
approach speeds up generation significantly while maintaining sample quality.
On reasoning benchmarks, KLASS achieves up to $2.78\times$ wall-clock speedups
while improving performance over standard greedy decoding, attaining
state-of-the-art results among diffusion-based samplers. We further validate
KLASS across diverse domains, including text, image, and molecular generation,
showing its effectiveness as a broadly applicable sampler across different
models.

</details>


### [77] [Catching Contamination Before Generation: Spectral Kill Switches for Agents](https://arxiv.org/abs/2511.05804)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需额外训练的诊断方法，通过分析注意力机制产生的token图，在早期层计算高频能量比和谱熵两个光谱统计量，用于在智能体执行过程中实时检测上下文不一致性。


<details>
  <summary>Details</summary>
Motivation: 智能体语言模型的多步推理链可能因上下文不一致、检索错误或对抗性输入而损坏，事后评估为时已晚，因为错误会在检测前传播。

Method: 使用前向传播分析注意力诱导的token图，在早期层计算高频能量比和谱熵两个光谱统计量，基于双机制混合假设和单调似然比特性，采用单一阈值进行贝叶斯最优检测。

Result: 高频能量比在多个模型家族中表现出稳健的双峰性，能够在模型仍在处理文本时检测污染，决策开销低于1毫秒。

Conclusion: 该方法可作为内联安全监控器集成到检索增强智能体管道中，在错误提交到推理链之前实时检测上下文不一致性。

Abstract: Agentic language models compose multi step reasoning chains, yet intermediate
steps can be corrupted by inconsistent context, retrieval errors, or
adversarial inputs, which makes post hoc evaluation too late because errors
propagate before detection. We introduce a diagnostic that requires no
additional training and uses only the forward pass to emit a binary accept or
reject signal during agent execution. The method analyzes token graphs induced
by attention and computes two spectral statistics in early layers, namely the
high frequency energy ratio and spectral entropy. We formalize these signals,
establish invariances, and provide finite sample estimators with uncertainty
quantification. Under a two regime mixture assumption with a monotone
likelihood ratio property, we show that a single threshold on the high
frequency energy ratio is optimal in the Bayes sense for detecting context
inconsistency. Empirically, the high frequency energy ratio exhibits robust
bimodality during context verification across multiple model families, which
enables gating decisions with overhead below one millisecond on our hardware
and configurations. We demonstrate integration into retrieval augmented agent
pipelines and discuss deployment as an inline safety monitor. The approach
detects contamination while the model is still processing the text, before
errors commit to the reasoning chain.

</details>


### [78] [Distributionally Robust Self Paced Curriculum Reinforcement Learning](https://arxiv.org/abs/2511.05694)
*Anirudh Satheesh,Keenan Powell,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一种分布鲁棒自步课程强化学习方法，通过将鲁棒性预算作为连续课程来自适应调度，解决了传统DRRL中固定鲁棒性预算导致的性能与鲁棒性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统分布鲁棒强化学习固定鲁棒性预算ε会导致性能与鲁棒性之间的权衡：小值获得高名义性能但弱鲁棒性，大值导致不稳定和过于保守的策略。

Method: 将鲁棒性预算ε作为连续课程，根据智能体进展自适应调度鲁棒性预算，实现名义性能和鲁棒性能的平衡。

Result: 在多个环境中，DR-SPCRL不仅稳定了训练，还实现了更优的鲁棒性-性能权衡，在变化扰动下平均获得11.8%的回合回报提升，性能约为相应名义RL算法的1.9倍。

Conclusion: 自适应调度鲁棒性预算的方法能够有效解决传统DRRL的性能-鲁棒性权衡问题，实现更稳定和优越的训练结果。

Abstract: A central challenge in reinforcement learning is that policies trained in
controlled environments often fail under distribution shifts at deployment into
real-world environments. Distributionally Robust Reinforcement Learning (DRRL)
addresses this by optimizing for worst-case performance within an uncertainty
set defined by a robustness budget $\epsilon$. However, fixing $\epsilon$
results in a tradeoff between performance and robustness: small values yield
high nominal performance but weak robustness, while large values can result in
instability and overly conservative policies. We propose Distributionally
Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that
overcomes this limitation by treating $\epsilon$ as a continuous curriculum.
DR-SPCRL adaptively schedules the robustness budget according to the agent's
progress, enabling a balance between nominal and robust performance. Empirical
results across multiple environments demonstrate that DR-SPCRL not only
stabilizes training but also achieves a superior robustness-performance
trade-off, yielding an average 11.8\% increase in episodic return under varying
perturbations compared to fixed or heuristic scheduling strategies, and
achieving approximately 1.9$\times$ the performance of the corresponding
nominal RL algorithms.

</details>


### [79] [Learning Time-Varying Graph Signals via Koopman](https://arxiv.org/abs/2511.06493)
*Sivaram Krishnan,Jinho Choi,Jihong Park*

Main category: cs.LG

TL;DR: 提出基于Koopman自编码器的框架来处理时变图数据，通过图嵌入将图结构转换为向量时间序列，在潜在空间中学习非线性动态系统，实现图演化的预测和缺失数据重建。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多数据（如海洋测量数据、无人机轨迹）可以自然地表示为图结构，这些图结构会随时间演变。有效建模和分析这类动态图数据对于预测图演化和重建缺失图数据至关重要。

Method: 使用Koopman自编码器框架，首先通过图嵌入将时变图数据转换为向量时间序列，在有限维潜在空间中应用KAE学习控制图特征时间演化的非线性动态系统。

Result: 该框架能够捕捉图结构的演化，在潜在空间中学习非线性动态，从而实现对图演化的预测和缺失图数据的重建。

Conclusion: 基于Koopman自编码器的框架为处理时变图数据提供了一种有效方法，能够同时处理图结构的动态演化和特征学习，适用于预测和重建任务。

Abstract: A wide variety of real-world data, such as sea measurements, e.g.,
temperatures collected by distributed sensors and multiple unmanned aerial
vehicles (UAV) trajectories, can be naturally represented as graphs, often
exhibiting non-Euclidean structures. These graph representations may evolve
over time, forming time-varying graphs. Effectively modeling and analyzing such
dynamic graph data is critical for tasks like predicting graph evolution and
reconstructing missing graph data. In this paper, we propose a framework based
on the Koopman autoencoder (KAE) to handle time-varying graph data.
Specifically, we assume the existence of a hidden non-linear dynamical system,
where the state vector corresponds to the graph embedding of the time-varying
graph signals. To capture the evolving graph structures, the graph data is
first converted into a vector time series through graph embedding, representing
the structural information in a finite-dimensional latent space. In this latent
space, the KAE is applied to learn the underlying non-linear dynamics governing
the temporal evolution of graph features, enabling both prediction and
reconstruction tasks.

</details>


### [80] [AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening](https://arxiv.org/abs/2511.05696)
*Jacob T. Rosenthal,Emma Hahesy,Sulov Chalise,Menglei Zhu,Mert R. Sabuncu,Lior Z. Braunstein,Anyi Li*

Main category: cs.LG

TL;DR: MSK-MATCH是一个用于癌症临床试验自动资格筛查的AI系统，通过整合大语言模型和肿瘤学试验知识库，在乳腺癌试验中实现了61.9%的自动病例处理和98.6%的准确率。


<details>
  <summary>Details</summary>
Motivation: 临床试验在癌症治疗和研究中至关重要，但参与率仍然较低，需要更高效的筛查方法。

Method: 开发了MSK-MATCH系统，整合大语言模型与肿瘤学试验知识库，采用检索增强架构，为所有AI预测提供基于源文本的解释。

Result: 在731名患者的88,518份临床文档中，系统自动处理61.9%的病例，AI辅助工作流达到98.6%准确率、98.4%敏感性和98.7%特异性。对于需要人工审核的病例，AI生成的解释将筛查时间从20分钟减少到43秒。

Conclusion: MSK-MATCH在临床试验资格筛查中表现出色，能够显著提高效率并降低成本，为AI在临床决策支持中的应用提供了有力证据。

Abstract: Clinical trials play an important role in cancer care and research, yet
participation rates remain low. We developed MSK-MATCH (Memorial Sloan
Kettering Multi-Agent Trial Coordination Hub), an AI system for automated
eligibility screening from clinical text. MSK-MATCH integrates a large language
model with a curated oncology trial knowledge base and retrieval-augmented
architecture providing explanations for all AI predictions grounded in source
text. In a retrospective dataset of 88,518 clinical documents from 731 patients
across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of
cases and triaged 38.1% for human review. This AI-assisted workflow achieved
98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level
eligibility classification, matching or exceeding performance of the human-only
and AI-only comparisons. For the triaged cases requiring manual review,
prepopulating eligibility screens with AI-generated explanations reduced
screening time from 20 minutes to 43 seconds at an average cost of $0.96 per
patient-trial pair.

</details>


### [81] [Distributionally Robust Multimodal Machine Learning](https://arxiv.org/abs/2511.05716)
*Peilin Yang,Yu Ma*

Main category: cs.LG

TL;DR: 本文提出了一个分布鲁棒的多模态机器学习框架，通过理论分析和实验验证，为高风险应用中多模态模型的使用提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖特征级融合或启发式不确定性建模，忽略了模态感知效应且提供有限的理论见解，需要更系统的分布鲁棒优化方法。

Method: 提出新颖的分布鲁棒优化框架，进行复杂度分析，建立泛化上界和极小极大下界，并扩展到编码器特定误差传播的设置。

Result: 理论分析提供了性能保证，实验证明该方法在模拟设置和真实数据集上都能提高鲁棒性。

Conclusion: 这些发现为在不确定性不可避免的高风险应用中采用多模态机器学习模型提供了原则性基础。

Abstract: We consider the problem of distributionally robust multimodal machine
learning. Existing approaches often rely on merging modalities on the feature
level (early fusion) or heuristic uncertainty modeling, which downplays
modality-aware ef- fects and provide limited insights. We propose a novel
distributionally robust optimization (DRO) framework that aims to study both
the theoretical and practical insights of multimodal machine learning. We first
justify this setup and show the significance of this problem through complexity
analysis. We then establish both generalization upper bounds and minimax lower
bounds which provide perfor- mance guarantees. These results are further
extended in settings where we consider encoder-specific error propogations.
Empirically, we demonstrate that our approach improves robustness in both
simulation settings and real-world datasets. Together, these findings provide a
principled foundation for employing multimodal machine learning models in
high-stakes applications where uncertainty is unavoidable.

</details>


### [82] [Compressing Chemistry Reveals Functional Groups](https://arxiv.org/abs/2511.05728)
*Ruben Sharma,Ross D. King*

Main category: cs.LG

TL;DR: 本文首次对传统化学官能团在化学解释中的效用进行了大规模评估，基于计算学习理论的最小消息长度原则，开发了一种无监督学习算法来发现压缩生物相关分子的子结构。


<details>
  <summary>Details</summary>
Motivation: 评估传统化学官能团在化学解释中的实际效用，基于"好的解释应该能够压缩数据"这一计算学习理论原则。

Method: 使用基于最小消息长度(MML)原则的无监督学习算法，在约300万个生物相关分子中搜索能够压缩数据的子结构。

Result: 发现的子结构包含了大多数人工整理的官能团，以及具有更特定功能的新型大模式；在24个生物活性预测数据集上，基于数据集特定官能团的指纹表示显著优于其他指纹表示方法。

Conclusion: 基于最小消息长度原则发现的子结构能够有效压缩化学数据，包含传统官能团和新模式，在生物活性预测任务中表现出优越性能。

Abstract: We introduce the first formal large-scale assessment of the utility of
traditional chemical functional groups as used in chemical explanations. Our
assessment employs a fundamental principle from computational learning theory:
a good explanation of data should also compress the data. We introduce an
unsupervised learning algorithm based on the Minimum Message Length (MML)
principle that searches for substructures that compress around three million
biologically relevant molecules. We demonstrate that the discovered
substructures contain most human-curated functional groups as well as novel
larger patterns with more specific functions. We also run our algorithm on 24
specific bioactivity prediction datasets to discover dataset-specific
functional groups. Fingerprints constructed from dataset-specific functional
groups are shown to significantly outperform other fingerprint representations,
including the MACCS and Morgan fingerprint, when training ridge regression
models on bioactivity regression tasks.

</details>


### [83] [Near-Exponential Savings for Mean Estimation with Active Learning](https://arxiv.org/abs/2511.05736)
*Julian M. Morimoto,Jacob Goldin,Daniel E. Ho*

Main category: cs.LG

TL;DR: 本文提出了一种名为PartiBandits的主动学习算法，用于在标签数量有限的情况下高效估计k类随机变量的均值。该算法通过两阶段方法：第一阶段学习未标记数据的划分以减少Y的条件方差，第二阶段使用UCB风格的子程序从各层请求标签。


<details>
  <summary>Details</summary>
Motivation: 在标签数量有限但可获得辅助信息（协变量）的设置中，需要高效估计随机变量的均值。传统方法可能不够有效，因此需要开发能够利用协变量信息并最小化标签使用的新算法。

Method: PartiBandits是一个两阶段主动学习算法：1）学习未标记数据的划分以减小Y的平均条件方差；2）使用WarmStart-UCB子程序在各层中轮次请求标签。

Result: 算法产生的估计误差平方为$\tilde{\mathcal{O}}\left( \frac{\nu + \exp(c \cdot (-N/\log(N))) }{N} \right)$，其中$\nu$是贝叶斯最优分类器的风险。该收敛率在经典设置下达到极小极大最优。

Conclusion: PartiBandits算法在UCB和基于分歧的主动学习方法之间建立了桥梁，并在全国电子健康记录数据模拟中展示了有效性。该算法可通过R包PartiBandits实现。

Abstract: We study the problem of efficiently estimating the mean of a $k$-class random
variable, $Y$, using a limited number of labels, $N$, in settings where the
analyst has access to auxiliary information (i.e.: covariates) $X$ that may be
informative about $Y$. We propose an active learning algorithm ("PartiBandits")
to estimate $\mathbb{E}[Y]$. The algorithm yields an estimate,
$\widehat{\mu}_{\text{PB}}$, such that $\left( \widehat{\mu}_{\text{PB}} -
\mathbb{E}[Y]\right)^2$ is $\tilde{\mathcal{O}}\left( \frac{\nu + \exp(c \cdot
(-N/\log(N))) }{N} \right)$, where $c > 0$ is a constant and $\nu$ is the risk
of the Bayes-optimal classifier. PartiBandits is essentially a two-stage
algorithm. In the first stage, it learns a partition of the unlabeled data that
shrinks the average conditional variance of $Y$. In the second stage it uses a
UCB-style subroutine ("WarmStart-UCB") to request labels from each stratum
round-by-round. Both the main algorithm's and the subroutine's convergence
rates are minimax optimal in classical settings. PartiBandits bridges the UCB
and disagreement-based approaches to active learning despite these two
approaches being designed to tackle very different tasks. We illustrate our
methods through simulation using nationwide electronic health records. Our
methods can be implemented using the PartiBandits package in R.

</details>


### [84] [Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs](https://arxiv.org/abs/2511.05758)
*Anirudh Satheesh,Sooraj Sathish,Swetha Ganesh,Keenan Powell,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出了一个用于平均成本鲁棒约束马尔可夫决策过程的演员-评论家算法，解决了强对偶性缺失和鲁棒贝尔曼算子非压缩性等挑战，实现了ε可行性和ε最优性。


<details>
  <summary>Details</summary>
Motivation: 在鲁棒约束平均成本MDP中，强对偶性的缺失阻碍了标准对偶方法的应用，且平均成本设置下鲁棒贝尔曼算子不具有压缩性，需要新的解决方案。

Method: 开发了一个演员-评论家算法来处理平均成本RCMDP问题，该方法能够应对缺乏强对偶性和贝尔曼算子非压缩性的挑战。

Result: 算法实现了ε可行性和ε最优性，在有无松弛假设的情况下分别达到Õ(ε⁻⁴)和Õ(ε⁻⁶)的样本复杂度，与折扣设置相当。

Conclusion: 提出的演员-评论家算法成功解决了平均成本RCMDP中的关键挑战，为鲁棒约束强化学习提供了有效的解决方案。

Abstract: In this work, we study the problem of finding robust and safe policies in
Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key
challenge in this setting is the lack of strong duality, which prevents the
direct use of standard primal-dual methods for constrained RL. Additional
difficulties arise from the average-cost setting, where the Robust Bellman
operator is not a contraction under any norm. To address these challenges, we
propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our
method achieves both \(\epsilon\)-feasibility and \(\epsilon\)-optimality, and
we establish a sample complexities of \(\tilde{O}\left(\epsilon^{-4}\right)\)
and \(\tilde{O}\left(\epsilon^{-6}\right)\) with and without slackness
assumption, which is comparable to the discounted setting.

</details>


### [85] [An Efficient Gradient-Aware Error-Bounded Lossy Compressor for Federated Learning](https://arxiv.org/abs/2511.05770)
*Zhijing Ye,Sheng Di,Jiamin Wang,Zhiqing Zhong,Zhaorui Zhang,Xiaodong Yu*

Main category: cs.LG

TL;DR: 本文提出了一种针对联邦学习梯度数据的误差有损压缩框架，通过利用跨轮次的时间相关性和卷积核的结构规律性来提高压缩比，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的部署受到通信成本的限制，特别是在系统异构性下，低带宽客户端会成为性能瓶颈。现有的误差有损压缩方法（如SZ）原本为具有强空间局部性的平滑科学数据设计，对低平滑度和弱空间相关性的梯度张量压缩效果不佳。

Method: 提出了一种创新的预测机制，利用跨联邦学习训练轮次的时间相关性和卷积核内的结构规律性来减少残差熵。该预测器包括：(1)基于归一化指数移动平均的跨轮次幅度预测器；(2)利用梯度振荡和核级符号一致性的符号预测器。

Result: 实验表明，新方法比SZ3实现了高达1.53倍的压缩比提升，且精度损失更低。集成到真实联邦学习框架APPFL中，在各种受限带宽场景下将端到端通信时间减少了76.1%-96.2%。

Conclusion: 该误差有损压缩框架在保持模型精度的同时显著提高了联邦学习梯度数据的压缩效率，展示了在实际联邦学习部署中的强大可扩展性。

Abstract: Federated learning (FL) enables collaborative model training without exposing
clients' private data, but its deployment is often constrained by the
communication cost of transmitting gradients between clients and the central
server, especially under system heterogeneity where low-bandwidth clients
bottleneck overall performance. Lossy compression of gradient data can mitigate
this overhead, and error-bounded lossy compression (EBLC) is particularly
appealing for its fine-grained utility-compression tradeoff. However, existing
EBLC methods (e.g., SZ), originally designed for smooth scientific data with
strong spatial locality, rely on generic predictors such as Lorenzo and
interpolation for entropy reduction to improve compression ratio. Gradient
tensors, in contrast, exhibit low smoothness and weak spatial correlation,
rendering these predictors ineffective and leading to poor compression ratios.
To address this limitation, we propose an EBLC framework tailored for FL
gradient data to achieve high compression ratios while preserving model
accuracy. The core of it is an innovative prediction mechanism that exploits
temporal correlations across FL training rounds and structural regularities
within convolutional kernels to reduce residual entropy. The predictor is
compatible with standard quantizers and entropy coders and comprises (1) a
cross-round magnitude predictor based on a normalized exponential moving
average, and (2) a sign predictor that leverages gradient oscillation and
kernel-level sign consistency. Experiments show that this new EBLC yields up to
1.53x higher compression ratios than SZ3 with lower accuracy loss. Integrated
into a real-world FL framework, APPFL, it reduces end-to-end communication time
by 76.1%-96.2% under various constrained-bandwidth scenarios, demonstrating
strong scalability for real-world FL deployments.

</details>


### [86] [MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories](https://arxiv.org/abs/2511.05773)
*Zishuai Liu,Weihang You,Jin Lu,Fei Dou*

Main category: cs.LG

TL;DR: 提出MARAuder's Map框架，用于从原始未分割传感器流进行实时活动识别，通过将传感器激活投影到物理平面图来生成轨迹感知的图像序列，结合混合深度学习模型和可学习时间嵌入模块，在多个真实世界智能家居数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决智能家居中基于环境传感器的人类活动识别的挑战，包括实时推理需求、空间基础推理和上下文感知时间建模，现有方法依赖预分割数据和忽略物理环境布局，限制了在实际部署中的鲁棒性。

Method: 将传感器激活投影到物理平面图生成轨迹感知的图像序列，使用混合深度学习模型联合捕捉空间结构和时间依赖，引入可学习时间嵌入模块编码上下文线索，采用基于注意力的编码器选择性关注信息片段。

Result: 在多个真实世界智能家居数据集上的广泛实验表明，该方法优于强基线方法，为环境传感器环境中的实时人类活动识别提供了实用解决方案。

Conclusion: MARAuder's Map框架通过空间投影和时间建模的有效结合，显著提升了实时人类活动识别的性能，特别是在跨活动转换和时间模糊情况下的准确性。

Abstract: Ambient sensor-based human activity recognition (HAR) in smart homes remains
challenging due to the need for real-time inference, spatially grounded
reasoning, and context-aware temporal modeling. Existing approaches often rely
on pre-segmented, within-activity data and overlook the physical layout of the
environment, limiting their robustness in continuous, real-world deployments.
In this paper, we propose MARAuder's Map, a novel framework for real-time
activity recognition from raw, unsegmented sensor streams. Our method projects
sensor activations onto the physical floorplan to generate trajectory-aware,
image-like sequences that capture the spatial flow of human movement. These
representations are processed by a hybrid deep learning model that jointly
captures spatial structure and temporal dependencies. To enhance temporal
awareness, we introduce a learnable time embedding module that encodes
contextual cues such as hour-of-day and day-of-week. Additionally, an
attention-based encoder selectively focuses on informative segments within each
observation window, enabling accurate recognition even under cross-activity
transitions and temporal ambiguity. Extensive experiments on multiple
real-world smart home datasets demonstrate that our method outperforms strong
baselines, offering a practical solution for real-time HAR in ambient sensor
environments.

</details>


### [87] [Beyond the Lower Bound: Bridging Regret Minimization and Best Arm Identification in Lexicographic Bandits](https://arxiv.org/abs/2511.05802)
*Bo Xue,Yuanyu Wan,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: 本文研究了具有分层偏好的多目标决策问题，提出了两种基于消除的算法来同时处理遗憾最小化和最优臂识别任务，其中第二种算法通过跨目标信息共享超越了单目标老虎机问题的已知下界。


<details>
  <summary>Details</summary>
Motivation: 在多目标分层偏好决策中，现有研究主要关注遗憾最小化，本文旨在填补遗憾最小化和最优臂识别之间的空白，探索如何同时优化这两个目标。

Method: 提出了两种基于消除的算法：第一种按目标优先级逐层顺序消除次优臂；第二种在每轮中同时利用所有目标的奖励信息，有效利用跨目标依赖关系。

Result: 第一种算法实现了与最佳单目标算法相当的样本复杂度和遗憾界；第二种算法超越了单目标老虎机问题的已知下界，显示了多目标设置中跨目标信息共享的优势。

Conclusion: 实证结果验证了所提算法相对于基线的优越性能，证明了在多目标分层偏好设置中同时处理遗憾最小化和最优臂识别的可行性，特别是跨目标信息共享带来的显著收益。

Abstract: In multi-objective decision-making with hierarchical preferences,
lexicographic bandits provide a natural framework for optimizing multiple
objectives in a prioritized order. In this setting, a learner repeatedly
selects arms and observes reward vectors, aiming to maximize the reward for the
highest-priority objective, then the next, and so on. While previous studies
have primarily focused on regret minimization, this work bridges the gap
between \textit{regret minimization} and \textit{best arm identification} under
lexicographic preferences. We propose two elimination-based algorithms to
address this joint objective. The first algorithm eliminates suboptimal arms
sequentially, layer by layer, in accordance with the objective priorities, and
achieves sample complexity and regret bounds comparable to those of the best
single-objective algorithms. The second algorithm simultaneously leverages
reward information from all objectives in each round, effectively exploiting
cross-objective dependencies. Remarkably, it outperforms the known lower bound
for the single-objective bandit problem, highlighting the benefit of
cross-objective information sharing in the multi-objective setting. Empirical
results further validate their superior performance over baselines.

</details>


### [88] [Measuring Model Performance in the Presence of an Intervention](https://arxiv.org/abs/2511.05805)
*Winston Chen,Michael W. Sjoding,Jenna Wiens*

Main category: cs.LG

TL;DR: 本文提出了一种在随机对照试验(RCT)中利用所有数据进行无偏模型评估的方法，解决了传统方法忽略治疗组数据导致的效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 在AI社会影响应用中，干预措施会偏置模型评估结果。虽然RCT的控制组数据可用于无偏评估，但忽略治疗组数据会导致效率低下，而RCT通常成本高昂，需要充分利用数据。

Method: 提出了nuisance parameter weighting (NPW)方法，通过重新加权治疗组数据来模拟无干预情况下会或不会经历结果的样本分布，从而实现无偏模型评估。

Result: 在合成和真实数据集上的实验表明，相比忽略治疗组数据的标准方法，NPW方法在各种干预效应和样本量设置下都能产生更好的模型选择结果。

Conclusion: 该研究为实现现实场景中更高效的模型评估迈出了重要一步，能够充分利用RCT的所有数据资源。

Abstract: AI models are often evaluated based on their ability to predict the outcome
of interest. However, in many AI for social impact applications, the presence
of an intervention that affects the outcome can bias the evaluation. Randomized
controlled trials (RCTs) randomly assign interventions, allowing data from the
control group to be used for unbiased model evaluation. However, this approach
is inefficient because it ignores data from the treatment group. Given the
complexity and cost often associated with RCTs, making the most use of the data
is essential. Thus, we investigate model evaluation strategies that leverage
all data from an RCT. First, we theoretically quantify the estimation bias that
arises from na\"ively aggregating performance estimates from treatment and
control groups, and derive the condition under which this bias leads to
incorrect model selection. Leveraging these theoretical insights, we propose
nuisance parameter weighting (NPW), an unbiased model evaluation approach that
reweights data from the treatment group to mimic the distributions of samples
that would or would not experience the outcome under no intervention. Using
synthetic and real-world datasets, we demonstrate that our proposed evaluation
approach consistently yields better model selection than the standard approach,
which ignores data from the treatment group, across various intervention effect
and sample size settings. Our contribution represents a meaningful step towards
more efficient model evaluation in real-world contexts.

</details>


### [89] [MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling](https://arxiv.org/abs/2511.05811)
*Yu Zhang,Hui-Ling Zhen,Mingxuan Yuan,Bei Yu*

Main category: cs.LG

TL;DR: MOSS是一个新颖的FP8训练框架，通过两级微缩放策略和自动权重缩放技术，在保持训练精度的同时显著提升训练效率，使7B参数模型达到与BF16基线相当的性能，训练吞吐量提升高达34%。


<details>
  <summary>Details</summary>
Motivation: FP8格式训练大型语言模型能带来显著效率提升，但现有方法存在去量化开销大和在线量化效率低的问题，需要开发既能保证数值稳定性又高效的FP8训练框架。

Method: 提出MOSS框架，包含两个关键技术：(1)两级微缩放策略，结合高精度全局缩放和紧凑的2次幂局部缩放来量化敏感激活；(2)线性层权重自动缩放，通过预测和调整缩放因子避免昂贵的最大归约操作。

Result: MOSS成功实现了7B参数模型的高效FP8训练，性能与BF16基线相当，训练吞吐量提升高达34%。

Conclusion: MOSS框架通过创新的缩放策略解决了FP8训练中的效率和稳定性问题，为大规模语言模型训练提供了有效的低精度解决方案。

Abstract: Training large language models with FP8 formats offers significant efficiency
gains. However, the reduced numerical precision of FP8 poses challenges for
stable and accurate training. Current frameworks preserve training performance
using mixed-granularity quantization, i.e., applying per-group quantization for
activations and per-tensor/block quantization for weights. While effective,
per-group quantization requires scaling along the inner dimension of matrix
multiplication, introducing additional dequantization overhead. Moreover, these
frameworks often rely on just-in-time scaling to dynamically adjust scaling
factors based on the current data distribution. However, this online
quantization is inefficient for FP8 training, as it involves multiple memory
reads and writes that negate the performance benefits of FP8. To overcome these
limitations, we propose MOSS, a novel FP8 training framework that ensures both
efficiency and numerical stability. MOSS introduces two key innovations: (1) a
two-level microscaling strategy for quantizing sensitive activations, which
balances precision and dequantization cost by combining a high-precision global
scale with compact, power-of-two local scales; and (2) automatic scaling for
weights in linear layers, which eliminates the need for costly max-reduction
operations by predicting and adjusting scaling factors during training.
Leveraging these techniques, MOSS enables efficient FP8 training of a 7B
parameter model, achieving performance comparable to the BF16 baseline while
achieving up to 34% higher training throughput.

</details>


### [90] [AiEDA: An Open-Source AI-Aided Design Library for Design-to-Vector](https://arxiv.org/abs/2511.05823)
*Yihang Qiu,Zengrong Huang,Simin Tao,Hongda Zhang,Weiguo Li,Xinhua Lai,Rui Wang,Weiqiang Wang,Xingquan Li*

Main category: cs.LG

TL;DR: 本文提出了一个统一的EDA开源库AiEDA，解决了当前AI-EDA基础设施碎片化的问题，通过设计到向量的数据表示技术，建立了AI辅助设计范式，并生成了600GB的结构化数据集iDATA。


<details>
  <summary>Details</summary>
Motivation: 当前AI-EDA基础设施存在碎片化问题，缺乏从设计执行到AI集成的完整数据管道解决方案，包括碎片化的流程引擎、异构文件格式、非标准化数据提取方法和组织不良的数据存储。

Method: 开发了AiEDA统一开源库，集成多种设计到向量的数据表示技术，将芯片设计数据转换为通用多级向量表示，提供完整的物理设计流程和标准化Python接口。

Result: 基于AiEDA生成了600GB的iDATA数据集，包含50个真实芯片设计（28nm）的结构化数据，并在7个代表性AAD任务中验证了有效性。

Conclusion: AiEDA为未来AI-EDA研究提供了基础，代码已公开，完整iDATA数据集正在准备公开发布。

Abstract: Recent research has demonstrated that artificial intelligence (AI) can assist
electronic design automation (EDA) in improving both the quality and efficiency
of chip design. But current AI for EDA (AI-EDA) infrastructures remain
fragmented, lacking comprehensive solutions for the entire data pipeline from
design execution to AI integration. Key challenges include fragmented flow
engines that generate raw data, heterogeneous file formats for data exchange,
non-standardized data extraction methods, and poorly organized data storage.
This work introduces a unified open-source library for EDA (AiEDA) that
addresses these issues. AiEDA integrates multiple design-to-vector data
representation techniques that transform diverse chip design data into
universal multi-level vector representations, establishing an AI-aided design
(AAD) paradigm optimized for AI-EDA workflows. AiEDA provides complete physical
design flows with programmatic data extraction and standardized Python
interfaces bridging EDA datasets and AI frameworks. Leveraging the AiEDA
library, we generate iDATA, a 600GB dataset of structured data derived from 50
real chip designs (28nm), and validate its effectiveness through seven
representative AAD tasks spanning prediction, generation, optimization and
analysis. The code is publicly available at
https://github.com/OSCC-Project/AiEDA, while the full iDATA dataset is being
prepared for public release, providing a foundation for future AI-EDA research.

</details>


### [91] [CADM: Cluster-customized Adaptive Distance Metric for Categorical Data Clustering](https://arxiv.org/abs/2511.05826)
*Taixi Chen,Yiu-ming Cheung,Yiqun Zhang*

Main category: cs.LG

TL;DR: 提出了一种针对分类数据聚类的定制化距离度量方法，能够根据每个聚类中属性的不同分布自适应更新距离，并扩展到混合数据类型。


<details>
  <summary>Details</summary>
Motivation: 分类数据聚类中合适的距离度量至关重要，但现有方法未考虑不同聚类中属性值距离因分布不同而变化的特性，导致距离测量不合理。

Method: 提出聚类定制化距离度量，基于每个聚类中属性的不同分布竞争性地更新距离，并将该方法扩展到包含数值和分类属性的混合数据。

Result: 在14个数据集上的实验表明该方法效果显著，平均排名约为第一。

Conclusion: 所提出的聚类定制化距离度量方法在分类数据聚类中表现出色，能够有效处理不同聚类中属性分布差异带来的距离测量问题。

Abstract: An appropriate distance metric is crucial for categorical data clustering, as
the distance between categorical data cannot be directly calculated. However,
the distances between attribute values usually vary in different clusters
induced by their different distributions, which has not been taken into
account, thus leading to unreasonable distance measurement. Therefore, we
propose a cluster-customized distance metric for categorical data clustering,
which can competitively update distances based on different distributions of
attributes in each cluster. In addition, we extend the proposed distance metric
to the mixed data that contains both numerical and categorical attributes.
Experiments demonstrate the efficacy of the proposed method, i.e., achieving an
average ranking of around first in fourteen datasets. The source code is
available at https://anonymous.4open.science/r/CADM-47D8

</details>


### [92] [Predicting the Future by Retrieving the Past](https://arxiv.org/abs/2511.05859)
*Dazhao Du,Tao Han,Song Guo*

Main category: cs.LG

TL;DR: PFRP是一种新颖的单变量时间序列预测方法，通过构建全局记忆库和检索机制，显式整合全局历史数据来提升预测精度，相比仅依赖局部滑动窗口的传统模型性能提升8.4%。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型（如MLP、Transformer、TCN）在训练时虽然隐式压缩了历史信息到参数中，但在推理时只能依赖滑动窗口的局部上下文，无法显式动态访问全局历史知识，导致丰富的全局模式未被充分利用。

Method: 提出PFRP方法：1）构建全局记忆库（GMB）存储和管理全局历史模式；2）使用检索机制从GMB中提取相似模式生成全局预测；3）自适应地将全局预测与任何局部预测模型的输出相结合。

Result: 在7个真实世界数据集上的广泛实验表明，PFRP显著提升了先进单变量预测模型的平均性能达8.4%。

Conclusion: PFRP通过显式整合全局历史数据，能够产生更准确和可解释的预测，有效弥补了传统模型仅依赖局部上下文的局限性。

Abstract: Deep learning models such as MLP, Transformer, and TCN have achieved
remarkable success in univariate time series forecasting, typically relying on
sliding window samples from historical data for training. However, while these
models implicitly compress historical information into their parameters during
training, they are unable to explicitly and dynamically access this global
knowledge during inference, relying only on the local context within the
lookback window. This results in an underutilization of rich patterns from the
global history. To bridge this gap, we propose Predicting the Future by
Retrieving the Past (PFRP), a novel approach that explicitly integrates global
historical data to enhance forecasting accuracy. Specifically, we construct a
Global Memory Bank (GMB) to effectively store and manage global historical
patterns. A retrieval mechanism is then employed to extract similar patterns
from the GMB, enabling the generation of global predictions. By adaptively
combining these global predictions with the outputs of any local prediction
model, PFRP produces more accurate and interpretable forecasts. Extensive
experiments conducted on seven real-world datasets demonstrate that PFRP
significantly enhances the average performance of advanced univariate
forecasting models by 8.4\%. Codes can be found in
https://github.com/ddz16/PFRP.

</details>


### [93] [EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning](https://arxiv.org/abs/2511.05863)
*Yuning Chen,Sha Zhao,Shijian Li,Gang Pan*

Main category: cs.LG

TL;DR: EMOD是一个统一的EEG情感表示框架，利用效价-唤醒度引导的对比学习来解决跨数据集情感识别的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在单一EEG情感数据集上表现良好，但由于标注方案和数据格式的异质性，跨数据集的泛化能力有限。现有模型通常需要针对特定数据集的架构，缺乏跨不同情感标签的语义对齐。

Method: EMOD通过将离散和连续情感标签投影到统一的效价-唤醒度空间，并制定软加权监督对比损失，鼓励情感相似的样本在潜在空间中聚集。采用灵活的三域编码器和时空变换器架构，适应不同的EEG格式。

Result: 在八个公共EEG数据集上预训练，并在三个基准数据集上评估，EMOD实现了最先进的性能，展示了强大的适应性和泛化能力。

Conclusion: EMOD通过统一的V-A空间表示和对比学习，有效解决了EEG情感识别中的异构性问题，为跨数据集情感识别提供了强大的通用框架。

Abstract: Emotion recognition from EEG signals is essential for affective computing and
has been widely explored using deep learning. While recent deep learning
approaches have achieved strong performance on single EEG emotion datasets,
their generalization across datasets remains limited due to the heterogeneity
in annotation schemes and data formats. Existing models typically require
dataset-specific architectures tailored to input structure and lack semantic
alignment across diverse emotion labels. To address these challenges, we
propose EMOD: A Unified EEG Emotion Representation Framework Leveraging
Valence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable and
emotion-aware representations from heterogeneous datasets by bridging both
semantic and structural gaps. Specifically, we project discrete and continuous
emotion labels into a unified V-A space and formulate a soft-weighted
supervised contrastive loss that encourages emotionally similar samples to
cluster in the latent space. To accommodate variable EEG formats, EMOD employs
a flexible backbone comprising a Triple-Domain Encoder followed by a
Spatial-Temporal Transformer, enabling robust extraction and integration of
temporal, spectral, and spatial features. We pretrain EMOD on eight public EEG
datasets and evaluate its performance on three benchmark datasets. Experimental
results show that EMOD achieves state-of-the-art performance, demonstrating
strong adaptability and generalization across diverse EEG-based emotion
recognition scenarios.

</details>


### [94] [Adaptation and Fine-tuning with TabPFN for Travelling Salesman Problem](https://arxiv.org/abs/2511.05872)
*Nguyen Gia Hien Vu,Yifan Tang,Rey Lim,Yifan Yang,Hang Ma,Ke Wang,G. Gary Wang*

Main category: cs.LG

TL;DR: 本文研究了TabPFN在组合优化问题中的应用，特别是旅行商问题(TSP)，旨在减少传统方法的时间和数据密集型训练需求。


<details>
  <summary>Details</summary>
Motivation: 减轻传统组合优化方法（包括精确算法、启发式算法和基于机器学习的模型）在解决组合优化问题时面临的时间和数据密集型训练挑战。

Method: 采用基于节点的方法和节点预测适应策略来构建完整的TSP路径，对TabPFN模型进行适应和微调。

Result: TabPFN需要最少的训练，仅使用单个样本就能适应TSP，在不同TSP实例大小上表现出更好的泛化能力，减少了性能下降。训练过程在几分钟内完成，即使没有后处理也能获得强大的解决方案质量。

Conclusion: TabPFN模型是在训练资源受限和快速部署需求下，有效解决结构化和组合优化问题的有前景的方法。

Abstract: Tabular Prior-Data Fitted Network (TabPFN) is a foundation model designed for
small to medium-sized tabular data, which has attracted much attention
recently. This paper investigates the application of TabPFN in Combinatorial
Optimization (CO) problems. The aim is to lessen challenges in time and
data-intensive training requirements often observed in using traditional
methods including exact and heuristic algorithms, Machine Learning (ML)-based
models, to solve CO problems. Proposing possibly the first ever application of
TabPFN for such a purpose, we adapt and fine-tune the TabPFN model to solve the
Travelling Salesman Problem (TSP), one of the most well-known CO problems.
Specifically, we adopt the node-based approach and the node-predicting
adaptation strategy to construct the entire TSP route. Our evaluation with
varying instance sizes confirms that TabPFN requires minimal training, adapts
to TSP using a single sample, performs better generalization across varying TSP
instance sizes, and reduces performance degradation. Furthermore, the training
process with adaptation and fine-tuning is completed within minutes. The
methodology leads to strong solution quality even without post-processing and
achieves performance comparable to other models with post-processing
refinement. Our findings suggest that the TabPFN model is a promising approach
to solve structured and CO problems efficiently under training resource
constraints and rapid deployment requirements.

</details>


### [95] [From Kernels to Attention: A Transformer Framework for Density and Score Estimation](https://arxiv.org/abs/2511.05924)
*Vasily Ilin,Peter Sushko*

Main category: cs.LG

TL;DR: 提出基于注意力机制的联合分数和密度估计统一框架，使用置换和仿射等变transformer直接从样本中估计概率密度和分数，相比传统方法具有更好的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统分数匹配方法需要为每个分布训练单独模型，缺乏泛化能力。希望开发一个分布无关的算子，能够跨密度和样本大小进行泛化。

Method: 将问题构建为序列到序列任务，开发置换和仿射等变transformer，使用交叉注意力连接观测样本和查询点，内置对称性约束确保等变性。

Result: 模型误差显著低于核密度估计和分数去偏核密度估计，具有更好的扩展性，注意力权重能够恢复经典核密度估计，建立了transformer与经典方法的原则性联系。

Conclusion: transformer可作为非参数密度和分数估计的通用数据自适应算子，在准确性和计算效率方面优于传统方法。

Abstract: We introduce a unified attention-based framework for joint score and density
estimation. Framing the problem as a sequence-to-sequence task, we develop a
permutation- and affine-equivariant transformer that estimates both the
probability density $f(x)$ and its score $\nabla_x \log f(x)$ directly from
i.i.d. samples. Unlike traditional score-matching methods that require training
a separate model for each distribution, our approach learns a single
distribution-agnostic operator that generalizes across densities and sample
sizes. The architecture employs cross-attention to connect observed samples
with arbitrary query points, enabling generalization beyond the training data,
while built-in symmetry constraints ensure equivariance to permutation and
affine transformations. Analytically, we show that the attention weights can
recover classical kernel density estimation (KDE), and verify it empirically,
establishing a principled link between classical KDE and the transformer
architecture. Empirically, the model achieves substantially lower error and
better scaling than KDE and score-debiased KDE (SD-KDE), while exhibiting
better runtime scaling. Together, these results establish transformers as
general-purpose, data-adaptive operators for nonparametric density and score
estimation.

</details>


### [96] [Deep Survival Analysis of Longitudinal EHR Data for Joint Prediction of Hospitalization and Death in COPD Patients](https://arxiv.org/abs/2511.05960)
*Enrico Manzini,Thomas Gonzalez Saito,Joan Escudero,Ana Génova,Cristina Caso,Tomas Perez-Porcuna,Alexandre Perera-Lluna*

Main category: cs.LG

TL;DR: 本研究使用纵向电子健康记录对COPD患者进行生存分析，预测住院和死亡事件，比较了统计模型、机器学习和深度学习方法。结果表明，采用循环架构的深度学习模型在预测性能上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: COPD患者住院风险增加且与生存率下降密切相关，但预测这些事件的时间仍然具有挑战性，文献中对此关注有限。

Method: 使用西班牙加泰罗尼亚SIDIAP数据库中超过15万患者的数据（2013-2017年），将住院建模为首次事件，死亡作为半竞争性终点事件，评估了Cox比例风险、SurvivalBoost、DeepPseudo、SurvTRACE、Dynamic Deep-Hit和Deep Recurrent Survival Machine等多种模型。

Result: 深度学习模型，特别是采用循环架构的模型，在一致性和时间依赖性AUC方面优于机器学习和线性方法，尤其是对于更难预测的住院事件。

Conclusion: 这是首个在纵向EHR数据上应用深度生存分析联合预测COPD患者多个时间到事件结果的研究，突显了深度学习方法捕捉时间模式和改进风险分层的潜力。

Abstract: Patients with chronic obstructive pulmonary disease (COPD) have an increased
risk of hospitalizations, strongly associated with decreased survival, yet
predicting the timing of these events remains challenging and has received
limited attention in the literature. In this study, we performed survival
analysis to predict hospitalization and death in COPD patients using
longitudinal electronic health records (EHRs), comparing statistical models,
machine learning (ML), and deep learning (DL) approaches. We analyzed data from
more than 150k patients from the SIDIAP database in Catalonia, Spain, from 2013
to 2017, modeling hospitalization as a first event and death as a
semi-competing terminal event. Multiple models were evaluated, including Cox
proportional hazards, SurvivalBoost, DeepPseudo, SurvTRACE, Dynamic Deep-Hit,
and Deep Recurrent Survival Machine. Results showed that DL models utilizing
recurrent architectures outperformed both ML and linear approaches in
concordance and time-dependent AUC, especially for hospitalization, which
proved to be the harder event to predict. This study is, to our knowledge, the
first to apply deep survival analysis on longitudinal EHR data to jointly
predict multiple time-to-event outcomes in COPD patients, highlighting the
potential of DL approaches to capture temporal patterns and improve risk
stratification.

</details>


### [97] [Next-Latent Prediction Transformers Learn Compact World Models](https://arxiv.org/abs/2511.05963)
*Jayden Teoh,Manan Tomar,Kwangjun Ahn,Edward S. Hu,Pratyusha Sharma,Riashat Islam,Alex Lamb,John Langford*

Main category: cs.LG

TL;DR: NextLat通过引入潜在空间的自监督预测来增强标准的下一个token训练，使transformer学习能够预测下一个潜在状态的表示，从而注入循环归纳偏置，形成紧凑的内部世界模型。


<details>
  <summary>Details</summary>
Motivation: 标准transformer缺乏将历史压缩为紧凑潜在状态的固有激励，导致学习解决方案泛化能力差。需要一种方法来鼓励transformer形成具有自身信念状态和转移动态的紧凑内部世界模型。

Method: NextLat训练transformer学习能够预测给定下一个输出token时的下一个潜在状态的潜在表示。这种简单的辅助目标在保持transformer架构、并行训练和推理不变的同时，为其注入了循环归纳偏置。

Result: 在针对核心序列建模能力的基准测试中，NextLat在下游准确性、表示压缩和前瞻规划方面相比标准下一个token训练显示出显著提升。

Conclusion: NextLat是一个简单高效的范式，用于塑造transformer表示以实现更强的泛化能力，使其能够形成紧凑的内部世界模型。

Abstract: Transformers replace recurrence with a memory that grows with sequence length
and self-attention that enables ad-hoc look ups over past tokens. Consequently,
they lack an inherent incentive to compress history into compact latent states
with consistent transition rules. This often leads to learning solutions that
generalize poorly. We introduce Next-Latent Prediction (NextLat), which extends
standard next-token training with self-supervised predictions in the latent
space. Specifically, NextLat trains a transformer to learn latent
representations that are predictive of its next latent state given the next
output token. Theoretically, we show that these latents provably converge to
belief states, compressed information of the history necessary to predict the
future. This simple auxiliary objective also injects a recurrent inductive bias
into transformers, while leaving their architecture, parallel training, and
inference unchanged. NextLat effectively encourages the transformer to form
compact internal world models with its own belief states and transition
dynamics -- a crucial property absent in standard next-token prediction
transformers. Empirically, across benchmarks targeting core sequence modeling
competencies -- world modeling, reasoning, planning, and language modeling --
NextLat demonstrates significant gains over standard next-token training in
downstream accuracy, representation compression, and lookahead planning.
NextLat stands as a simple and efficient paradigm for shaping transformer
representations toward stronger generalization.

</details>


### [98] [Explainable Deep Learning-based Classification of Wolff-Parkinson-White Electrocardiographic Signals](https://arxiv.org/abs/2511.05973)
*Alice Ragonesi,Stefania Fresca,Karli Gillette,Stefan Kurath-Koller,Gernot Plank,Elena Zappon*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的方法，结合可解释人工智能技术，用于从心电图精确定位Wolff-Parkinson-White综合征的副传导通路位置，在24个心脏区域实现超过95%的定位准确率。


<details>
  <summary>Details</summary>
Motivation: 传统诊断树方法和机器学习方法在副传导通路定位上存在解剖定位分辨率有限、可解释性差以及使用小规模临床数据集的局限性，需要开发更准确、透明的非侵入性定位方法。

Method: 使用个性化虚拟心脏模型生成的大规模生理现实合成心电图数据库训练深度学习模型，并集成引导反向传播、Grad-CAM和引导Grad-CAM等可解释人工智能方法。

Result: 模型在24个心脏区域实现95%以上的定位准确率，灵敏度94.32%，特异性99.78%。XAI输出与已知去极化模式生理验证一致，V2导联被确定为最关键定位导联。

Conclusion: 结合心脏数字孪生与可解释深度学习可实现准确、透明、非侵入性的副传导通路定位，具有临床转化潜力。

Abstract: Wolff-Parkinson-White (WPW) syndrome is a cardiac electrophysiology (EP)
disorder caused by the presence of an accessory pathway (AP) that bypasses the
atrioventricular node, faster ventricular activation rate, and provides a
substrate for atrio-ventricular reentrant tachycardia (AVRT). Accurate
localization of the AP is critical for planning and guiding catheter ablation
procedures. While traditional diagnostic tree (DT) methods and more recent
machine learning (ML) approaches have been proposed to predict AP location from
surface electrocardiogram (ECG), they are often constrained by limited
anatomical localization resolution, poor interpretability, and the use of small
clinical datasets. In this study, we present a Deep Learning (DL) model for the
localization of single manifest APs across 24 cardiac regions, trained on a
large, physiologically realistic database of synthetic ECGs generated using a
personalized virtual heart model. We also integrate eXplainable Artificial
Intelligence (XAI) methods, Guided Backpropagation, Grad-CAM, and Guided
Grad-CAM, into the pipeline. This enables interpretation of DL decision-making
and addresses one of the main barriers to clinical adoption: lack of
transparency in ML predictions. Our model achieves localization accuracy above
95%, with a sensitivity of 94.32% and specificity of 99.78%. XAI outputs are
physiologically validated against known depolarization patterns, and a novel
index is introduced to identify the most informative ECG leads for AP
localization. Results highlight lead V2 as the most critical, followed by aVF,
V1, and aVL. This work demonstrates the potential of combining cardiac digital
twins with explainable DL to enable accurate, transparent, and non-invasive AP
localization.

</details>


### [99] [Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference](https://arxiv.org/abs/2511.05978)
*Yuyang Liu,Jingjing Cai,Jiayi Ren,Peng Zhou,Danyang Zhang,Yin Du,Shijian Li*

Main category: cs.LG

TL;DR: KAT是首个针对大模型分布式推理的异常排查框架，通过GPU工作器的同步性和一致性，利用函数追踪数据在纳秒级精度检测内核级异常，并结合领域适应的LLM提供系统性因果推理和自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 大模型分布式推理中的异常排查需要大量专家手动处理，诊断过程耗时且准确率低，亟需自动化解决方案。

Method: KAT采用两个核心创新：1) 利用GPU工作器的同步性和一致性，通过函数追踪数据在纳秒级精度检测内核级异常；2) 将检测结果集成到领域适应的LLM中，提供系统性因果推理和自然语言解释。

Result: 在阿里巴巴云服务生产环境中的评估显示，KAT在异常检测中达到0.884的精确率和0.936的召回率，显著缩小诊断范围并提高排查效率和成功率。

Conclusion: KAT框架有效解决了大模型分布式推理中的异常排查挑战，通过高精度异常检测和智能推理显著提升了诊断效率和准确性。

Abstract: Anomaly troubleshooting for large model distributed inference (LMDI) remains
a critical challenge. Resolving anomalies such as inference performance
degradation or latency jitter in distributed system demands significant manual
efforts from domain experts, resulting in extremely time-consuming diagnosis
processes with relatively low accuracy. In this paper, we introduce Kunlun
Anomaly Troubleshooter (KAT), the first anomaly troubleshooting framework
tailored for LMDI. KAT addresses this problem through two core innovations.
First, KAT exploits the synchronicity and consistency of GPU workers,
innovatively leverages function trace data to precisely detect kernel-level
anomalies and associated hardware components at nanosecond resolution. Second,
KAT integrates these detection results into a domain-adapted LLM, delivering
systematic causal reasoning and natural language interpretation of complex
anomaly symptoms. Evaluations conducted in Alibaba Cloud Service production
environment indicate that KAT achieves over 0.884 precision and 0.936 recall in
anomaly detection, providing detail anomaly insights that significantly narrow
down the diagnostic scope and improve both the efficiency and success rate of
troubleshooting.

</details>


### [100] [Are Time-Indexed Foundation Models the Future of Time Series Imputation?](https://arxiv.org/abs/2511.05980)
*Etienne Le Naour,Tahar Nabil,Adrien Petralia,Ghislain Agoua*

Main category: cs.LG

TL;DR: 本文首次对时间序列基础模型TabPFN-TS和MoTM进行大规模零样本插补实证研究，在33个域外数据集上评估其无需重新训练即可处理缺失值的能力。


<details>
  <summary>Details</summary>
Motivation: 时间序列插补的基础模型研究尚不充分，TabPFN-TS和MoTM作为时间索引基础模型家族的代表，其零样本插补能力需要系统评估。

Method: 在33个域外数据集（约130万个插补窗口）上进行广泛的单变量实验，评估模型在推理时整合协变量以提高准确性的能力，无需微调。

Result: 结果表明时间索引基础模型是实现真实世界时间序列通用零样本插补的强大实用步骤。

Conclusion: 时间索引基础模型为时间序列零样本插补提供了有效解决方案，展示了在实际应用中的潜力。

Abstract: Foundation models for time series imputation remain largely unexplored.
Recently, two such models, TabPFN-TS and MoTM, have emerged. These models share
a common philosophy that places them within the family of time-indexed
foundation models. This paper presents the first large-scale empirical study of
these models for zero-shot imputation, which enables missing value recovery
without retraining across a wide range of scenarios. We conduct extensive
univariate experiments across 33 out-of-domain datasets (approximately 1.3M
imputation windows) and evaluate their ability to integrate covariates at
inference time to improve accuracy without fine-tuning. Our results demonstrate
that time-indexed foundation models are a powerful and practical step toward
achieving general-purpose, zero-shot imputation for real-world time series.

</details>


### [101] [Bespoke Co-processor for Energy-Efficient Health Monitoring on RISC-V-based Flexible Wearables](https://arxiv.org/abs/2511.05985)
*Theofanis Vergos,Polykarpos Vergos,Mehdi B. Tahoori,Georgios Zervakis*

Main category: cs.LG

TL;DR: 本文提出了一种机械柔性的RISC-V处理器，集成了定制化的乘积累加协处理器，用于医疗可穿戴设备的机器学习分类，实现了更高的能效和更低的延迟。


<details>
  <summary>Details</summary>
Motivation: 柔性电子器件在医疗可穿戴设备中具有独特优势，但现有系统存在门数有限、特征尺寸大、静态功耗高等问题，导致在体机器学习分类面临挑战。现有可弯曲RISC-V系统虽然紧凑但能效不足。

Method: 通过构建约束规划问题，联合确定协处理器常数并优化映射多层感知机推理操作，利用柔性技术的低制造成本优势，实现紧凑的模型专用硬件。

Result: 布局后结果显示在多个医疗数据集上实现近实时性能，电路功耗符合现有柔性电池预算，面积仅2.42 mm²，相比现有技术平均加速2.35倍，能耗降低2.15倍。

Conclusion: 该技术为实现可访问、可持续和贴合性医疗可穿戴设备提供了有前景的路径。

Abstract: Flexible electronics offer unique advantages for conformable, lightweight,
and disposable healthcare wearables. However, their limited gate count, large
feature sizes, and high static power consumption make on-body machine learning
classification highly challenging. While existing bendable RISC-V systems
provide compact solutions, they lack the energy efficiency required. We present
a mechanically flexible RISC-V that integrates a bespoke multiply-accumulate
co-processor with fixed coefficients to maximize energy efficiency and minimize
latency. Our approach formulates a constrained programming problem to jointly
determine co-processor constants and optimally map Multi-Layer Perceptron (MLP)
inference operations, enabling compact, model-specific hardware by leveraging
the low fabrication and non-recurring engineering costs of flexible
technologies. Post-layout results demonstrate near-real-time performance across
several healthcare datasets, with our circuits operating within the power
budget of existing flexible batteries and occupying only 2.42 mm^2, offering a
promising path toward accessible, sustainable, and conformable healthcare
wearables. Our microprocessors achieve an average 2.35x speedup and 2.15x lower
energy consumption compared to the state of the art.

</details>


### [102] [MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference](https://arxiv.org/abs/2511.06010)
*Myunghyun Rhee,Sookyung Choi,Euiseok Kim,Joonseop Sim,Youngpyo Joo,Hoshik Kim*

Main category: cs.LG

TL;DR: MoSKA通过区分请求中的独特序列和共享序列，将共享数据的注意力计算从内存绑定的GEMV操作转换为计算绑定的GEMM操作，显著提升了LLM推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM中不断增长的上下文长度导致的KV缓存性能瓶颈问题，该瓶颈由于内存绑定特性导致GPU利用率低下。

Method: 提出MoSKA架构，包括：1）共享KV注意力机制，将共享数据的注意力计算批处理为GEMM操作；2）MoE启发的稀疏注意力策略修剪搜索空间；3）专门为独特和共享数据设计的解耦基础设施。

Result: 在高上下文共享的工作负载中，相比基线实现了高达538.7倍的吞吐量提升。

Conclusion: MoSKA为可扩展的LLM推理提供了一条清晰的架构路径，通过利用上下文数据的异构性有效解决了KV缓存瓶颈问题。

Abstract: The escalating context length in Large Language Models (LLMs) creates a
severe performance bottleneck around the Key-Value (KV) cache, whose
memory-bound nature leads to significant GPU under-utilization. This paper
introduces Mixture of Shared KV Attention (MoSKA), an architecture that
addresses this challenge by exploiting the heterogeneity of context data. It
differentiates between per-request unique and massively reused shared
sequences. The core of MoSKA is a novel Shared KV Attention mechanism that
transforms the attention on shared data from a series of memory-bound GEMV
operations into a single, compute-bound GEMM by batching concurrent requests.
This is supported by an MoE-inspired sparse attention strategy that prunes the
search space and a tailored Disaggregated Infrastructure that specializes
hardware for unique and shared data. This comprehensive approach demonstrates a
throughput increase of up to 538.7x over baselines in workloads with high
context sharing, offering a clear architectural path toward scalable LLM
inference.

</details>


### [103] [Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving](https://arxiv.org/abs/2511.06029)
*Hui Zeng,Daming Zhao,Pengfei Yang,Wenxuan Hou,Tianyang Zheng,Hui Li,Weiye Ji,Jidong Zhai*

Main category: cs.LG

TL;DR: Lethe是一个动态KV缓存管理框架，通过空间维度的分层稀疏感知分配和时间维度的多轮token剪枝，有效减少大语言模型推理时的内存和延迟开销。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成式推理时会产生长解码序列，导致KV缓存积累造成显著的内存和延迟开销。现有KV压缩方法主要关注减少长输入序列的预填充内存，但无法有效处理长文本生成的动态性和分层敏感性。

Method: Lethe框架在空间维度进行分层稀疏感知分配，根据估计的注意力冗余为每个transformer层分配token剪枝预算；在时间维度通过基于最近性感知选择性保留(RASR)机制进行多轮token剪枝，RASR扩展了传统的基于最近性的启发式方法，同时考虑从演化注意力模式中得出的token相关性。

Result: 实验结果表明，Lethe在不同模型和任务上实现了效率和生成质量的良好平衡，吞吐量最高提升2.56倍。

Conclusion: Lethe通过动态KV缓存管理有效解决了大语言模型长文本生成推理中的内存和延迟问题，在保持生成质量的同时显著提升了效率。

Abstract: Generative reasoning with large language models (LLMs) often involves long
decoding sequences, leading to substantial memory and latency overheads from
accumulating key-value (KV) caches. While existing KV compression methods
primarily focus on reducing prefill memory from long input sequences, they fall
short in addressing the dynamic and layer-sensitive nature of long-form
generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV
cache management framework that introduces adaptivity along both the spatial
and temporal dimensions of decoding. Along the spatial dimension, Lethe
performs layerwise sparsity-aware allocation, assigning token pruning budgets
to each transformer layer based on estimated attention redundancy. Along the
temporal dimension, Lethe conducts multi-round token pruning during generation,
driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends
traditional recency-based heuristics by also considering token relevance
derived from evolving attention patterns, enabling informed decisions about
which tokens to retain or evict. Empirical results demonstrate that Lethe
achieves a favorable balance between efficiency and generation quality across
diverse models and tasks, increases throughput by up to 2.56x.

</details>


### [104] [Advancing Ocean State Estimation with efficient and scalable AI](https://arxiv.org/abs/2511.06041)
*Yanfei Xiang,Yuan Gao,Hao Wu,Quan Zhang,Ruiqi Shu,Xiao Zhou,Xi Wu,Xiaomeng Huang*

Main category: cs.LG

TL;DR: ADAF-Ocean是一个AI驱动的海洋数据同化框架，能够直接同化多源多尺度观测数据，无需插值或数据稀疏化，通过AI驱动的超分辨率从粗分辨率场重建中尺度动力学，显著提升全球预报技能。


<details>
  <summary>Details</summary>
Motivation: 解决传统数据同化和深度学习方法在计算可扩展性和数据保真度方面的瓶颈，为地球系统科学提供准确高效的全球海洋状态估计。

Method: 基于神经过程学习从异构输入到海洋状态的连续映射，通过AI驱动的超分辨率技术从1°粗分辨率场重建0.25°中尺度动力学，仅增加3.7%参数。

Result: 与没有同化的基线相比，ADAF-Ocean将全球预报技能延长了最多20天，同时保持了计算效率和可扩展性。

Conclusion: 该框架为实时高分辨率地球系统监测建立了一条计算可行且科学严谨的途径。

Abstract: Accurate and efficient global ocean state estimation remains a grand
challenge for Earth system science, hindered by the dual bottlenecks of
computational scalability and degraded data fidelity in traditional data
assimilation (DA) and deep learning (DL) approaches. Here we present an
AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly
assimilates multi-source and multi-scale observations, ranging from sparse
in-situ measurements to 4 km satellite swaths, without any interpolation or
data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous
mapping from heterogeneous inputs to ocean states, preserving native data
fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\circ$
mesoscale dynamics from coarse 1$^\circ$ fields, which ensures both efficiency
and scalability, with just 3.7\% more parameters than the 1$^\circ$
configuration. When coupled with a DL forecasting system, ADAF-Ocean extends
global forecast skill by up to 20 days compared to baselines without
assimilation. This framework establishes a computationally viable and
scientifically rigorous pathway toward real-time, high-resolution Earth system
monitoring.

</details>


### [105] [How Particle-System Random Batch Methods Enhance Graph Transformer: Memory Efficiency and Parallel Computing Strategy](https://arxiv.org/abs/2511.06044)
*Hanwen Liu,Yixuan Ma,Shi Jin,Yuguang Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为随机批量注意力（RBA）的线性自注意力机制，该机制在保持表达能力的同时具有线性时间复杂度，并能通过并行化实现内存节省。


<details>
  <summary>Details</summary>
Motivation: 注意力机制是Transformer模型的重要组成部分，但其二次复杂度限制了实用性。现有稀疏注意力机制缺乏理论分析，无法保证在降低复杂度的同时保持表达能力。

Method: 提出随机批量注意力（RBA），这是一种线性自注意力机制，基于计算数学中的随机批量方法，具有理论收敛性保证。

Result: 在大规模图数据上的实验证明了RBA的优势：线性时间复杂度、内存节省、能够改进现有模型，并且具有理论解释。

Conclusion: 随机批量注意力机制不仅解决了注意力机制的复杂度问题，还为注意力机制的理论分析提供了新工具。

Abstract: Attention mechanism is a significant part of Transformer models. It helps
extract features from embedded vectors by adding global information and its
expressivity has been proved to be powerful. Nevertheless, the quadratic
complexity restricts its practicability. Although several researches have
provided attention mechanism in sparse form, they are lack of theoretical
analysis about the expressivity of their mechanism while reducing complexity.
In this paper, we put forward Random Batch Attention (RBA), a linear
self-attention mechanism, which has theoretical support of the ability to
maintain its expressivity. Random Batch Attention has several significant
strengths as follows: (1) Random Batch Attention has linear time complexity.
Other than this, it can be implemented in parallel on a new dimension, which
contributes to much memory saving. (2) Random Batch Attention mechanism can
improve most of the existing models by replacing their attention mechanisms,
even many previously improved attention mechanisms. (3) Random Batch Attention
mechanism has theoretical explanation in convergence, as it comes from Random
Batch Methods on computation mathematics. Experiments on large graphs have
proved advantages mentioned above. Also, the theoretical modeling of
self-attention mechanism is a new tool for future research on
attention-mechanism analysis.

</details>


### [106] [Function Based Isolation Forest (FuBIF): A Unifying Framework for Interpretable Isolation-Based Anomaly Detection](https://arxiv.org/abs/2511.06054)
*Alessio Arcudi,Alessandro Ferreri,Francesco Borsatti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文提出了Function-based Isolation Forest (FuBIF)，这是Isolation Forest的泛化版本，使用实值函数进行数据集分支，提高了评估树构建的灵活性。同时提出了FuBIF Feature Importance (FuBIFFI)算法来增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的Isolation Forest在异常检测中存在适应性和偏差限制，需要更灵活和可解释的方法来处理复杂数据集中的异常值。

Method: FuBIF通过使用实值函数进行数据集分支来构建评估树，FuBIFFI算法为FuBIF模型提供特征重要性评分以增强可解释性。

Result: 论文详细描述了FuBIF的操作框架，评估了其性能并与现有方法进行比较，展示了其理论贡献。

Conclusion: FuBIF显著提高了Isolation Forest的灵活性，FuBIFFI增强了可解释性，开源实现促进了进一步研究和可复现性。

Abstract: Anomaly Detection (AD) is evolving through algorithms capable of identifying
outliers in complex datasets. The Isolation Forest (IF), a pivotal AD
technique, exhibits adaptability limitations and biases. This paper introduces
the Function-based Isolation Forest (FuBIF), a generalization of IF that
enables the use of real-valued functions for dataset branching, significantly
enhancing the flexibility of evaluation tree construction. Complementing this,
the FuBIF Feature Importance (FuBIFFI) algorithm extends the interpretability
in IF-based approaches by providing feature importance scores across possible
FuBIF models. This paper details the operational framework of FuBIF, evaluates
its performance against established methods, and explores its theoretical
contributions. An open-source implementation is provided to encourage further
research and ensure reproducibility.

</details>


### [107] [CatBack: Universal Backdoor Attacks on Tabular Data via Categorical Encoding](https://arxiv.org/abs/2511.06072)
*Behrad Tajalli,Stefanos Koffas,Stjepan Picek*

Main category: cs.LG

TL;DR: 提出了一种针对表格数据的后门攻击方法，通过将分类值转换为浮点表示，创建适用于所有特征的梯度扰动，在5个数据集和4个模型上实现了高达100%的攻击成功率，并能绕过现有防御机制。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击研究主要关注图像等同质数据，而表格数据由于包含数值和分类特征，攻击更具挑战性。本文旨在填补这一空白，开发针对表格数据的有效后门攻击方法。

Method: 提出将分类值转换为浮点表示的新技术，保持清洁模型准确性的同时，创建适用于所有特征的基于梯度的通用扰动。

Result: 在5个数据集和4个流行模型上评估，白盒和黑盒设置下攻击成功率高达100%，在Vertex AI等真实应用中验证了有效性，性能超越Tabdoor等先前工作。

Conclusion: 该方法揭示了表格数据的严重安全漏洞，能够成功绕过Spectral Signatures、Neural Cleanse、Beatrix、Fine-Pruning等先进防御机制和异常检测方法。

Abstract: Backdoor attacks in machine learning have drawn significant attention for
their potential to compromise models stealthily, yet most research has focused
on homogeneous data such as images. In this work, we propose a novel backdoor
attack on tabular data, which is particularly challenging due to the presence
of both numerical and categorical features. Our key idea is a novel technique
to convert categorical values into floating-point representations. This
approach preserves enough information to maintain clean-model accuracy compared
to traditional methods like one-hot or ordinal encoding. By doing this, we
create a gradient-based universal perturbation that applies to all features,
including categorical ones.
  We evaluate our method on five datasets and four popular models. Our results
show up to a 100% attack success rate in both white-box and black-box settings
(including real-world applications like Vertex AI), revealing a severe
vulnerability for tabular data. Our method is shown to surpass the previous
works like Tabdoor in terms of performance, while remaining stealthy against
state-of-the-art defense mechanisms. We evaluate our attack against Spectral
Signatures, Neural Cleanse, Beatrix, and Fine-Pruning, all of which fail to
defend successfully against it. We also verify that our attack successfully
bypasses popular outlier detection mechanisms.

</details>


### [108] [Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin](https://arxiv.org/abs/2511.06077)
*Lin Guan,Jia-Qi Yang,Zhishan Zhao,Beichuan Zhang,Bo Sun,Xuanyuan Luo,Jinan Ni,Xiaowen Li,Yuhang Qi,Zhifang Fan,Hangyu Wang,Qiwei Chen,Yi Cheng,Feng Zhang,Xiao Yang*

Main category: cs.LG

TL;DR: 提出了一个端到端系统，将长序列建模扩展到10k长度的用户历史记录，包括STCA注意力机制、RLB批处理方案和长度外推训练策略，在抖音上实现了显著改进。


<details>
  <summary>Details</summary>
Motivation: 短视频推荐系统需要处理极长的用户历史记录，同时不能突破延迟或成本预算，需要解决长序列建模的挑战。

Method: 1. STCA：用目标到历史的堆叠交叉注意力替代历史自注意力，将复杂度从二次降低到线性；2. RLB：用户中心批处理方案，聚合同一用户的多个目标共享用户侧编码；3. 长度外推训练策略：在较短窗口训练，在更长窗口推理。

Result: 在离线和在线实验中，随着历史长度和模型容量的扩展，观察到可预测的单调增益，部署在抖音全流量上，在关键参与度指标上实现显著改进，同时满足生产延迟要求。

Conclusion: 展示了一条实用的路径，将端到端长序列推荐扩展到10k级别，证明了在大规模推荐系统中应用长序列建模的可行性。

Abstract: Short-video recommenders such as Douyin must exploit extremely long user
histories without breaking latency or cost budgets. We present an end-to-end
system that scales long-sequence modeling to 10k-length histories in
production. First, we introduce Stacked Target-to-History Cross Attention
(STCA), which replaces history self-attention with stacked cross-attention from
the target to the history, reducing complexity from quadratic to linear in
sequence length and enabling efficient end-to-end training. Second, we propose
Request Level Batching (RLB), a user-centric batching scheme that aggregates
multiple targets for the same user/request to share the user-side encoding,
substantially lowering sequence-related storage, communication, and compute
without changing the learning objective. Third, we design a
length-extrapolative training strategy -- train on shorter windows, infer on
much longer ones -- so the model generalizes to 10k histories without
additional training cost. Across offline and online experiments, we observe
predictable, monotonic gains as we scale history length and model capacity,
mirroring the scaling law behavior observed in large language models. Deployed
at full traffic on Douyin, our system delivers significant improvements on key
engagement metrics while meeting production latency, demonstrating a practical
path to scaling end-to-end long-sequence recommendation to the 10k regime.

</details>


### [109] [Approximating Shapley Explanations in Reinforcement Learning](https://arxiv.org/abs/2511.06094)
*Daniel Beechey,Özgür Şimşek*

Main category: cs.LG

TL;DR: FastSVERL是一种可扩展的方法，通过近似Shapley值来解释强化学习，解决了传统Shapley解释计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在复杂决策环境中取得了显著成功，但其缺乏透明度限制了在实际应用中的部署，特别是在安全关键场景中。Shapley值提供了理论框架来解释强化学习，但计算成本是其使用的主要障碍。

Method: FastSVERL引入了一种可扩展的方法来近似Shapley值，专门设计用于处理强化学习的独特挑战，包括多步轨迹中的时间依赖性、从离策略数据中学习以及实时适应不断演化的智能体行为。

Result: FastSVERL提供了一种实用、可扩展的方法，为强化学习提供了原则性和严格的解释性。

Conclusion: FastSVERL解决了Shapley解释在强化学习中的计算成本问题，为强化学习提供了可扩展的、原则性的解释框架。

Abstract: Reinforcement learning has achieved remarkable success in complex
decision-making environments, yet its lack of transparency limits its
deployment in practice, especially in safety-critical settings. Shapley values
from cooperative game theory provide a principled framework for explaining
reinforcement learning; however, the computational cost of Shapley explanations
is an obstacle to their use. We introduce FastSVERL, a scalable method for
explaining reinforcement learning by approximating Shapley values. FastSVERL is
designed to handle the unique challenges of reinforcement learning, including
temporal dependencies across multi-step trajectories, learning from off-policy
data, and adapting to evolving agent behaviours in real time. FastSVERL
introduces a practical, scalable approach for principled and rigorous
interpretability in reinforcement learning.

</details>


### [110] [Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices](https://arxiv.org/abs/2511.06111)
*Aysin Tumay,Sophia Sun,Sonia Fereidooni,Aaron Dumas,Elise Jortberg,Rose Yu*

Main category: cs.LG

TL;DR: 本文提出了一种用于心源性休克患者机械循环支持设备自动脱机的端到端机器学习框架，包含临床感知的OOD正则化模型策略优化算法和基于Transformer的概率数字孪生模型。


<details>
  <summary>Details</summary>
Motivation: 当前机械循环支持设备的脱机策略在不同医疗团队间差异显著，缺乏数据驱动方法，而离线强化学习在医疗决策中面临禁止在线患者交互、循环动力学高度不确定和数据有限等挑战。

Method: 开发了CORMPO算法（临床感知的OOD正则化模型策略优化），结合密度正则化抑制分布外数据，并融入临床知识进行奖励塑形；同时构建基于Transformer的概率数字孪生模型来模拟循环动力学。

Result: CORMPO在真实和合成数据集上比离线强化学习基线获得28%的更高奖励，在临床指标上获得82.6%的更高分数，并在温和假设下实现了理论性能保证。

Conclusion: 该方法为高风险医疗应用中的安全离线策略学习提供了一个原则性框架，其中领域专业知识和安全约束至关重要。

Abstract: We study the sequential decision-making problem for automated weaning of
mechanical circulatory support (MCS) devices in cardiogenic shock patients. MCS
devices are percutaneous micro-axial flow pumps that provide left ventricular
unloading and forward blood flow, but current weaning strategies vary
significantly across care teams and lack data-driven approaches. Offline
reinforcement learning (RL) has proven to be successful in sequential
decision-making tasks, but our setting presents challenges for training and
evaluating traditional offline RL methods: prohibition of online patient
interaction, highly uncertain circulatory dynamics due to concurrent
treatments, and limited data availability. We developed an end-to-end machine
learning framework with two key contributions (1) Clinically-aware
OOD-regularized Model-based Policy Optimization (CORMPO), a density-regularized
offline RL algorithm for out-of-distribution suppression that also incorporates
clinically-informed reward shaping and (2) a Transformer-based probabilistic
digital twin that models MCS circulatory dynamics for policy evaluation with
rich physiological and clinical metrics. We prove that \textsf{CORMPO} achieves
theoretical performance guarantees under mild assumptions. CORMPO attains a
higher reward than the offline RL baselines by 28% and higher scores in
clinical metrics by 82.6% on real and synthetic datasets. Our approach offers a
principled framework for safe offline policy learning in high-stakes medical
applications where domain expertise and safety constraints are essential.

</details>


### [111] [On the Convergence and Stability of Distributed Sub-model Training](https://arxiv.org/abs/2511.06132)
*Yuyang Deng,Fuli Qiao,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 提出分布式洗牌子模型训练方法，通过预先划分完整模型为多个子模型并在每轮训练中洗牌分配给客户端，解决了联邦学习中大模型本地训练的内存限制问题，同时提升了收敛性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模不断增大，联邦学习中的本地设备训练面临内存限制挑战。现有的随机子模型采样方法收敛性能不佳，需要更有效的子模型训练策略。

Method: 将完整模型预先划分为多个子模型，服务器在每轮训练中洗牌这些子模型并分配给客户端。客户端只更新接收到的子模型，训练结束后服务器对更新后的子模型进行平均。

Result: 建立了该算法的收敛率理论分析，并通过稳定性分析发现子模型训练能够通过放大训练过程的稳定性来提升泛化性能。大量实验验证了理论发现。

Conclusion: 分布式洗牌子模型训练方法有效解决了大模型联邦学习的内存限制问题，同时提供了更好的收敛性能和泛化能力。

Abstract: As learning models continue to grow in size, enabling on-device local
training of these models has emerged as a critical challenge in federated
learning. A popular solution is sub-model training, where the server only
distributes randomly sampled sub-models to the edge clients, and clients only
update these small models. However, those random sampling of sub-models may not
give satisfying convergence performance. In this paper, observing the success
of SGD with shuffling, we propose a distributed shuffled sub-model training,
where the full model is partitioned into several sub-models in advance, and the
server shuffles those sub-models, sends each of them to clients at each round,
and by the end of local updating period, clients send back the updated
sub-models, and server averages them. We establish the convergence rate of this
algorithm. We also study the generalization of distributed sub-model training
via stability analysis, and find that the sub-model training can improve the
generalization via amplifying the stability of training process. The extensive
experiments also validate our theoretical findings.

</details>


### [112] [LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains](https://arxiv.org/abs/2511.06161)
*Ibna Kowsar,Kazi F. Akhter,Manar D. Samad*

Main category: cs.LG

TL;DR: 提出了一种轻量级迁移学习框架LATTLE，通过将LLM的选择性键值投影权重移植到专门为表格数据设计的gFTT模型中，实现跨领域表格数据的有效迁移学习，无需共享特征或大规模预训练模型。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据迁移学习中由于特征空间异构性导致的挑战，传统深度学习方法效果有限，而LLM在处理混合数据类型表格数据时受限于文本提示和上下文学习。

Method: 首先使用源表格数据微调LLM，然后将其选择性键值投影权重移植到gFTT模型中，最后使用目标表格数据微调具有跨领域注意力的gFTT模型。

Result: 在10对源-目标数据集和12个基线方法的实验中，LATTLE方法优于传统机器学习模型、最先进的深度表格架构以及基于数千到数十亿表格样本训练的迁移学习模型。

Conclusion: 提出的注意力转移方法为在低资源学习环境中使用LLM学习数据表之间的关系提供了有效解决方案。

Abstract: Transfer learning of tabular data is non-trivial due to heterogeneity in the
feature space across disparate domains. The limited success of traditional deep
learning in tabular knowledge transfer can be advanced by leveraging large
language models (LLMs). However, the efficacy of LLMs often stagnates for mixed
data types structured in tables due to the limitations of text prompts and
in-context learning. We propose a lightweight transfer learning framework that
fine-tunes an LLM using source tabular data and transplants the LLM's selective
$key$ and $value$ projection weights into a gated feature tokenized transformer
(gFTT) built for tabular data. The gFTT model with cross-domain attention is
fine-tuned using target tabular data for transfer learning, eliminating the
need for shared features, LLM prompt engineering, and large-scale pretrained
models. Our experiments using ten pairs of source-target data sets and 12
baselines demonstrate the superiority of the proposed LLM-attention transplant
for transfer learning (LATTLE) method over traditional ML models,
state-of-the-art deep tabular architectures, and transfer learning models
trained on thousands to billions of tabular samples. The proposed attention
transfer demonstrates an effective solution to learning relationships between
data tables using an LLM in a low-resource learning environment. The source
code for the proposed method is publicly available.

</details>


### [113] [Learning Gaussian DAG Models without Condition Number Bounds](https://arxiv.org/abs/2511.06164)
*Constantinos Daskalakis,Vardis Kandiros,Rui Yao*

Main category: cs.LG

TL;DR: 本文研究了在有向高斯图模型中学习拓扑结构的问题，提出了一种样本复杂度与条件数无关的新算法，并建立了几乎紧的样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 现有方法在样本复杂度上对协方差矩阵条件数有多项式依赖，这在条件数随节点数增长时会导致高维设置下不实用。

Method: 开发了一种新算法来恢复底层图结构，在变量方差有界的情况下设计了多项式时间算法。

Result: 证明了所需样本数与条件数无关，建立了与上界几乎匹配的下界，并在合成数据集上验证了理论结果。

Conclusion: 提供了有向高斯图模型拓扑学习问题的几乎紧样本复杂度刻画，解决了条件数依赖问题。

Abstract: We study the problem of learning the topology of a directed Gaussian
Graphical Model under the equal-variance assumption, where the graph has $n$
nodes and maximum in-degree $d$. Prior work has established that $O(d \log n)$
samples are sufficient for this task. However, an important factor that is
often overlooked in these analyses is the dependence on the condition number of
the covariance matrix of the model. Indeed, all algorithms from prior work
require a number of samples that grows polynomially with this condition number.
In many cases this is unsatisfactory, since the condition number could grow
polynomially with $n$, rendering these prior approaches impractical in
high-dimensional settings. In this work, we provide an algorithm that recovers
the underlying graph and prove that the number of samples required is
independent of the condition number. Furthermore, we establish lower bounds
that nearly match the upper bound up to a $d$-factor, thus providing an almost
tight characterization of the true sample complexity of the problem. Moreover,
under a further assumption that all the variances of the variables are bounded,
we design a polynomial-time algorithm that recovers the underlying graph, at
the cost of an additional polynomial dependence of the sample complexity on
$d$. We complement our theoretical findings with simulations on synthetic
datasets that confirm our predictions.

</details>


### [114] [Local K-Similarity Constraint for Federated Learning with Label Noise](https://arxiv.org/abs/2511.06169)
*Sanskar Amgain,Prashant Shrestha,Bidur Khanal,Alina Devkota,Yash Raj Shrestha,Seungryul Baek,Prashnna Gyawali,Binod Bhattarai*

Main category: cs.LG

TL;DR: 提出一种联邦学习中处理噪声标签客户的本地正则化方法，通过解耦预训练模型和分类模型，利用自监督预训练模型的特征空间来评估数据点相似性，在标准噪声联邦设置中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有处理噪声客户端的方法假设有足够多的干净标签客户端可用，但在高比例异构噪声客户端场景下失效。需要本地正则化来防止噪声客户端污染全局模型，但现有依赖预训练初始化的集中式方法在联邦学习中通信成本过高。

Method: 提出客户端模型的正则化目标，通过强制客户端内相近数据点的相似性来解耦预训练和分类模型，利用自监督预训练模型的特征空间评估示例间距离。

Result: 在多个计算机视觉和医学图像分类基准测试中，该方法显著提升性能，优于现有最先进的联邦学习方法。

Conclusion: 该方法不需要预训练模型和分类器主干共享相同架构，具有架构无关性，在噪声联邦学习场景下有效提升模型鲁棒性。

Abstract: Federated learning on clients with noisy labels is a challenging problem, as
such clients can infiltrate the global model, impacting the overall
generalizability of the system. Existing methods proposed to handle noisy
clients assume that a sufficient number of clients with clean labels are
available, which can be leveraged to learn a robust global model while
dampening the impact of noisy clients. This assumption fails when a high number
of heterogeneous clients contain noisy labels, making the existing approaches
ineffective. In such scenarios, it is important to locally regularize the
clients before communication with the global model, to ensure the global model
isn't corrupted by noisy clients. While pre-trained self-supervised models can
be effective for local regularization, existing centralized approaches relying
on pretrained initialization are impractical in a federated setting due to the
potentially large size of these models, which increases communication costs. In
that line, we propose a regularization objective for client models that
decouples the pre-trained and classification models by enforcing similarity
between close data points within the client. We leverage the representation
space of a self-supervised pretrained model to evaluate the closeness among
examples. This regularization, when applied with the standard objective
function for the downstream task in standard noisy federated settings,
significantly improves performance, outperforming existing state-of-the-art
federated methods in multiple computer vision and medical image classification
benchmarks. Unlike other techniques that rely on self-supervised pretrained
initialization, our method does not require the pretrained model and classifier
backbone to share the same architecture, making it architecture-agnostic.

</details>


### [115] [Sparse Linear Regression is Easy on Random Supports](https://arxiv.org/abs/2511.06211)
*Gautam Chandrasekaran,Raghu Meka,Konstantinos Stavropoulos*

Main category: cs.LG

TL;DR: 该论文提出了稀疏线性回归问题的首个通用正结果，对于任意设计矩阵X，当信号向量w*的支撑集随机选择时，可以在多项式时间内实现接近最优的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 稀疏线性回归是机器学习和统计学中的基本问题，目前存在计算复杂度和样本复杂度之间的指数差距：信息理论上需要O(k log d/ε)样本，但计算上需要d^Ω(k)运行时间。论文旨在填补这一差距。

Method: 提出了一种新算法，对于任意设计矩阵X，当w*的支撑集随机选择时，使用poly(k, log d, 1/ε)样本和poly(d,N)运行时间即可达到预测误差ε。该方法适用于条件数高达2^poly(d)的任意设计矩阵。

Result: 首次证明了对于最坏情况设计矩阵，在多项式时间内实现接近最优样本复杂度的可能性，突破了之前只能针对随机设计矩阵或特殊结构矩阵的限制。

Conclusion: 该工作为稀疏线性回归提供了第一个通用的计算高效算法，显著推进了对该问题计算复杂度的理解，填补了信息理论界限与计算可行性之间的重要空白。

Abstract: Sparse linear regression is one of the most basic questions in machine
learning and statistics. Here, we are given as input a design matrix $X \in
\mathbb{R}^{N \times d}$ and measurements or labels ${y} \in \mathbb{R}^N$
where ${y} = {X} {w}^* + {\xi}$, and ${\xi}$ is the noise in the measurements.
Importantly, we have the additional constraint that the unknown signal vector
${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than
the ambient dimension. Our goal is to output a prediction vector
$\widehat{{w}}$ that has small prediction error: $\frac{1}{N}\cdot \|{X} {w}^*
- {X} \widehat{{w}}\|^2_2$.
  Information-theoretically, we know what is best possible in terms of
measurements: under most natural noise distributions, we can get prediction
error at most $\epsilon$ with roughly $N = O(k \log d/\epsilon)$ samples.
Computationally, this currently needs $d^{\Omega(k)}$ run-time. Alternately,
with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap
(in the dependence on $d$) between the two and we do not know if it is possible
to get $d^{o(k)}$ run-time and $o(d)$ samples.
  We give the first generic positive result for worst-case design matrices
${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at
random, we can get prediction error $\epsilon$ with $N = \text{poly}(k, \log d,
1/\epsilon)$ samples and run-time $\text{poly}(d,N)$. This run-time holds for
any design matrix ${X}$ with condition number up to $2^{\text{poly}(d)}$.
  Previously, such results were known for worst-case ${w}^*$, but only for
random design matrices from well-behaved families, matrices that have a very
low condition number ($\text{poly}(\log d)$; e.g., as studied in compressed
sensing), or those with special structural properties.

</details>


### [116] [Adaptive Multi-view Graph Contrastive Learning via Fractional-order Neural Diffusion Networks](https://arxiv.org/abs/2511.06216)
*Yanan Zhao,Feng Ji,Jingyang Dai,Jiaze Ma,Keyue Jiang,Kai Zhao,Wee Peng Tay*

Main category: cs.LG

TL;DR: 本文提出了一种基于分数阶连续动力学的无增强多视图图对比学习框架，通过可学习的分数阶导数参数自动生成多尺度视图，无需手动增强即可获得多样化的互补表示。


<details>
  <summary>Details</summary>
Motivation: 现有的图对比学习方法通常依赖固定的手工视图（局部和全局视角），限制了捕捉多尺度结构模式的能力。

Method: 使用分数阶连续动力学框架，通过变化分数阶导数参数α∈(0,1]产生连续谱的视图：小α产生局部化特征，大α诱导更广泛的全局聚合。将α作为可学习参数，使模型能够自适应扩散尺度并自动发现信息丰富的视图。

Result: 在标准基准测试上的广泛实验表明，该方法产生更鲁棒和富有表现力的嵌入，并优于最先进的图对比学习基线方法。

Conclusion: 这种基于分数阶动力学的原理性方法无需手动增强即可生成多样化、互补的表示，为图对比学习提供了更有效的多尺度特征提取能力。

Abstract: Graph contrastive learning (GCL) learns node and graph representations by
contrasting multiple views of the same graph. Existing methods typically rely
on fixed, handcrafted views-usually a local and a global perspective, which
limits their ability to capture multi-scale structural patterns. We present an
augmentation-free, multi-view GCL framework grounded in fractional-order
continuous dynamics. By varying the fractional derivative order $\alpha \in
(0,1]$, our encoders produce a continuous spectrum of views: small $\alpha$
yields localized features, while large $\alpha$ induces broader, global
aggregation. We treat $\alpha$ as a learnable parameter so the model can adapt
diffusion scales to the data and automatically discover informative views. This
principled approach generates diverse, complementary representations without
manual augmentations. Extensive experiments on standard benchmarks demonstrate
that our method produces more robust and expressive embeddings and outperforms
state-of-the-art GCL baselines.

</details>


### [117] [Scaling Laws and In-Context Learning: A Unified Theoretical Framework](https://arxiv.org/abs/2511.06232)
*Sushant Mehta,Ishan Gupta*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，将缩放定律与transformer中的上下文学习涌现联系起来，揭示了ICL性能与模型深度、宽度、上下文长度和训练数据之间的幂律关系，并证明了transformer在前向传播中实现了基于梯度的元学习。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量实证研究，但对于大规模模型中上下文学习涌现的原理性理解仍然较为模糊。本文旨在建立一个统一的理论框架，将缩放定律与transformer中ICL的涌现联系起来。

Method: 通过理论分析建立ICL性能与模型深度L、宽度d、上下文长度k和训练数据D之间的幂律关系，证明在特定条件下transformer在前向传播中实现了基于梯度的元学习，有效学习率为η_eff = Θ(1/√Ld)。

Result: 发现了在临界尺度处的急剧相变，推导出最优深度-宽度分配为L* ∝ N^{2/3}、d* ∝ N^{1/3}（固定参数预算N = Ld）。在合成任务上的系统实验验证了理论预测，测量的缩放指数与理论紧密匹配。

Conclusion: 这项工作为ICL的涌现提供了必要和充分条件，并建立了transformer在上下文中学习能力的基本计算极限。

Abstract: In-context learning (ICL) enables large language models to adapt to new tasks
from demonstrations without parameter updates. Despite extensive empirical
studies, a principled understanding of ICL emergence at scale remains more
elusive. We present a unified theoretical framework connecting scaling laws to
ICL emergence in transformers. Our analysis establishes that ICL performance
follows power-law relationships with model depth $L$, width $d$, context length
$k$, and training data $D$, with exponents determined by task structure. We
show that under specific conditions, transformers implement gradient-based
metalearning in their forward pass, with an effective learning rate
$\eta_{\text{eff}} = \Theta(1/\sqrt{Ld})$. We demonstrate sharp phase
transitions at critical scales and derive optimal depth-width allocations
favoring $L^* \propto N^{2/3}$, $d^* \propto N^{1/3}$ for the fixed parameter
budget $N = Ld$. Systematic experiments on synthetic tasks validate our
predictions, with measured scaling exponents closely matching theory. This work
provides both necessary and sufficient conditions for the emergence of ICLs and
establishes fundamental computational limits on what transformers can learn
in-context.

</details>


### [118] [Mixtures of SubExperts for Large Language Continual Learning](https://arxiv.org/abs/2511.06237)
*Haeyong Kang*

Main category: cs.LG

TL;DR: 提出了一种名为Mixtures of SubExperts (MoSEs)的自适应参数高效微调方法，用于解决大语言模型在持续学习中的灾难性遗忘和模型规模线性增长问题。


<details>
  <summary>Details</summary>
Motivation: 传统参数高效微调方法在持续学习中面临两难困境：重用参数会导致灾难性遗忘，为每个任务分配独立参数则导致模型规模线性增长且无法实现任务间知识迁移。

Method: 在Transformer层中集成稀疏子专家混合机制，通过任务特定路由机制动态选择和组合先前学习的稀疏参数，实现知识隔离和保护。

Result: 在TRACE基准数据集上的实验表明，MoSEs在知识保留和新任务可扩展性方面显著优于传统持续学习方法，实现了最先进的性能，同时大幅节省内存和计算资源。

Conclusion: MoSEs框架通过子专家混合和自适应路由机制，有效解决了持续学习中的灾难性遗忘问题，同时确保模型容量呈亚线性增长，实现了高效的知识迁移和保留。

Abstract: Adapting Large Language Models (LLMs) to a continuous stream of tasks is a
critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT)
methods have become a standard for this, they face a fundamental dilemma in
continual learning. Reusing a single set of PEFT parameters for new tasks often
leads to catastrophic forgetting of prior knowledge. Conversely, allocating
distinct parameters for each task prevents forgetting but results in a linear
growth of the model's size and fails to facilitate knowledge transfer between
related tasks. To overcome these limitations, we propose a novel adaptive PEFT
method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel
continual learning framework designed for minimal forgetting and efficient
scalability. MoSEs integrate a sparse Mixture of SubExperts into the
transformer layers, governed by a task-specific routing mechanism. This
architecture allows the model to isolate and protect knowledge within dedicated
SubExperts, thereby minimizing parameter interference and catastrophic
forgetting. Crucially, the router can adaptively select and combine previously
learned sparse parameters for new tasks, enabling effective knowledge transfer
while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs
on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that
MoSEs significantly outperform conventional continual learning approaches in
both knowledge retention and scalability to new tasks, achieving
state-of-the-art performance with substantial memory and computational savings.

</details>


### [119] [Test-Time Iterative Error Correction for Efficient Diffusion Models](https://arxiv.org/abs/2511.06250)
*Yunshan Zhong,Yanwei Qi,Yuxin Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为迭代误差校正（IEC）的测试时方法，用于缓解高效扩散模型中的近似误差累积问题，无需重新训练或架构修改即可将误差传播从指数增长降低到线性增长。


<details>
  <summary>Details</summary>
Motivation: 随着资源受限设备对高质量图像生成需求的增长，高效扩散模型受到关注，但效率技术引入的近似误差会显著降低生成质量，且部署后难以修正。研究发现这些误差会在扩散时间步中指数累积。

Method: 提出迭代误差校正（IEC）方法，通过在推理过程中迭代细化模型输出来缓解推理时误差。该方法可无缝集成到现有扩散模型的推理过程中，实现性能与效率的灵活权衡。

Result: 大量实验表明，IEC在各种数据集、效率技术和模型架构上都能持续提高生成质量，证明了其作为高效扩散模型测试时增强的实用性和泛化性。

Conclusion: IEC是一种实用且可泛化的解决方案，能够有效提升高效扩散模型的生成质量，同时保持部署灵活性。

Abstract: With the growing demand for high-quality image generation on
resource-constrained devices, efficient diffusion models have received
increasing attention. However, such models suffer from approximation errors
introduced by efficiency techniques, which significantly degrade generation
quality. Once deployed, these errors are difficult to correct, as modifying the
model is typically infeasible in deployment environments. Through an analysis
of error propagation across diffusion timesteps, we reveal that these
approximation errors can accumulate exponentially, severely impairing output
quality. Motivated by this insight, we propose Iterative Error Correction
(IEC), a novel test-time method that mitigates inference-time errors by
iteratively refining the model's output. IEC is theoretically proven to reduce
error propagation from exponential to linear growth, without requiring any
retraining or architectural changes. IEC can seamlessly integrate into the
inference process of existing diffusion models, enabling a flexible trade-off
between performance and efficiency. Extensive experiments show that IEC
consistently improves generation quality across various datasets, efficiency
techniques, and model architectures, establishing it as a practical and
generalizable solution for test-time enhancement of efficient diffusion models.

</details>


### [120] [MrCoM: A Meta-Regularized World-Model Generalizing Across Multi-Scenarios](https://arxiv.org/abs/2511.06252)
*Xuantang Xiong,Ni Mu,Runpeng Xie,Senhao Yang,Yaqing Wang,Lexiang Wang,Yao Luan,Siyuan Li,Shuang Xu,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 本文提出了一种名为Meta-Regularized Contextual World-Model (MrCoM)的统一世界模型方法，旨在解决模型强化学习在多场景下的泛化问题，通过动态特征分解和元正则化技术显著提升了跨场景性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于模型的强化学习方法主要关注单任务世界模型构建，很少解决不同场景间的泛化问题。基于同一仿真引擎中动态具有内在共性的洞察，作者试图构建能够跨不同场景泛化的统一世界模型。

Method: MrCoM方法首先基于动态特征将潜在状态空间分解为不同组件，提高世界模型预测精度；采用元状态正则化提取场景相关信息的统一表示；使用元价值正则化在不同场景目标下对齐世界模型优化与策略学习。

Result: 作者理论分析了MrCoM在多场景设置下的泛化误差上界，并系统评估了算法在多样化场景中的泛化能力，证明其性能显著优于先前最先进方法。

Conclusion: MrCoM通过动态分解和元正则化技术成功构建了能够跨场景泛化的统一世界模型，在多样化场景中展现出优越的泛化性能。

Abstract: Model-based reinforcement learning (MBRL) is a crucial approach to enhance
the generalization capabilities and improve the sample efficiency of RL
algorithms. However, current MBRL methods focus primarily on building world
models for single tasks and rarely address generalization across different
scenarios. Building on the insight that dynamics within the same simulation
engine share inherent properties, we attempt to construct a unified world model
capable of generalizing across different scenarios, named Meta-Regularized
Contextual World-Model (MrCoM). This method first decomposes the latent state
space into various components based on the dynamic characteristics, thereby
enhancing the accuracy of world-model prediction. Further, MrCoM adopts
meta-state regularization to extract unified representation of
scenario-relevant information, and meta-value regularization to align
world-model optimization with policy learning across diverse scenario
objectives. We theoretically analyze the generalization error upper bound of
MrCoM in multi-scenario settings. We systematically evaluate our algorithm's
generalization ability across diverse scenarios, demonstrating significantly
better performance than previous state-of-the-art methods.

</details>


### [121] [Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra](https://arxiv.org/abs/2511.06259)
*Yiwen Zhang,Keyan Ding,Yihang Wu,Xiang Zhuang,Yi Yang,Qiang Zhang,Huajun Chen*

Main category: cs.LG

TL;DR: GLMR是一个基于生成语言模型的检索框架，通过两阶段过程解决质谱-分子结构跨模态对齐问题，显著提升了分子结构检索的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有分子结构检索方法存在谱库覆盖有限和跨模态对齐不佳的问题，导致检索精度和泛化能力不足。

Method: 采用两阶段框架：预检索阶段使用对比学习模型识别候选分子作为上下文先验；生成检索阶段整合候选分子和输入质谱，通过生成模型产生精炼分子结构进行重排序。

Result: 在MassSpecGym和MassRET-20k数据集上，GLMR显著优于现有方法，top-1准确率提升超过40%，并展现出强泛化能力。

Conclusion: GLMR框架通过缓解跨模态对齐问题，为从串联质谱中检索分子结构提供了更准确和通用的解决方案。

Abstract: Retrieving molecular structures from tandem mass spectra is a crucial step in
rapid compound identification. Existing retrieval methods, such as traditional
mass spectral library matching, suffer from limited spectral library coverage,
while recent cross-modal representation learning frameworks often encounter
modality misalignment, resulting in suboptimal retrieval accuracy and
generalization. To address these limitations, we propose GLMR, a Generative
Language Model-based Retrieval framework that mitigates the cross-modal
misalignment through a two-stage process. In the pre-retrieval stage, a
contrastive learning-based model identifies top candidate molecules as
contextual priors for the input mass spectrum. In the generative retrieval
stage, these candidate molecules are integrated with the input mass spectrum to
guide a generative model in producing refined molecular structures, which are
then used to re-rank the candidates based on molecular similarity. Experiments
on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR
significantly outperforms existing methods, achieving over 40% improvement in
top-1 accuracy and exhibiting strong generalizability.

</details>


### [122] [Achieving Fairness Without Harm via Selective Demographic Experts](https://arxiv.org/abs/2511.06293)
*Xuwei Tan,Yuanlong Wang,Thai-Hoang Pham,Ping Zhang,Xueru Zhang*

Main category: cs.LG

TL;DR: 提出了一种公平无伤害的方法，通过为不同人口群体学习不同的表示，并通过无伤害约束选择选择性应用人口专家（群体特定表示和个性化分类器），在医疗和面部数据集上实现了公平性而不损害性能。


<details>
  <summary>Details</summary>
Motivation: 在医疗等高风险领域，现有偏见缓解技术通常在公平性和准确性之间进行权衡，这会导致某些人口群体的性能下降，这在伦理和实践上都是不可接受的。

Method: 为不同人口群体学习不同的表示，并通过无伤害约束选择选择性应用人口专家（群体特定表示和个性化分类器）。

Result: 在三个真实世界医疗数据集（眼病、皮肤癌和X射线诊断）以及两个面部数据集上的广泛实证结果表明，该方法在实现公平性而不造成伤害方面是有效的。

Conclusion: 该方法在高风险领域实现了公平性而不损害性能，解决了现有方法在公平性和准确性之间权衡的问题。

Abstract: As machine learning systems become increasingly integrated into
human-centered domains such as healthcare, ensuring fairness while maintaining
high predictive performance is critical. Existing bias mitigation techniques
often impose a trade-off between fairness and accuracy, inadvertently degrading
performance for certain demographic groups. In high-stakes domains like
clinical diagnosis, such trade-offs are ethically and practically unacceptable.
In this study, we propose a fairness-without-harm approach by learning distinct
representations for different demographic groups and selectively applying
demographic experts consisting of group-specific representations and
personalized classifiers through a no-harm constrained selection. We evaluate
our approach on three real-world medical datasets -- covering eye disease, skin
cancer, and X-ray diagnosis -- as well as two face datasets. Extensive
empirical results demonstrate the effectiveness of our approach in achieving
fairness without harm.

</details>


### [123] [Transolver is a Linear Transformer: Revisiting Physics-Attention through the Lens of Linear Attention](https://arxiv.org/abs/2511.06294)
*Wenjie Hu,Sidun Liu,Peng Qiao,Zhenglun Sun,Yong Dou*

Main category: cs.LG

TL;DR: 本文提出Linear Attention Neural Operator (LinearNO)，将Transolver中的Physics-Attention重新设计为规范的线性注意力，在六个标准PDE基准测试中达到最先进性能，同时减少40.0%参数和36.2%计算成本。


<details>
  <summary>Details</summary>
Motivation: 发现Transolver中的Physics-Attention实际上是线性注意力的特例，其切片注意力可能损害模型性能，有效性主要来自切片和反切片操作而非切片间交互。

Method: 通过两步变换将Physics-Attention重新设计为规范的线性注意力，形成LinearNO方法。

Result: 在六个标准PDE基准测试中达到最先进性能，参数减少40.0%，计算成本降低36.2%，在AirfRANS和Shape-Net Car两个工业级数据集上表现优异。

Conclusion: LinearNO通过重新设计注意力机制，在保持性能的同时显著提升了效率，为PDE数据驱动求解器提供了更高效的解决方案。

Abstract: Recent advances in Transformer-based Neural Operators have enabled
significant progress in data-driven solvers for Partial Differential Equations
(PDEs). Most current research has focused on reducing the quadratic complexity
of attention to address the resulting low training and inference efficiency.
Among these works, Transolver stands out as a representative method that
introduces Physics-Attention to reduce computational costs. Physics-Attention
projects grid points into slices for slice attention, then maps them back
through deslicing. However, we observe that Physics-Attention can be
reformulated as a special case of linear attention, and that the slice
attention may even hurt the model performance. Based on these observations, we
argue that its effectiveness primarily arises from the slice and deslice
operations rather than interactions between slices. Building on this insight,
we propose a two-step transformation to redesign Physics-Attention into a
canonical linear attention, which we call Linear Attention Neural Operator
(LinearNO). Our method achieves state-of-the-art performance on six standard
PDE benchmarks, while reducing the number of parameters by an average of 40.0%
and computational cost by 36.2%. Additionally, it delivers superior performance
on two challenging, industrial-level datasets: AirfRANS and Shape-Net Car.

</details>


### [124] [3dSAGER: Geospatial Entity Resolution over 3D Objects (Technical Report)](https://arxiv.org/abs/2511.06300)
*Bar Genossar,Sagi Dalyot,Roee Shraga,Avigdor Gal*

Main category: cs.LG

TL;DR: 3dSAGER是一个用于3D地理空间实体解析的端到端管道，通过几何特征提取和轻量级阻塞方法BKAFI，解决了跨数据源坐标系不兼容时的实体匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有地理空间实体解析方法依赖空间邻近性、文本元数据或外部标识符，但这些信号在跨源场景中常常不可用、不可靠或不对齐，特别是在坐标系不兼容时传统方法会失效。

Method: 提出3dSAGER管道，包含空间参考无关的特征化机制来捕捉匹配对的几何特征，以及BKAFI轻量级阻塞方法用于高效生成高召回候选集。

Result: 在真实世界城市数据集上的实验表明，3dSAGER在准确性和效率上都显著优于强基线方法。

Conclusion: 3dSAGER通过专注于3D空间对象的内在几何特性，为跨数据源的地理空间实体解析提供了鲁棒的解决方案，特别是在传统空间方法失效的场景中表现优异。

Abstract: Urban environments are continuously mapped and modeled by various data
collection platforms, including satellites, unmanned aerial vehicles and street
cameras. The growing availability of 3D geospatial data from multiple
modalities has introduced new opportunities and challenges for integrating
spatial knowledge at scale, particularly in high-impact domains such as urban
planning and rapid disaster management. Geospatial entity resolution is the
task of identifying matching spatial objects across different datasets, often
collected independently under varying conditions. Existing approaches typically
rely on spatial proximity, textual metadata, or external identifiers to
determine correspondence. While useful, these signals are often unavailable,
unreliable, or misaligned, especially in cross-source scenarios. To address
these limitations, we shift the focus to the intrinsic geometry of 3D spatial
objects and present 3dSAGER (3D Spatial-Aware Geospatial Entity Resolution), an
end-to-end pipeline for geospatial entity resolution over 3D objects. 3dSAGER
introduces a novel, spatial-reference-independent featurization mechanism that
captures intricate geometric characteristics of matching pairs, enabling robust
comparison even across datasets with incompatible coordinate systems where
traditional spatial methods fail. As a key component of 3dSAGER, we also
propose a new lightweight and interpretable blocking method, BKAFI, that
leverages a trained model to efficiently generate high-recall candidate sets.
We validate 3dSAGER through extensive experiments on real-world urban datasets,
demonstrating significant gains in both accuracy and efficiency over strong
baselines. Our empirical study further dissects the contributions of each
component, providing insights into their impact and the overall design choices.

</details>


### [125] [Kaggle Chronicles: 15 Years of Competitions, Community and Data Science Innovation](https://arxiv.org/abs/2511.06304)
*Kevin Bönisch,Leandro Losaria*

Main category: cs.LG

TL;DR: 该研究分析了Kaggle平台15年的数据科学发展，通过元数据、共享代码、社区讨论和竞赛本身，探索了Kaggle的增长、对数据科学社区的影响、技术趋势、竞赛获胜者以及用户解决问题的方式。


<details>
  <summary>Details</summary>
Motivation: 随着Kaggle元代码和元数据集的发布，研究者有机会深入探索这些竞赛、技术以及机器学习和AI的实际应用，了解Kaggle作为数据科学平台的发展历程和影响力。

Method: 通过分析数百万个内核和讨论线程，进行纵向趋势分析和标准探索性数据分析。

Result: 研究发现Kaggle是一个稳步增长的平台，用例日益多样化，用户能够快速适应新趋势并将其应用于现实挑战，同时产生的模型具有良好的泛化能力。

Conclusion: 研究提供了Kaggle平台的整体快照，突出了其历史和技术演变，展示了该平台在推动数据科学发展方面的重要作用。

Abstract: Since 2010, Kaggle has been a platform where data scientists from around the
world come together to compete, collaborate, and push the boundaries of Data
Science. Over these 15 years, it has grown from a purely competition-focused
site into a broader ecosystem with forums, notebooks, models, datasets, and
more. With the release of the Kaggle Meta Code and Kaggle Meta Datasets, we now
have a unique opportunity to explore these competitions, technologies, and
real-world applications of Machine Learning and AI. And so in this study, we
take a closer look at 15 years of data science on Kaggle - through metadata,
shared code, community discussions, and the competitions themselves. We explore
Kaggle's growth, its impact on the data science community, uncover hidden
technological trends, analyze competition winners, how Kagglers approach
problems in general, and more. We do this by analyzing millions of kernels and
discussion threads to perform both longitudinal trend analysis and standard
exploratory data analysis. Our findings show that Kaggle is a steadily growing
platform with increasingly diverse use cases, and that Kagglers are quick to
adapt to new trends and apply them to real-world challenges, while producing -
on average - models with solid generalization capabilities. We also offer a
snapshot of the platform as a whole, highlighting its history and technological
evolution. Finally, this study is accompanied by a video
(https://www.youtube.com/watch?v=YVOV9bIUNrM) and a Kaggle write-up
(https://kaggle.com/competitions/meta-kaggle-hackathon/writeups/kaggle-chronicles-15-years-of-competitions-communi)
for your convenience.

</details>


### [126] [DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation](https://arxiv.org/abs/2511.06307)
*Speed Zhu,Jianwei Cai,Guang Chen,Lulu Wu,Saiyong Yang,Wiggin Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种用于竞争性编程代码生成的强化学习验证推理（RLVR）方法，通过两阶段训练流程和精心设计的数据集构建策略，在Qwen2.5-32B模型上实现了与领先系统相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前推理优先模型在数学领域取得进展，但竞争性编程代码生成研究不足，数据构建受到的关注少于强化学习算法设计。本文旨在探索如何构建RLVR数据集并开发实用的训练技术。

Method: 采用监督微调（SFT）增强通用和推理密集型数据，然后进行两阶段强化学习：第一阶段使用GRPO在大量均匀分布的竞争性编程问题上训练，第二阶段在高质量挑战性问题集上进行Pre-GRPO训练，采用硬聚焦课程设计。

Result: 在Qwen2.5-32B模型上评估LeetCode和Codeforces周赛，实现了同类规模模型中最先进的性能，与DeepSeek v3.1和Doubao-1.5-Thinking等领先系统相当。

Conclusion: 研究提炼了数据构建、熵扩展和课程设计的最佳实践，为竞争性编程代码生成的RLVR提供了简洁有效的解决方案。

Abstract: Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a
resurgence of interest in RLVR. Nevertheless, advances are dominated by
mathematics (e.g., AIME), with competitive-programming code generation
underexplored and data curation receiving less attention than RL algorithm
design. We investigate how to construct RLVR datasets (i.e., RL prompts) and
present practical training techniques that yield strong performance on
competitive-programming code generation. Our pipeline begins with supervised
fine-tuning (SFT) distilled from strong open-source models, augmented with
general-purpose and reasoning-intensive data. RL then follows a two-stage
process with executable, testcase-driven rewards: first, training on a large,
uniformly distributed set of competitive-programming problems using Group
Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively
short response-generation window (e.g., 32k during SFT and 24k in this stage)
to expand entropy and mitigate repetition and truncation; second, we perform
\textbf{Pre-GRPO}: updating on a small, high-quality set of challenging
problems with a large rollout budget (64 rollouts per prompt) under a
hard-focus curriculum that continuously retains the most difficult instances
throughout training. We implement our method on Qwen2.5-32B and evaluate on
LeetCode and Codeforces weekly contests to avoid data leakage. The resulting
model achieves state-of-the-art performance among models of similar scale and
is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.
We also examine scaling trends and observe strong RL scaling on an internal
large-scale MoE model. Our study distills concise best practices for data
curation, entropy expansion, and curriculum design in RLVR for
competitive-programming code generation.

</details>


### [127] [Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets](https://arxiv.org/abs/2511.06356)
*Runhan Shi,Letian Chen,Gufeng Yu,Yang Yang*

Main category: cs.LG

TL;DR: ReaDISH是一个新型化学反应预测模型，通过对称差异shingle编码和几何结构交互注意力机制，解决了现有模型对输入排列敏感和子结构交互建模不足的问题，显著提升了预测性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在有机化学反应预测中存在两个关键局限：对输入排列（分子/原子顺序）的敏感性，以及对控制反应性的子结构交互建模不足，导致预测不一致和泛化能力差。

Method: 提出ReaDISH模型，包含两个创新：(1) 对称差异shingle编码，通过计算分子shingle差异来捕捉反应特异性结构变化，消除顺序敏感性；(2) 几何结构交互注意力机制，在shingle级别建模分子内和分子间交互。

Result: 大量实验表明，ReaDISH在多个基准测试中提升了反应预测性能。在排列扰动下，R²指标平均提升了8.76%，显示出增强的鲁棒性。

Conclusion: ReaDISH通过引入排列不变表示和交互感知特征，有效解决了化学反应预测中的关键挑战，为有机化学中的反应预测提供了更可靠和鲁棒的解决方案。

Abstract: Chemical reaction prediction remains a fundamental challenge in organic
chemistry, where existing machine learning models face two critical
limitations: sensitivity to input permutations (molecule/atom orderings) and
inadequate modeling of substructural interactions governing reactivity. These
shortcomings lead to inconsistent predictions and poor generalization to
real-world scenarios. To address these challenges, we propose ReaDISH, a novel
reaction prediction model that learns permutation-invariant representations
while incorporating interaction-aware features. It introduces two innovations:
(1) symmetric difference shingle encoding, which computes molecular shingle
differences to capture reaction-specific structural changes while eliminating
order sensitivity; and (2) geometry-structure interaction attention, a
mechanism that models intra- and inter-molecular interactions at the shingle
level. Extensive experiments demonstrate that ReaDISH improves reaction
prediction performance across diverse benchmarks. It shows enhanced robustness
with an average improvement of 8.76% on R$^2$ under permutation perturbations.

</details>


### [128] [Adaptive Regularization for Large-Scale Sparse Feature Embedding Models](https://arxiv.org/abs/2511.06374)
*Mang Li,Wei Lyu*

Main category: cs.LG

TL;DR: 本文针对CTR和CVR预估模型中的一周期过拟合问题进行了理论分析，提出了自适应正则化方法来解决大规模稀疏分类特征导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在搜索、广告和推荐领域的CTR和CVR预估模型中，依赖大规模稀疏分类特征的模型在训练多个周期时经常出现性能显著下降的问题，现有启发式解决方案未能明确识别这一现象的根本原因。

Method: 首先对大规模稀疏分类特征导致过拟合的原因进行理论分析，然后基于分析提出自适应正则化方法。

Result: 该方法不仅防止了多周期训练中观察到的严重性能下降，还提高了单周期内的模型性能，并已在在线生产系统中部署。

Conclusion: 通过理论分析识别了大规模稀疏分类特征导致过拟合的根本原因，提出的自适应正则化方法有效解决了这一问题，在实际应用中取得了良好效果。

Abstract: The one-epoch overfitting problem has drawn widespread attention, especially
in CTR and CVR estimation models in search, advertising, and recommendation
domains. These models which rely heavily on large-scale sparse categorical
features, often suffer a significant decline in performance when trained for
multiple epochs. Although recent studies have proposed heuristic solutions,
they have not clearly identified the fundamental cause of this phenomenon. In
this work, we provide a theoretical analysis that explains why overfitting
occurs in models that use large-scale sparse categorical features. Based on
this analysis, we propose an adaptive regularization method to address it. Our
approach not only prevents the severe performance degradation observed during
multi-epoch training, but also improves model performance within a single
epoch. This method has already been deployed in online production systems.

</details>


### [129] [Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding](https://arxiv.org/abs/2511.06376)
*Qian Ma,Ruoxiang Xu,Yongqiang Cai*

Main category: cs.LG

TL;DR: 本文研究了词汇表上下文学习(VICL)在Transformer中的通用逼近性质，发现无位置编码的单层Transformer不具备UAP，但加入位置编码后可以实现UAP。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer架构在词汇表上下文学习场景下的通用逼近能力，特别关注位置编码对逼近性质的影响。

Method: 理论分析单层Transformer在有无位置编码情况下的函数逼近能力，提供位置编码实现UAP的充分条件。

Result: 无位置编码的单层Transformer在VICL中不具备UAP，但加入合适的位置编码后可以具备UAP。

Conclusion: 位置编码从逼近理论角度为上下文学习提供了重要益处，是实现通用逼近性质的关键因素。

Abstract: Numerous studies have demonstrated that the Transformer architecture
possesses the capability for in-context learning (ICL). In scenarios involving
function approximation, context can serve as a control parameter for the model,
endowing it with the universal approximation property (UAP). In practice,
context is represented by tokens from a finite set, referred to as a
vocabulary, which is the case considered in this paper, \emph{i.e.}, vocabulary
in-context learning (VICL). We demonstrate that VICL in single-layer
Transformers, without positional encoding, does not possess the UAP; however,
it is possible to achieve the UAP when positional encoding is included. Several
sufficient conditions for the positional encoding are provided. Our findings
reveal the benefits of positional encoding from an approximation theory
perspective in the context of ICL.

</details>


### [130] [CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models](https://arxiv.org/abs/2511.06430)
*Peyman Hosseini,Ondrej Bohdal,Taha Ceritli,Ignacio Castro,Matthew Purver,Mete Ozay,Umberto Michieli*

Main category: cs.LG

TL;DR: 本文提出了上下文引导的测试时强化学习(CG-TTRL)，通过将上下文动态集成到两阶段采样中，改进了原有的测试时强化学习方法，在数学和科学问答基准测试中取得了更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时强化学习(TTRL)方法在两阶段采样策略中未能充分利用上下文指导，而上下文学习已被证明能在不更新权重的情况下提升模型性能。作者希望将上下文指导整合到TTRL中，以改进初始阶段的伪标签准确性和调节第二阶段的探索。

Method: 提出了上下文引导的TTRL(CG-TTRL)，将上下文动态集成到两阶段采样中：在初始利用阶段使用上下文提高伪标签准确性，在第二阶段使用上下文调节探索。同时提出了适用于设备端应用的高效上下文选择方法。

Result: 在数学和科学问答基准测试中，CG-TTRL显著优于TTRL（相对准确率提升7%），并且在仅进行少量测试时训练步骤后就能获得强劲性能（3步后相对提升8%，而TTRL仅1%）。

Conclusion: CG-TTRL通过有效整合上下文指导，不仅提升了测试时强化学习的性能，还显著提高了训练效率，为设备端应用提供了实用的解决方案。

Abstract: Test-time Reinforcement Learning (TTRL) has shown promise in adapting
foundation models for complex tasks at test-time, resulting in large
performance improvements. TTRL leverages an elegant two-phase sampling
strategy: first, multi-sampling derives a pseudo-label via majority voting,
while subsequent downsampling and reward-based fine-tuning encourages the model
to explore and learn diverse valid solutions, with the pseudo-label modulating
the reward signal. Meanwhile, in-context learning has been widely explored at
inference time and demonstrated the ability to enhance model performance
without weight updates. However, TTRL's two-phase sampling strategy
under-utilizes contextual guidance, which can potentially improve pseudo-label
accuracy in the initial exploitation phase while regulating exploration in the
second. To address this, we propose context-guided TTRL (CG-TTRL), integrating
context dynamically into both sampling phases and propose a method for
efficient context selection for on-device applications. Our evaluations on
mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g.
additional 7% relative accuracy improvement over TTRL), while boosting
efficiency by obtaining strong performance after only a few steps of test-time
training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).

</details>


### [131] [FLEX: Continuous Agent Evolution via Forward Learning from Experience](https://arxiv.org/abs/2511.06449)
*Zhicheng Cai,Xinyuan Guo,Yu Pei,JiangTao Feng,Jiangjie Chen,Ya-Qin Zhang,Wei-Ying Ma,Mingxuan Wang,Hao Zhou*

Main category: cs.LG

TL;DR: FLEX是一种无需梯度的学习范式，使LLM驱动的自主智能体能够通过积累经验持续进化，在数学推理、化学逆合成和蛋白质适应性预测等任务上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主智能体在训练后保持静态，无法像智能生物一样在部署过程中通过经验成长，这限制了其持续进化的能力。

Method: FLEX通过持续反思与环境交互中的成功和失败，构建结构化经验库，实现可扩展和可继承的进化。

Result: 在AIME25上提升23%，USPTO50k上提升10%，ProteinGym上提升14%，并发现了经验增长的缩放规律和跨智能体经验继承现象。

Conclusion: FLEX标志着向可扩展和可继承的连续智能体进化迈出了一步，为LLM智能体的持续学习提供了新范式。

Abstract: Autonomous agents driven by Large Language Models (LLMs) have revolutionized
reasoning and problem-solving but remain static after training, unable to grow
with experience as intelligent beings do during deployment. We introduce
Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that
enables LLM agents to continuously evolve through accumulated experience.
Specifically, FLEX cultivates scalable and inheritable evolution by
constructing a structured experience library through continual reflection on
successes and failures during interaction with the environment. FLEX delivers
substantial improvements on mathematical reasoning, chemical retrosynthesis,
and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14%
on ProteinGym). We further identify a clear scaling law of experiential growth
and the phenomenon of experience inheritance across agents, marking a step
toward scalable and inheritable continuous agent evolution. Project Page:
https://flex-gensi-thuair.github.io.

</details>


### [132] [A Risk-Neutral Neural Operator for Arbitrage-Free SPX-VIX Term Structures](https://arxiv.org/abs/2511.06451)
*Jian'an Zhang*

Main category: cs.LG

TL;DR: ARBITER是一个风险中性神经算子，用于在无套利约束下学习SPX-VIX联合期限结构。它通过约束解码器和外梯度更新来确保静态套利约束，并在历史数据上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够学习SPX-VIX联合期限结构的神经算子模型，同时确保满足各种无套利约束条件，以改进衍生品定价和风险管理。

Method: 结合算子学习与约束解码器，使用外梯度风格更新加投影方法进行训练，强制执行静态套利约束、Lipschitz边界和单调性。

Result: 在历史SPX和VIX数据上优于傅里叶神经算子、DeepONet和状态空间序列模型，消融研究表明绑定SPX和VIX分支能减少对偶间隙并改进NI。

Conclusion: ARBITER提供了一个实用的无套利插值和推断方法，在跨期限和行权价范围内实现了稳定的校准和良好的泛化性能。

Abstract: We propose ARBITER, a risk-neutral neural operator for learning joint SPX-VIX
term structures under no-arbitrage constraints. ARBITER maps market states to
an operator that outputs implied volatility and variance curves while enforcing
static arbitrage (calendar, vertical, butterfly), Lipschitz bounds, and
monotonicity. The model couples operator learning with constrained decoders and
is trained with extragradient-style updates plus projection. We introduce
evaluation metrics for derivatives term structures (NAS, CNAS, NI, Dual-Gap,
Stability Rate) and show gains over Fourier Neural Operator, DeepONet, and
state-space sequence models on historical SPX and VIX data. Ablation studies
indicate that tying the SPX and VIX legs reduces Dual-Gap and improves NI,
Lipschitz projection stabilizes calibration, and selective state updates
improve long-horizon generalization. We provide identifiability and
approximation results and describe practical recipes for arbitrage-free
interpolation and extrapolation across maturities and strikes.

</details>


### [133] [MULTIBENCH++: A Unified and Comprehensive Multimodal Fusion Benchmarking Across Specialized Domains](https://arxiv.org/abs/2511.06452)
*Leyan Xue,Zongbo Han,Kecheng Xue,Xiaohong Liu,Guangyu Wang,Changqing Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一个大规模、领域自适应的多模态评估基准，整合了30多个数据集、15种模态和20个预测任务，并开发了开源统一评估流水线，旨在解决当前多模态融合评估基准不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态融合方法缺乏足够的评估基准，通常在有限的数据集上进行评估，无法充分代表现实世界的复杂性和多样性，导致评估偏差，阻碍了通用高性能融合模型的发展。

Method: 开发大规模领域自适应多模态评估基准，整合30+数据集、15种模态和20个预测任务；构建开源统一自动化评估流水线，包含标准化实现的最先进模型和多样化融合范式。

Result: 通过该平台进行了大规模实验，成功在多个任务上建立了新的性能基准，为多模态模型提供了严格可复现的评估平台。

Conclusion: 这项工作为学术界提供了关键的多模态模型评估平台，旨在推动多模态人工智能领域达到新的高度。

Abstract: Although multimodal fusion has made significant progress, its advancement is
severely hindered by the lack of adequate evaluation benchmarks. Current fusion
methods are typically evaluated on a small selection of public datasets, a
limited scope that inadequately represents the complexity and diversity of
real-world scenarios, potentially leading to biased evaluations. This issue
presents a twofold challenge. On one hand, models may overfit to the biases of
specific datasets, hindering their generalization to broader practical
applications. On the other hand, the absence of a unified evaluation standard
makes fair and objective comparisons between different fusion methods
difficult. Consequently, a truly universal and high-performance fusion model
has yet to emerge. To address these challenges, we have developed a
large-scale, domain-adaptive benchmark for multimodal evaluation. This
benchmark integrates over 30 datasets, encompassing 15 modalities and 20
predictive tasks across key application domains. To complement this, we have
also developed an open-source, unified, and automated evaluation pipeline that
includes standardized implementations of state-of-the-art models and diverse
fusion paradigms. Leveraging this platform, we have conducted large-scale
experiments, successfully establishing new performance baselines across
multiple tasks. This work provides the academic community with a crucial
platform for rigorous and reproducible assessment of multimodal models, aiming
to propel the field of multimodal artificial intelligence to new heights.

</details>


### [134] [Explainable AI For Early Detection Of Sepsis](https://arxiv.org/abs/2511.06492)
*Atharva Thakur,Shruti Dhumal*

Main category: cs.LG

TL;DR: 提出一种可解释的AI方法用于脓毒症分析，将机器学习与临床知识相结合，不仅提供准确的脓毒症发作预测，还能让临床医生理解、验证模型输出并与医学专业知识对齐。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是一种危及生命的疾病，需要快速检测和治疗。虽然机器学习模型在预测脓毒症发作方面显示出潜力，但其黑盒性质限制了可解释性和临床信任度。

Method: 开发了一种可解释的AI方法，将机器学习与临床知识集成，使模型输出能够被临床医生理解和验证。

Result: 该方法能够准确预测脓毒症发作，同时提供可解释的结果，使临床医生能够理解模型决策过程。

Conclusion: 提出的可解释AI方法在保持准确性的同时提高了临床信任度，有助于脓毒症的早期检测和治疗。

Abstract: Sepsis is a life-threatening condition that requires rapid detection and
treatment to prevent progression to severe sepsis, septic shock, or multi-organ
failure. Despite advances in medical technology, it remains a major challenge
for clinicians. While recent machine learning models have shown promise in
predicting sepsis onset, their black-box nature limits interpretability and
clinical trust. In this study, we present an interpretable AI approach for
sepsis analysis that integrates machine learning with clinical knowledge. Our
method not only delivers accurate predictions of sepsis onset but also enables
clinicians to understand, validate, and align model outputs with established
medical expertise.

</details>


### [135] [Route Experts by Sequence, not by Token](https://arxiv.org/abs/2511.06494)
*Tiansheng Wen,Yifei Wang,Aosong Feng,Long Ma,Xinyang Liu,Yifan Wang,Lixuan Guo,Bo Chen,Stefanie Jegelka,Chenyu You*

Main category: cs.LG

TL;DR: SeqTopK是一种改进的MoE路由策略，将专家预算从token级别转移到序列级别，实现动态专家分配而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 标准TopK路由为所有token分配固定数量的专家，忽略了token复杂度的差异，而现有自适应路由方法需要额外模块和重新训练。

Method: 通过选择序列中所有token的前T·K个专家，实现端到端的动态专家分配，保持总体预算不变。

Result: 在数学、编程、法律和写作任务上相比TopK和现有参数无关方法都有持续改进，在高稀疏度下增益可达16.9%。

Conclusion: SeqTopK是一种简单、高效且可扩展的路由策略，特别适合下一代LLM的极端稀疏场景。

Abstract: Mixture-of-Experts (MoE) architectures scale large language models (LLMs) by
activating only a subset of experts per token, but the standard TopK routing
assigns the same fixed number of experts to all tokens, ignoring their varying
complexity. Prior adaptive routing methods introduce additional modules and
hyperparameters, often requiring costly retraining from scratch. We propose
Sequence-level TopK (SeqTopK), a minimal modification that shifts the expert
budget from the token level to the sequence level. By selecting the top $T
\cdot K$ experts across all $T$ tokens, SeqTopK enables end-to-end learned
dynamic allocation -- assigning more experts to difficult tokens and fewer to
easy ones -- while preserving the same overall budget. SeqTopK requires only a
few lines of code, adds less than 1% overhead, and remains fully compatible
with pretrained MoE models. Experiments across math, coding, law, and writing
show consistent improvements over TopK and prior parameter-free adaptive
methods, with gains that become substantially larger under higher sparsity (up
to 16.9%). These results highlight SeqTopK as a simple, efficient, and scalable
routing strategy, particularly well-suited for the extreme sparsity regimes of
next-generation LLMs. Code is available at
https://github.com/Y-Research-SBU/SeqTopK.

</details>


### [136] [Error Estimate and Convergence Analysis for Data Valuation](https://arxiv.org/abs/2511.06463)
*Zhangyong Liang,Huanhuan Gao,Ji Zhang*

Main category: cs.LG

TL;DR: 该论文基于神经动态数据估值方法，首次探索了数据估值中的误差估计和收敛性分析，在Lipschitz和平滑性假设下推导了二次误差界，并证明了训练损失梯度范数的渐近消失以及元损失的次线性收敛。


<details>
  <summary>Details</summary>
Motivation: 现有数据估值方法无法在单次训练过程中确保有效性，神经动态数据估值方法解决了这一限制，需要进一步探索其误差估计和收敛性分析。

Method: 基于神经动态数据估值方法，在Lipschitz和平滑性假设下进行理论分析，推导误差界和收敛性结果。

Result: 推导了损失差异的二次误差界，该误差界与时间步长成反比、与控制变量变化成二次关系；证明了训练损失梯度范数渐近消失，元损失在迭代中次线性收敛。

Conclusion: 神经动态数据估值方法实现了次线性收敛，在理论分析上确保了数据估值过程的稳定性和收敛性。

Abstract: Data valuation quantifies data importance, but existing methods cannot ensure
validity in a single training process. The neural dynamic data valuation (NDDV)
method [3] addresses this limitation. Based on NDDV, we are the first to
explore error estimation and convergence analysis in data valuation. Under
Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss
differences that scale inversely with time steps and quadratically with control
variations, ensuring stability. We also prove that the expected squared
gradient norm for the training loss vanishes asymptotically, and that the meta
loss converges sublinearly over iterations. In particular, NDDV achieves
sublinear convergence.

</details>


### [137] [DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning](https://arxiv.org/abs/2511.06477)
*Nikolay Yudin,Ekaterina Grishina,Andrey Veprikov,Alexandr Beznosikov,Maxim Rakhuba*

Main category: cs.LG

TL;DR: 本文提出了DyKAF优化器，利用投影分裂积分器构建有效的预处理器，在大型语言模型预训练和微调中优于现有优化器。


<details>
  <summary>Details</summary>
Motivation: 现有的将权重视为矩阵而非展平向量的优化器虽然有效，但构建高效准确的Fisher矩阵结构化近似仍是一个挑战，因为获得最优分解需要大量资源，而实际方法依赖启发式设计选择。

Method: 引入投影分裂积分器来构建有效的预处理器，提出DyKAF优化器，动态近似Fisher矩阵的Kronecker分解形式。

Result: DyKAF持续提高了Fisher矩阵近似质量，在大型语言模型预训练和微调实验中，在各种评估指标上都优于现有优化器。

Conclusion: DyKAF方法通过投影分裂积分器有效解决了Fisher矩阵结构化近似的挑战，在实践应用中表现出优越性能。

Abstract: Recently, optimizers that explicitly treat weights as matrices, rather than
flattened vectors, have demonstrated their effectiveness. This perspective
naturally leads to structured approximations of the Fisher matrix as
preconditioners, where the matrix view induces a Kronecker-factorized form that
enables memory-efficient representation. However, constructing such
approximations both efficiently and accurately remains an open challenge, since
obtaining the optimal factorization is resource-intensive and practical methods
therefore rely on heuristic design choices. In this work, we introduce a novel
approach that leverages projector-splitting integrators to construct effective
preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the
Fisher Matrix), consistently improves the Fisher matrix approximation quality.
Experiments on large language model pre-training and fine-tuning demonstrate
that DyKAF outperforms existing optimizers across a range of evaluation
metrics.

</details>


### [138] [Breaking the Dyadic Barrier: Rethinking Fairness in Link Prediction Beyond Demographic Parity](https://arxiv.org/abs/2511.06568)
*João Mattos,Debolina Halder Lina,Arlei Silva*

Main category: cs.LG

TL;DR: 本文指出现有图机器学习中链接预测公平性评估的局限性，提出新的评估框架和轻量级后处理方法，实现更好的公平性-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有链接预测公平性采用二元定义，可能掩盖子群体间的潜在差异，且人口均等性不适用于排名任务，需要更准确的公平性评估方法。

Method: 提出新的公平性评估框架，并开发轻量级后处理方法结合解耦链接预测器来减轻偏见。

Result: 该方法有效缓解偏见，在公平性-效用权衡方面达到最先进水平。

Conclusion: 现有二元公平性框架存在局限性，需要更细粒度的评估方法，提出的框架和后处理方法能更好地解决链接预测中的公平性问题。

Abstract: Link prediction is a fundamental task in graph machine learning with
applications, ranging from social recommendation to knowledge graph completion.
Fairness in this setting is critical, as biased predictions can exacerbate
societal inequalities. Prior work adopts a dyadic definition of fairness,
enforcing fairness through demographic parity between intra-group and
inter-group link predictions. However, we show that this dyadic framing can
obscure underlying disparities across subgroups, allowing systemic biases to go
undetected. Moreover, we argue that demographic parity does not meet desired
properties for fairness assessment in ranking-based tasks such as link
prediction. We formalize the limitations of existing fairness evaluations and
propose a framework that enables a more expressive assessment. Additionally, we
propose a lightweight post-processing method combined with decoupled link
predictors that effectively mitigates bias and achieves state-of-the-art
fairness-utility trade-offs.

</details>


### [139] [CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction](https://arxiv.org/abs/2511.06634)
*Kaiyuan Zhai,Jiacheng Cui,Zhehao Zhang,Junyu Xue,Yang Deng,Kui Wu,Guoming Tang*

Main category: cs.LG

TL;DR: CaberNet是一种因果可解释的深度序列模型，用于解决跨域HVAC能耗预测中的数据稀缺和异质性问题，通过全局特征门控和域平衡训练方案学习不变表示，在三个不同气候建筑数据集上表现优于所有基线方法。


<details>
  <summary>Details</summary>
Motivation: 跨域HVAC能耗预测对于可扩展的建筑能源管理至关重要，但为每个新建筑收集大量标记数据成本高昂且不切实际。现有方法容易过拟合虚假相关性、依赖专家干预或牺牲数据多样性。

Method: CaberNet结合了：1）通过自监督伯努利正则化训练的全局特征门控，区分因果特征；2）域平衡训练方案，平衡域贡献、最小化跨域损失方差并促进潜在因子独立性。

Result: 在三个不同气候城市的真实建筑数据集上评估，CaberNet始终优于所有基线方法，相比最佳基准实现了22.9%的归一化均方误差（NMSE）降低。

Conclusion: CaberNet提供了一种纯粹数据驱动的方法，无需先验知识即可学习不变表示，为跨域HVAC能耗预测提供了稳健且可解释的解决方案。

Abstract: Cross-domain HVAC energy prediction is essential for scalable building energy
management, particularly because collecting extensive labeled data for every
new building is both costly and impractical. Yet, this task remains highly
challenging due to the scarcity and heterogeneity of data across different
buildings, climate zones, and seasonal patterns. In particular, buildings
situated in distinct climatic regions introduce variability that often leads
existing methods to overfit to spurious correlations, rely heavily on expert
intervention, or compromise on data diversity. To address these limitations, we
propose CaberNet, a causal and interpretable deep sequence model that learns
invariant (Markov blanket) representations for robust cross-domain prediction.
In a purely data-driven fashion and without requiring any prior knowledge,
CaberNet integrates i) a global feature gate trained with a self-supervised
Bernoulli regularization to distinguish superior causal features from inferior
ones, and ii) a domain-wise training scheme that balances domain contributions,
minimizes cross-domain loss variance, and promotes latent factor independence.
We evaluate CaberNet on real-world datasets collected from three buildings
located in three climatically diverse cities, and it consistently outperforms
all baselines, achieving a 22.9\% reduction in normalized mean squared error
(NMSE) compared to the best benchmark. Our code is available at
https://github.com/rickzky1001/CaberNet-CRL.

</details>


### [140] [ML-EcoLyzer: Quantifying the Environmental Cost of Machine Learning Inference Across Frameworks and Hardware](https://arxiv.org/abs/2511.06694)
*Jose Marie Antonio Minoza,Rex Gregor Laylo,Christian F Villarin,Sebastian C. Ibanez*

Main category: cs.LG

TL;DR: ML-EcoLyzer是一个跨框架工具，用于测量机器学习推理在CPU、消费级GPU和数据中心加速器上的碳、能源、热和水成本，并引入环境可持续性评分来量化每克二氧化碳排放所能服务的有效参数数量。


<details>
  <summary>Details</summary>
Motivation: 机器学习推理规模巨大但其环境影响缺乏量化，特别是在低资源硬件上，需要工具来测量和评估推理的环境成本。

Method: 开发ML-EcoLyzer工具，支持经典和现代模型，应用自适应监控和硬件感知评估，涵盖1900多种推理配置，包括不同模型架构、任务模态、硬件类型和精度级别。

Result: 量化结果显示量化技术可提高环境可持续性评分，大型加速器在轻量级应用中可能效率低下，即使小型模型在实施不当时也可能产生显著成本。

Conclusion: ML-EcoLyzer为可持续性意识模型选择设定了标准，并提供了推理过程中环境成本的广泛实证评估。

Abstract: Machine learning inference occurs at a massive scale, yet its environmental
impact remains poorly quantified, especially on low-resource hardware. We
present ML-EcoLyzer, a cross-framework tool for measuring the carbon, energy,
thermal, and water costs of inference across CPUs, consumer GPUs, and
datacenter accelerators. The tool supports both classical and modern models,
applying adaptive monitoring and hardware-aware evaluation.
  We introduce the Environmental Sustainability Score (ESS), which quantifies
the number of effective parameters served per gram of CO$_2$ emitted. Our
evaluation covers over 1,900 inference configurations, spanning diverse model
architectures, task modalities (text, vision, audio, tabular), hardware types,
and precision levels. These rigorous and reliable measurements demonstrate that
quantization enhances ESS, huge accelerators can be inefficient for lightweight
applications, and even small models may incur significant costs when
implemented suboptimally. ML-EcoLyzer sets a standard for
sustainability-conscious model selection and offers an extensive empirical
evaluation of environmental costs during inference.

</details>


### [141] [Sensor Calibration Model Balancing Accuracy, Real-time, and Efficiency](https://arxiv.org/abs/2511.06715)
*Jinyong Yun,Hyungjin Kim,Seokho Ahn,Euijong Lee,Young-Duk Seo*

Main category: cs.LG

TL;DR: Scare是一个超压缩的Transformer模型，专门用于设备端传感器校准，通过三个核心组件（序列透镜投影器、高效位注意力模块和哈希优化策略）同时满足八个微观需求，在保持高精度的同时最小化计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有设备端传感器校准研究仅关注三个宏观需求（精度、实时性和资源效率），但隐藏了瞬时误差和最坏情况延迟等部署瓶颈。因此需要将这些宏观需求分解为八个微观需求，并开发能够同时满足所有这些需求的模型。

Method: Scare包含三个核心组件：1）序列透镜投影器（SLP）对数压缩时间序列数据同时保留边界信息；2）高效位注意力模块（EBA）通过二进制哈希码用位运算替代昂贵的乘法运算；3）哈希优化策略确保稳定训练而无需辅助损失项。

Result: 在大规模空气质量数据集和真实微控制器部署上的广泛实验表明，Scare在线性、混合和深度学习基线方法中表现最优，成为首个同时满足所有八个微观需求的模型。

Conclusion: Scare是第一个能够同时满足设备端传感器校准所有八个微观需求的模型，在保持高精度的同时实现了计算效率的显著提升，具有良好的微控制器兼容性。

Abstract: Most on-device sensor calibration studies benchmark models only against three
macroscopic requirements (i.e., accuracy, real-time, and resource efficiency),
thereby hiding deployment bottlenecks such as instantaneous error and
worst-case latency. We therefore decompose this triad into eight microscopic
requirements and introduce Scare (Sensor Calibration model balancing Accuracy,
Real-time, and Efficiency), an ultra-compressed transformer that fulfills them
all. SCARE comprises three core components: (1) Sequence Lens Projector (SLP)
that logarithmically compresses time-series data while preserving boundary
information across bins, (2) Efficient Bitwise Attention (EBA) module that
replaces costly multiplications with bitwise operations via binary hash codes,
and (3) Hash optimization strategy that ensures stable training without
auxiliary loss terms. Together, these components minimize computational
overhead while maintaining high accuracy and compatibility with microcontroller
units (MCUs). Extensive experiments on large-scale air-quality datasets and
real microcontroller deployments demonstrate that Scare outperforms existing
linear, hybrid, and deep-learning baselines, making Scare, to the best of our
knowledge, the first model to meet all eight microscopic requirements
simultaneously.

</details>


### [142] [Rank-1 LoRAs Encode Interpretable Reasoning Signals](https://arxiv.org/abs/2511.06739)
*Jake Ward,Paul Riechers,Adam Shai*

Main category: cs.LG

TL;DR: 研究发现推理模型性能提升主要源于基模型参数的微小单秩变化，通过rank-1 LoRA适配器即可恢复73-90%的推理基准性能，这些变化具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管推理模型在困难逻辑任务上显著提升了语言模型性能并被广泛采用，但其性能增强机制尚未被充分理解。

Method: 使用rank-1 LoRA为Qwen-2.5-32B-Instruct创建最小参数适配器，并在该LoRA的激活状态上训练稀疏自编码器来识别细粒度特征。

Result: rank-1 LoRA适配器恢复了73-90%的推理基准性能，其激活与MLP神经元一样可解释，且针对推理特定行为触发。

Conclusion: 推理性能主要源于基模型参数的微小变化，参数高效训练方法可作为揭示语言模型行为动态的有针对性工具。

Abstract: Reasoning models leverage inference-time compute to significantly enhance the
performance of language models on difficult logical tasks, and have become a
dominating paradigm in frontier LLMs. Despite their wide adoption, the
mechanisms underpinning the enhanced performance of these reasoning models are
not well understood. In this work, we show that the majority of new
capabilities in reasoning models can be elicited by small, single-rank changes
to base model parameters, with many of these changes being interpretable.
Specifically, we use a rank-1 LoRA to create a minimal parameter adapter for
Qwen-2.5-32B-Instruct which recovers 73-90% of reasoning-benchmark performance
compared to a full parameter finetune. We find that the activations of this
LoRA are as interpretable as MLP neurons, and fire for reasoning-specific
behaviors. Finally, we train a sparse autoencoder on the entire activation
state of this LoRA and identify fine-grained and monosemantic features. Our
findings highlight that reasoning performance can arise largely from minimal
changes to base model parameters, and explore what these changes affect. More
broadly, our work shows that parameter-efficient training methods can be used
as a targeted lens for uncovering fundamental insights about language model
behavior and dynamics.

</details>


### [143] [Bayesian Uncertainty Quantification with Anchored Ensembles for Robust EV Power Consumption Prediction](https://arxiv.org/abs/2511.06538)
*Ghazal Farhani,Taufiq Rahman,Kieran Humphries*

Main category: cs.LG

TL;DR: 提出了一种基于锚定集成LSTM和Student-t似然的方法，用于电动汽车功率估计，同时捕捉模型和数据不确定性，实现高精度和校准良好的预测区间。


<details>
  <summary>Details</summary>
Motivation: 电动汽车功率估计需要既准确又可信的不确定性量化，以支持续航预测和能源管理决策。

Method: 使用锚定集成LSTM结合Student-t似然函数，通过高斯权重先验实现后验多样性，无需测试时采样，t-head提供重尾鲁棒性和闭式预测区间。

Result: 在车辆运动学时间序列数据上，模型达到RMSE 3.36±1.10，MAE 2.21±0.89，R²=0.93±0.02，解释方差0.93±0.02，提供良好校准的不确定性区间。

Conclusion: 该方法在准确性、校准性和系统效率方面实现平衡，为生产级电动汽车能源管理提供可靠的续航估计和决策支持。

Abstract: Accurate EV power estimation underpins range prediction and energy
management, yet practitioners need both point accuracy and trustworthy
uncertainty. We propose an anchored-ensemble Long Short-Term Memory (LSTM) with
a Student-t likelihood that jointly captures epistemic (model) and aleatoric
(data) uncertainty. Anchoring imposes a Gaussian weight prior (MAP training),
yielding posterior-like diversity without test-time sampling, while the t-head
provides heavy-tailed robustness and closed-form prediction intervals. Using
vehicle-kinematic time series (e.g., speed, motor RPM), our model attains
strong accuracy: RMSE 3.36 +/- 1.10, MAE 2.21 +/- 0.89, R-squared = 0.93 +/-
0.02, explained variance 0.93 +/- 0.02, and delivers well-calibrated
uncertainty bands with near-nominal coverage. Against competitive baselines
(Student-t MC dropout; quantile regression with/without anchoring), our method
matches or improves log-scores while producing sharper intervals at the same
coverage. Crucially for real-time deployment, inference is a single
deterministic pass per ensemble member (or a weight-averaged collapse),
eliminating Monte Carlo latency. The result is a compact, theoretically
grounded estimator that couples accuracy, calibration, and systems efficiency,
enabling reliable range estimation and decision-making for production EV energy
management.

</details>


### [144] [Implicit Federated In-context Learning For Task-Specific LLM Fine-Tuning](https://arxiv.org/abs/2511.06757)
*Dongcheng Li,Junhan Chen,Aoxiang Zhou,Chunpei Li,Youquan Xian,Peng Liu,Xianxian Li*

Main category: cs.LG

TL;DR: IFed-ICL是一种基于联邦学习的新型分布式协作框架，通过将客户端本地上下文示例转换为隐式向量表示，在推理阶段实现分布式协作计算，并注入模型残差流来提升性能，避免了传统微调方法的大量参数更新需求。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，公共数据面临枯竭风险，如何利用组织内部私有数据提升大模型性能成为关键挑战。联邦学习结合模型微调虽然减少了可训练参数，但处理高维特征空间仍带来巨大计算开销。

Method: 提出隐式联邦上下文学习框架，从联邦学习获得灵感建立分布式协作范式，将客户端本地上下文示例转换为隐式向量表示，在推理阶段实现分布式协作计算，并注入模型残差流。

Result: 实验表明该方法在多个文本分类任务中表现出色，相比传统方法避免了大量参数更新，同时减少了联邦学习中客户端的数据传输和本地计算。

Conclusion: IFed-ICL能够利用本地私有域数据实现高效的分布式上下文学习，显著提升模型在特定任务上的性能。

Abstract: As large language models continue to develop and expand, the extensive public
data they rely on faces the risk of depletion. Consequently, leveraging private
data within organizations to enhance the performance of large models has
emerged as a key challenge. The federated learning paradigm, combined with
model fine-tuning techniques, effectively reduces the number of trainable
parameters. However,the necessity to process high-dimensional feature spaces
results in substantial overall computational overhead. To address this issue,
we propose the Implicit Federated In-Context Learning (IFed-ICL) framework.
IFed-ICL draws inspiration from federated learning to establish a novel
distributed collaborative paradigm, by converting client local context examples
into implicit vector representations, it enables distributed collaborative
computation during the inference phase and injects model residual streams to
enhance model performance. Experiments demonstrate that our proposed method
achieves outstanding performance across multiple text classification tasks.
Compared to traditional methods, IFed-ICL avoids the extensive parameter
updates required by conventional fine-tuning methods while reducing data
transmission and local computation at the client level in federated learning.
This enables efficient distributed context learning using local private-domain
data, significantly improving model performance on specific tasks.

</details>


### [145] [QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations](https://arxiv.org/abs/2511.06767)
*Zhixiong Zhao,Haomin Li,Fangxin Liu,Yuncheng Lu,Zongwu Wang,Tao Yang,Li Jiang,Haibing Guan*

Main category: cs.LG

TL;DR: QUARK是一个基于量化的FPGA加速框架，通过共享非线性操作电路来降低Transformer模型的推理延迟和硬件资源需求，在保持模型精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在CV和NLP领域取得了最先进的性能，但其中的非线性操作显著增加了推理延迟，给硬件加速带来了独特挑战。

Method: 提出QUARK框架，利用非线性操作中的共同模式实现电路共享，通过新颖的电路共享设计专门加速Transformer模型中的所有非线性操作。

Result: QUARK显著降低了主流Transformer架构中非线性算子的计算开销，相比GPU实现实现了1.96倍的端到端加速，并将非线性模块的硬件开销降低了50%以上，同时在超低位量化下还能显著提升模型精度。

Conclusion: QUARK框架通过量化驱动的FPGA加速和电路共享设计，有效解决了Transformer模型中非线性操作的效率瓶颈，为高效硬件加速提供了可行方案。

Abstract: Transformer-based models have revolutionized computer vision (CV) and natural
language processing (NLP) by achieving state-of-the-art performance across a
range of benchmarks. However, nonlinear operations in models significantly
contribute to inference latency, presenting unique challenges for efficient
hardware acceleration. To this end, we propose QUARK, a quantization-enabled
FPGA acceleration framework that leverages common patterns in nonlinear
operations to enable efficient circuit sharing, thereby reducing hardware
resource requirements. QUARK targets all nonlinear operations within
Transformer-based models, achieving high-performance approximation through a
novel circuit-sharing design tailored to accelerate these operations. Our
evaluation demonstrates that QUARK significantly reduces the computational
overhead of nonlinear operators in mainstream Transformer architectures,
achieving up to a 1.96 times end-to-end speedup over GPU implementations.
Moreover, QUARK lowers the hardware overhead of nonlinear modules by more than
50% compared to prior approaches, all while maintaining high model accuracy --
and even substantially boosting accuracy under ultra-low-bit quantization.

</details>


### [146] [Data Trajectory Alignment for LLM Domain Adaptation: A Two-Phase Synthesis Framework for Telecommunications Mathematics](https://arxiv.org/abs/2511.06776)
*Zhicheng Zhou,Jing Li,Suming Qiu,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.LG

TL;DR: DTA是一个两阶段的数据整理框架，通过将解题过程作为监督信号，在电信数学领域实现了最先进的性能，同时显著提升了推理效率和能效。


<details>
  <summary>Details</summary>
Motivation: 解决通用大语言模型在电信等垂直领域部署时面临的数据稀缺、信息密度低以及移动/边缘设备资源受限的挑战。

Method: 两阶段框架：第一阶段使用教师模型集合生成多样化候选解；第二阶段重写教师解以对齐中间步骤和呈现风格，并通过一致性检查和反思式评判进行信号感知的示例选择。

Result: 在TELEMATH数据集上达到72.45%的pass@1准确率，比蒸馏训练提升17.65个百分点，比启用思考模式的Qwen3-32B基线提升2.94个百分点；推理效率提升42%能耗降低和60%延迟降低。

Conclusion: 对齐解题过程能够为紧凑模型提供高效监督，在低资源垂直领域实现准确性和效率的双重提升，为领域自适应提供了实用方案。

Abstract: General-purpose large language models (LLMs) are increasingly deployed in
verticals such as telecommunications, where adaptation is hindered by scarce,
low-information-density corpora and tight mobile/edge constraints. We propose
Data Trajectory Alignment (DTA), a two-phase, model-agnostic data curation
framework that treats solution processes - not only final answers - as
first-class supervision. Phase I (Initializing) synthesizes diverse,
high-coverage candidates using an ensemble of strong teachers. Phase II (DTA)
rewrites teacher solutions to align intermediate steps and presentation style
with the target student's inductive biases and then performs signal-aware
exemplar selection via agreement checks and reflection-based judging.
Instantiated on telecommunications mathematics (e.g., link budgets, SNR/AMC
selection, and power-control feasibility), DTA yields state-of-the-art (SOTA)
accuracy on TELEMATH without enabling explicit "thinking" modes: 72.45% pass@1,
surpassing distilled-only training by +17.65 points and outperforming a strong
baseline (Qwen3-32B with thinking enabled) by +2.94 points. Token-shift
analyses indicate that DTA concentrates gains on logical-structural discourse
markers rather than merely amplifying domain nouns, indicating improved
reasoning scaffolding. Under edge-like inference settings, DTA improves
efficiency by reducing reliance on multi-sample voting and disabling expensive
reasoning heuristics, cutting energy per output token by ~42% versus Qwen3-32B
(thinking mode enabled) and end-to-end latency by ~60% versus Qwen3-32B
(thinking mode disabled). These results demonstrate that aligning how solutions
are produced enables compact, high-yield supervision that is effective for both
accuracy and efficiency, offering a practical recipe for domain adaptation in
low-resource verticals beyond telecom.

</details>


### [147] [Optimistic Online-to-Batch Conversions for Accelerated Convergence and Universality](https://arxiv.org/abs/2511.06597)
*Yu-Hu Yan,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了新颖的乐观在线到批量转换方法，将乐观主义理论融入分析，显著简化了在线算法设计，同时保持了最优收敛速率。该方法在平滑目标函数上实现了最优加速收敛，并首次通过在线到批量转换视角在强凸平滑目标上达到最优加速收敛率。


<details>
  <summary>Details</summary>
Motivation: 研究离线凸优化中的平滑目标函数问题，经典Nesterov加速梯度方法已达到最优加速收敛。现有研究从在线学习和在线到批量转换角度理解NAG，强调乐观在线算法在加速中的作用。本文旨在从这一视角进一步贡献，通过理论分析将乐观主义融入转换过程。

Method: 提出新颖的乐观在线到批量转换方法，结合简单在线梯度下降实现最优加速收敛。该方法适用于强凸目标，通过结合乐观在线到批量转换和乐观在线算法达到最优加速收敛率。转换方法具有平滑性普适性，适用于平滑和非平滑目标。

Result: 提出的乐观转换方法：1）结合简单在线梯度下降实现最优加速收敛；2）在强凸平滑目标上首次通过在线到批量转换达到最优加速收敛率；3）具有平滑性普适性，无需平滑系数知识，每轮迭代仅需一次梯度查询。

Conclusion: 乐观在线到批量转换方法有效简化了在线算法设计，保持了最优收敛性能，并与Nesterov加速梯度方法建立了精确对应关系，为理解加速机制提供了新视角。

Abstract: In this work, we study offline convex optimization with smooth objectives,
where the classical Nesterov's Accelerated Gradient (NAG) method achieves the
optimal accelerated convergence. Extensive research has aimed to understand NAG
from various perspectives, and a recent line of work approaches this from the
viewpoint of online learning and online-to-batch conversion, emphasizing the
role of optimistic online algorithms for acceleration. In this work, we
contribute to this perspective by proposing novel optimistic online-to-batch
conversions that incorporate optimism theoretically into the analysis, thereby
significantly simplifying the online algorithm design while preserving the
optimal convergence rates. Specifically, we demonstrate the effectiveness of
our conversions through the following results: (i) when combined with simple
online gradient descent, our optimistic conversion achieves the optimal
accelerated convergence; (ii) our conversion also applies to strongly convex
objectives, and by leveraging both optimistic online-to-batch conversion and
optimistic online algorithms, we achieve the optimal accelerated convergence
rate for strongly convex and smooth objectives, for the first time through the
lens of online-to-batch conversion; (iii) our optimistic conversion can achieve
universality to smoothness -- applicable to both smooth and non-smooth
objectives without requiring knowledge of the smoothness coefficient -- and
remains efficient as non-universal methods by using only one gradient query in
each iteration. Finally, we highlight the effectiveness of our optimistic
online-to-batch conversions by a precise correspondence with NAG.

</details>


### [148] [On the Mechanisms of Collaborative Learning in VAE Recommenders](https://arxiv.org/abs/2511.06781)
*Tung-Long Vuong,Julien Monteil,Hien Dang,Volodymyr Vaskovych,Trung Le,Vu Nguyen*

Main category: cs.LG

TL;DR: 本文分析了VAE在推荐系统中的协作机制，提出了潜在共享半径概念，研究了局部与全局协作的平衡，并提出了锚点正则化器来稳定用户表示。


<details>
  <summary>Details</summary>
Motivation: 当前VAE推荐系统中二进制输入掩码技术虽然能提升性能，但其理论机制尚未充分探索，特别是协作如何产生以及局部与全局协作的平衡问题。

Method: 通过推导潜在共享半径分析协作机制，比较β-KL正则化和输入掩码两种全局混合机制，提出锚点正则化器来稳定用户表示并促进全局一致性。

Result: 在Netflix、MovieLens-20M和Million Song数据集上验证了分析结果，并在Amazon流媒体平台上成功部署了所提算法。

Conclusion: VAE推荐系统中的协作由潜在邻近性主导，输入掩码能促进全局协作但会引入邻域漂移，锚点正则化器能有效平衡用户身份保持与全局一致性。

Abstract: Variational Autoencoders (VAEs) are a powerful alternative to matrix
factorization for recommendation. A common technique in VAE-based collaborative
filtering (CF) consists in applying binary input masking to user interaction
vectors, which improves performance but remains underexplored theoretically. In
this work, we analyze how collaboration arises in VAE-based CF and show it is
governed by latent proximity: we derive a latent sharing radius that informs
when an SGD update on one user strictly reduces the loss on another user, with
influence decaying as the latent Wasserstein distance increases. We further
study the induced geometry: with clean inputs, VAE-based CF primarily exploits
\emph{local} collaboration between input-similar users and under-utilizes
global collaboration between far-but-related users. We compare two mechanisms
that encourage \emph{global} mixing and characterize their trade-offs: (1)
$\beta$-KL regularization directly tightens the information bottleneck,
promoting posterior overlap but risking representational collapse if too large;
(2) input masking induces stochastic geometric contractions and expansions,
which can bring distant users onto the same latent neighborhood but also
introduce neighborhood drift. To preserve user identity while enabling global
consistency, we propose an anchor regularizer that aligns user posteriors with
item embeddings, stabilizing users under masking and facilitating signal
sharing across related items. Our analyses are validated on the Netflix,
MovieLens-20M, and Million Song datasets. We also successfully deployed our
proposed algorithm on an Amazon streaming platform following a successful
online experiment.

</details>


### [149] [Resource Efficient Sleep Staging via Multi-Level Masking and Prompt Learning](https://arxiv.org/abs/2511.06785)
*Lejun Ai,Yulong Li,Haodong Yi,Jixuan Xie,Yue Wang,Jia Liu,Min Chen,Rui Wang*

Main category: cs.LG

TL;DR: 提出了资源高效的睡眠分期任务，通过掩码感知睡眠分期（MASS）框架，在减少信号采集量的同时保持可靠的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有睡眠分期方法严重依赖长时连续EEG记录，这在资源受限系统（如可穿戴设备）中面临数据采集挑战。

Method: 采用掩码和提示学习策略，设计多级掩码策略促进部分和随机观测下的特征建模，并提出分层提示学习机制聚合未掩码数据作为语义锚点。

Result: 在四个数据集上评估显示最先进的性能，特别是在数据量非常有限时表现优异。

Conclusion: MASS在现实世界低资源睡眠监测环境中具有高效和可扩展部署的潜力。

Abstract: Automatic sleep staging plays a vital role in assessing sleep quality and
diagnosing sleep disorders. Most existing methods rely heavily on long and
continuous EEG recordings, which poses significant challenges for data
acquisition in resource-constrained systems, such as wearable or home-based
monitoring systems. In this paper, we propose the task of resource-efficient
sleep staging, which aims to reduce the amount of signal collected per sleep
epoch while maintaining reliable classification performance. To solve this
task, we adopt the masking and prompt learning strategy and propose a novel
framework called Mask-Aware Sleep Staging (MASS). Specifically, we design a
multi-level masking strategy to promote effective feature modeling under
partial and irregular observations. To mitigate the loss of contextual
information introduced by masking, we further propose a hierarchical prompt
learning mechanism that aggregates unmasked data into a global prompt, serving
as a semantic anchor for guiding both patch-level and epoch-level feature
modeling. MASS is evaluated on four datasets, demonstrating state-of-the-art
performance, especially when the amount of data is very limited. This result
highlights its potential for efficient and scalable deployment in real-world
low-resource sleep monitoring environments.

</details>


### [150] [Explainable Probabilistic Machine Learning for Predicting Drilling Fluid Loss of Circulation in Marun Oil Field](https://arxiv.org/abs/2511.06607)
*Seshu Kumar Damarla,Xiuli Zhu*

Main category: cs.LG

TL;DR: 本研究提出了基于高斯过程回归的概率机器学习框架，用于预测复杂地层中的钻井液漏失，通过量化预测不确定性提高决策可靠性，并使用LIME方法增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 井漏仍然是钻井作业中的主要且成本高昂的挑战，常导致井筒失稳、卡钻和延长非生产时间。准确预测流体损失对于提高钻井安全性和效率至关重要。

Method: 采用高斯过程回归模型捕获钻井参数间的非线性依赖关系并量化预测不确定性，使用LBFGS算法优化模型超参数以确保数值稳定性和鲁棒泛化能力，应用LIME方法提高模型可解释性。

Result: 研究结果表明，该概率机器学习框架能够有效预测钻井液漏失，为高风险决策提供增强的可靠性，并能够阐明单个特征对模型预测的影响。

Conclusion: 可解释的概率学习方法具有主动识别井漏风险、优化堵漏材料设计和减少钻井作业中操作不确定性的潜力。

Abstract: Lost circulation remains a major and costly challenge in drilling operations,
often resulting in wellbore instability, stuck pipe, and extended
non-productive time. Accurate prediction of fluid loss is therefore essential
for improving drilling safety and efficiency. This study presents a
probabilistic machine learning framework based on Gaussian Process Regression
(GPR) for predicting drilling fluid loss in complex formations. The GPR model
captures nonlinear dependencies among drilling parameters while quantifying
predictive uncertainty, offering enhanced reliability for high-risk
decision-making. Model hyperparameters are optimized using the Limited memory
Broyden Fletcher Goldfarb Shanno (LBFGS) algorithm to ensure numerical
stability and robust generalization. To improve interpretability, Local
Interpretable Model agnostic Explanations (LIME) are employed to elucidate how
individual features influence model predictions. The results highlight the
potential of explainable probabilistic learning for proactive identification of
lost-circulation risks, optimized design of lost circulation materials (LCM),
and reduction of operational uncertainties in drilling applications.

</details>


### [151] [Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models](https://arxiv.org/abs/2511.06793)
*Kunhao Li,Wenhao Li,Di Wu,Lei Yang,Jun Bai,Ju Jia,Jason Xue*

Main category: cs.LG

TL;DR: 本文提出了一种多模态影响神经元路径编辑器（MIP-Editor），用于解决多模态大语言模型在机器遗忘任务中面临的跨模态遗忘不一致和通用知识性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在现实应用中存在隐私泄露、毒性缓解和知识产权侵权等风险，而现有的基于神经元编辑的机器遗忘方法在处理多模态数据时面临遗忘不一致和通用性能下降的挑战。

Method: 提出MIP-Editor方法，引入模态特定的归因分数来识别编码遗忘集知识的影响神经元路径，并通过表示误导进行影响路径感知的神经元编辑。

Result: 实验结果显示，MIP-Editor在多模态任务上实现了87.75%的最大遗忘率和54.26%的通用知识保留改进，在文本任务上实现了80.65%的遗忘率和77.9%的通用性能保留。

Conclusion: MIP-Editor通过影响神经元路径编辑实现了多模态模型的有效协调遗忘，同时保持了模型的通用能力，为解决多模态机器遗忘问题提供了实用解决方案。

Abstract: Multimodal Large Language Models (MLLMs) extend foundation models to
real-world applications by integrating inputs such as text and vision. However,
their broad knowledge capacity raises growing concerns about privacy leakage,
toxicity mitigation, and intellectual property violations. Machine Unlearning
(MU) offers a practical solution by selectively forgetting targeted knowledge
while preserving overall model utility. When applied to MLLMs, existing
neuron-editing-based MU approaches face two fundamental challenges: (1)
forgetting becomes inconsistent across modalities because existing point-wise
attribution methods fail to capture the structured, layer-by-layer information
flow that connects different modalities; and (2) general knowledge performance
declines when sensitive neurons that also support important reasoning paths are
pruned, as this disrupts the model's ability to generalize. To alleviate these
limitations, we propose a multimodal influential neuron path editor
(MIP-Editor) for MU. Our approach introduces modality-specific attribution
scores to identify influential neuron paths responsible for encoding forget-set
knowledge and applies influential-path-aware neuron-editing via representation
misdirection. This strategy also enables effective and coordinated forgetting
across modalities while preserving the model's general capabilities.
Experimental results demonstrate that MIP-Editor achieves a superior unlearning
performance on multimodal tasks, with a maximum forgetting rate of 87.75% and
up to 54.26% improvement in general knowledge retention. On textual tasks,
MIP-Editor achieves up to 80.65% forgetting and preserves 77.9% of general
performance. Codes are available at https://github.com/PreckLi/MIP-Editor.

</details>


### [152] [A Weak Penalty Neural ODE for Learning Chaotic Dynamics from Noisy Time Series](https://arxiv.org/abs/2511.06609)
*Xuyang Li,John Harlim,Romit Maulik*

Main category: cs.LG

TL;DR: 本文提出弱惩罚神经常微分方程（WP-NODE）方法，通过结合弱形式和强形式训练策略，显著提升了噪声环境下混沌动力系统的预测精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的高维动力系统观测数据往往受到噪声污染，这会严重降低数据驱动模型的性能。特别是在混沌动力系统中，小误差会迅速放大，难以在保持长期不变性的同时实现短期准确预测。

Method: 提出弱惩罚神经常微分方程（WP-NODE）方法，将弱形式作为惩罚项与经典的强形式训练相结合。弱形式通过时间子域上的积分残差约束模型，而强形式基于NODE的离散化优化。

Result: 数值实验表明，WP-NODE在基准混沌动力系统上实现了最先进的预测精度和卓越的鲁棒性。

Conclusion: 弱形式作为强形式训练的补充方法，能够有效提升神经常微分方程在噪声环境下的性能，为复杂动力系统的数据驱动建模提供了新思路。

Abstract: Accurate forecasting of complex high-dimensional dynamical systems from
observational data is essential for several applications across science and
engineering. A key challenge, however, is that real-world measurements are
often corrupted by noise, which severely degrades the performance of
data-driven models. Particularly, in chaotic dynamical systems, where small
errors amplify rapidly, it is challenging to identify a data-driven model from
noisy data that achieves short-term accuracy while preserving long-term
invariant properties. In this paper, we propose the use of the weak formulation
as a complementary approach to the classical strong formulation of data-driven
time-series forecasting models. Specifically, we focus on the neural ordinary
differential equation (NODE) architecture. Unlike the standard strong
formulation, which relies on the discretization of the NODE followed by
optimization, the weak formulation constrains the model using a set of
integrated residuals over temporal subdomains. While such a formulation yields
an effective NODE model, we discover that the performance of a NODE can be
further enhanced by employing this weak formulation as a penalty alongside the
classical strong formulation-based learning. Through numerical demonstrations,
we illustrate that our proposed training strategy, which we coined as the
Weak-Penalty NODE (WP-NODE), achieves state-of-the-art forecasting accuracy and
exceptional robustness across benchmark chaotic dynamical systems.

</details>


### [153] [Non-Rival Data as Rival Products: An Encapsulation-Forging Approach for Data Synthesis](https://arxiv.org/abs/2511.06610)
*Kaidong Wang,Jiale Li,Shao-Bo Lin,Yao Wang*

Main category: cs.LG

TL;DR: EnFo框架通过封装关键模型和锻造合成数据，将非竞争性数据转化为具有不对称效用的竞争性产品，保护数据所有者的竞争优势。


<details>
  <summary>Details</summary>
Motivation: 解决企业在数据共享时面临的困境：共享数据能创造价值但会削弱竞争优势，现有数据合成方法因产生对称效用数据而加剧这一问题。

Method: 两阶段框架：1) 从原始数据中封装预测知识到指定的"关键"模型；2) 通过优化数据故意使关键模型过拟合来锻造合成数据集。

Result: 框架表现出显著的样本效率，仅用原始数据的一小部分就能匹配其性能，同时提供强大的隐私保护和抗滥用能力。

Conclusion: EnFo为企业提供了在不损害核心分析优势的情况下进行战略合作的实用解决方案。

Abstract: The non-rival nature of data creates a dilemma for firms: sharing data
unlocks value but risks eroding competitive advantage. Existing data synthesis
methods often exacerbate this problem by creating data with symmetric utility,
allowing any party to extract its value. This paper introduces the
Encapsulation-Forging (EnFo) framework, a novel approach to generate rival
synthetic data with asymmetric utility. EnFo operates in two stages: it first
encapsulates predictive knowledge from the original data into a designated
``key'' model, and then forges a synthetic dataset by optimizing the data to
intentionally overfit this key model. This process transforms non-rival data
into a rival product, ensuring its value is accessible only to the intended
model, thereby preventing unauthorized use and preserving the data owner's
competitive edge. Our framework demonstrates remarkable sample efficiency,
matching the original data's performance with a fraction of its size, while
providing robust privacy protection and resistance to misuse. EnFo offers a
practical solution for firms to collaborate strategically without compromising
their core analytical advantage.

</details>


### [154] [Controllable Flow Matching for Online Reinforcement Learning](https://arxiv.org/abs/2511.06816)
*Bin Wang,Boxiang Tao,Haifeng Jing,Hongbo Dou,Zijian Wang*

Main category: cs.LG

TL;DR: CtrlFlow是一种基于条件流匹配的轨迹级合成方法，直接建模从初始状态到高回报终端状态的轨迹分布，无需显式建模环境转移函数，解决了传统基于模型的强化学习中模型误差累积的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的强化学习方法依赖环境动态建模以提高数据效率，但由于模型误差在长时域推演中的累积，这些方法在保持建模稳定性方面面临挑战。

Method: 提出CtrlFlow方法，使用条件流匹配直接建模轨迹分布，通过最小化由非线性可控性Gramian矩阵控制的控制能量来确保最优轨迹采样。

Result: 在在线设置中，CtrlFlow在常见MuJoCo基准任务上表现出比动态模型更好的性能，并且相比标准基于模型的强化学习方法实现了更优的样本效率。

Conclusion: CtrlFlow方法通过直接建模轨迹分布，避免了传统动态建模中的误差累积问题，生成多样化的轨迹数据显著增强了策略学习的鲁棒性和跨任务泛化能力。

Abstract: Model-based reinforcement learning (MBRL) typically relies on modeling
environment dynamics for data efficiency. However, due to the accumulation of
model errors over long-horizon rollouts, such methods often face challenges in
maintaining modeling stability. To address this, we propose CtrlFlow, a
trajectory-level synthetic method using conditional flow matching (CFM), which
directly modeling the distribution of trajectories from initial states to
high-return terminal states without explicitly modeling the environment
transition function. Our method ensures optimal trajectory sampling by
minimizing the control energy governed by the non-linear Controllability
Gramian Matrix, while the generated diverse trajectory data significantly
enhances the robustness and cross-task generalization of policy learning. In
online settings, CtrlFlow demonstrates the better performance on common MuJoCo
benchmark tasks than dynamics models and achieves superior sample efficiency
compared to standard MBRL methods.

</details>


### [155] [TuckA: Hierarchical Compact Tensor Experts for Efficient Fine-Tuning](https://arxiv.org/abs/2511.06859)
*Qifeng Lei,Zhiyong Yang,Qianqian Xu,Cong Hua,Peisong Wen,Qingming Huang*

Main category: cs.LG

TL;DR: 提出TuckA方法，通过Tucker分解创建紧凑的3D张量专家系统，结合分层策略和高效路由机制，实现参数高效的微调，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统参数高效微调方法使用单一专家难以处理复杂任务中数据的多样性问题，需要更有效的多专家集成方案。

Method: 使用Tucker分解构建3D张量专家系统，采用分层组织策略，开发批量级路由机制和数据感知初始化方法。

Result: 在自然语言理解、图像分类和数学推理等基准测试中表现出色，提供了PEFT问题的新解决方案。

Conclusion: TuckA方法通过紧凑的多专家结构有效解决了复杂任务中的数据多样性挑战，为参数高效微调提供了创新方案。

Abstract: Efficiently fine-tuning pre-trained models for downstream tasks is a key
challenge in the era of foundation models. Parameter-efficient fine-tuning
(PEFT) presents a promising solution, achieving performance comparable to full
fine-tuning by updating only a small number of adaptation weights per layer.
Traditional PEFT methods typically rely on a single expert, where the
adaptation weight is a low-rank matrix. However, for complex tasks, the data's
inherent diversity poses a significant challenge for such models, as a single
adaptation weight cannot adequately capture the features of all samples. To
address this limitation, we explore how to integrate multiple small adaptation
experts into a compact structure to defeat a large adapter. Specifically, we
propose Tucker Adaptation (TuckA), a method with four key properties: (i) We
use Tucker decomposition to create a compact 3D tensor where each slice
naturally serves as an expert. The low-rank nature of this decomposition
ensures that the number of parameters scales efficiently as more experts are
added. (ii) We introduce a hierarchical strategy that organizes these experts
into groups at different granularities, allowing the model to capture both
local and global data patterns. (iii) We develop an efficient batch-level
routing mechanism, which reduces the router's parameter size by a factor of $L$
compared to routing at every adapted layer (where $L$ is the number of adapted
layers) (iv) We propose data-aware initialization to achieve loss-free expert
load balancing based on theoretical analysis. Extensive experiments on
benchmarks in natural language understanding, image classification, and
mathematical reasoning speak to the efficacy of TuckA, offering a new and
effective solution to the PEFT problem.

</details>


### [156] [Neyman-Pearson Classification under Both Null and Alternative Distributions Shift](https://arxiv.org/abs/2511.06641)
*Mohammadreza M. Kalan,Yuyang Deng,Eitan J. Neugut,Samory Kpotufe*

Main category: cs.LG

TL;DR: 本文研究了Neyman-Pearson分类中的迁移学习问题，提出了一种自适应方法，在源数据有信息时能改善Type-I和Type-II错误，在源数据无信息时能避免负迁移。


<details>
  <summary>Details</summary>
Motivation: 传统分类中的迁移学习已被广泛研究，但在不平衡分类如Neyman-Pearson分类中的迁移学习关注较少。该设置面临独特挑战，需要同时控制两种类型的错误。现有工作仅处理μ1分布偏移的情况，而实际场景中μ0和μ1都可能发生偏移。

Method: 推导了一种自适应程序，不仅在有信息源时保证改进Type-I和Type-II错误，还能在源无信息时自动适应，避免负迁移。

Result: 该方法具有统计保证，能有效控制两种错误类型，同时具有计算效率保证。

Conclusion: 提出的自适应方法在Neyman-Pearson分类迁移学习中能有效处理双分布偏移问题，避免负迁移，并保证统计和计算效率。

Abstract: We consider the problem of transfer learning in Neyman-Pearson
classification, where the objective is to minimize the error w.r.t. a
distribution $\mu_1$, subject to the constraint that the error w.r.t. a
distribution $\mu_0$ remains below a prescribed threshold. While transfer
learning has been extensively studied in traditional classification, transfer
learning in imbalanced classification such as Neyman-Pearson classification has
received much less attention. This setting poses unique challenges, as both
types of errors must be simultaneously controlled. Existing works address only
the case of distribution shift in $\mu_1$, whereas in many practical scenarios
shifts may occur in both $\mu_0$ and $\mu_1$. We derive an adaptive procedure
that not only guarantees improved Type-I and Type-II errors when the source is
informative, but also automatically adapt to situations where the source is
uninformative, thereby avoiding negative transfer. In addition to such
statistical guarantees, the procedures is efficient, as shown via complementary
computational guarantees.

</details>


### [157] [DeepBooTS: Dual-Stream Residual Boosting for Drift-Resilient Time-Series Forecasting](https://arxiv.org/abs/2511.06893)
*Daojun Liang,Jing Chen,Xiao Wang,Yinglong Wang,Suo Li*

Main category: cs.LG

TL;DR: DeepBooTS是一种新颖的双流残差递减增强方法，通过逐块重构内在信号来提升时间序列预测的鲁棒性，在概念漂移场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列存在显著的非平稳性，大多数预测方法对概念漂移的鲁棒性较差，尽管普遍应用了实例归一化。

Method: 通过偏差-方差分析概念漂移，提出DeepBooTS：端到端双流残差递减增强方法，每个块成为学习器集合，具有辅助输出分支形成最终预测的高速通道，逐块输出修正先前块的残差。

Result: 在大规模数据集上的广泛实验表明，该方法大幅超越现有方法，在各种数据集上平均性能提升15.8%。

Conclusion: DeepBooTS建立了时间序列预测的新基准，显著提高了对概念漂移的鲁棒性，同时增强了多功能性和可解释性。

Abstract: Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most
forecasting methods display compromised robustness to concept drift, despite
the prevalent application of instance normalization. We tackle this challenge
by first analysing concept drift through a bias-variance lens and proving that
weighted ensemble reduces variance without increasing bias. These insights
motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting
method that progressively reconstructs the intrinsic signal. In our design,
each block of a deep model becomes an ensemble of learners with an auxiliary
output branch forming a highway to the final prediction. The block-wise outputs
correct the residuals of previous blocks, leading to a learning-driven
decomposition of both inputs and targets. This method enhances versatility and
interpretability while substantially improving robustness to concept drift.
Extensive experiments, including those on large-scale datasets, show that the
proposed method outperforms existing methods by a large margin, yielding an
average performance improvement of 15.8% across various datasets, establishing
a new benchmark for TS forecasting.

</details>


### [158] [Improving Asset Allocation in a Fast Moving Consumer Goods B2B Company: An Interpretable Machine Learning Framework for Commercial Cooler Assignment Based on Multi-Tier Growth Targets](https://arxiv.org/abs/2511.06642)
*Renato Castro,Rodrigo Paredes,Douglas Kahn*

Main category: cs.LG

TL;DR: 本文提出了一个机器学习框架，用于预测哪些饮料客户在获得冷藏设备后最有可能实现销量增长，从而优化资产分配决策。


<details>
  <summary>Details</summary>
Motivation: 在快消品行业，资产分配决策直接影响收入增长和执行效率，但使用机器学习指导资产分配的研究相对较少。

Method: 使用XGBoost、LightGBM和CatBoost等机器学习模型，结合SHAP进行可解释特征分析，基于3,119个B2B传统贸易渠道客户的数据集，跟踪冷藏设备安装前后12个月的销售交易。

Result: 最佳模型在验证集上针对10%、30%和50%增长阈值的AUC得分分别为0.857、0.877和0.898。模拟显示该方法相比传统基于销量的方法能提高投资回报率。

Conclusion: 该框架能够更准确地选择有增长潜力的客户，避免向不会增长的客户分配资产，从而提高成本节约和投资回报率。

Abstract: In the fast-moving consumer goods (FMCG) industry, deciding where to place
physical assets, such as commercial beverage coolers, can directly impact
revenue growth and execution efficiency. Although churn prediction and demand
forecasting have been widely studied in B2B contexts, the use of machine
learning to guide asset allocation remains relatively unexplored. This paper
presents a framework focused on predicting which beverage clients are most
likely to deliver strong returns in volume after receiving a cooler. Using a
private dataset from a well-known Central American brewing and beverage company
of 3,119 B2B traditional trade channel clients that received a cooler from
2022-01 to 2024-07, and tracking 12 months of sales transactions before and
after cooler installation, three growth thresholds were defined: 10%, 30% and
50% growth in sales volume year over year. The analysis compares results of
machine learning models such as XGBoost, LightGBM, and CatBoost combined with
SHAP for interpretable feature analysis in order to have insights into
improving business operations related to cooler allocation; the results show
that the best model has AUC scores of 0.857, 0.877, and 0.898 across the
thresholds on the validation set. Simulations suggest that this approach can
improve ROI because it better selects potential clients to grow at the expected
level and increases cost savings by not assigning clients that will not grow,
compared to traditional volume-based approaches with substantial business
management recommendations

</details>


### [159] [COGNOS: Universal Enhancement for Time Series Anomaly Detection via Constrained Gaussian-Noise Optimization and Smoothing](https://arxiv.org/abs/2511.06894)
*Wenlong Shang,Peng Chang*

Main category: cs.LG

TL;DR: COGNOS是一个通用的时间序列异常检测增强框架，通过高斯白噪声正则化和卡尔曼平滑后处理，显著提升了基于重构方法的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于重构的时间序列异常检测方法普遍依赖均方误差损失，导致重构残差存在统计缺陷，产生噪声大、不稳定的异常分数，影响检测可靠性。

Method: COGNOS包含两个核心组件：1）高斯白噪声正则化策略，在训练时约束模型输出残差符合高斯白噪声分布；2）卡尔曼平滑后处理器，作为统计最优估计器对原始异常分数进行去噪。

Result: 在多个真实世界基准数据集上，COGNOS应用于12种不同的骨干模型，平均F1分数提升了57.9%。

Conclusion: 直接正则化输出统计特性是显著改进异常检测系统的强大且可泛化的策略。

Abstract: Reconstruction-based methods are a dominant paradigm in time series anomaly
detection (TSAD), however, their near-universal reliance on Mean Squared Error
(MSE) loss results in statistically flawed reconstruction residuals. This
fundamental weakness leads to noisy, unstable anomaly scores with a poor
signal-to-noise ratio, hindering reliable detection. To address this, we
propose Constrained Gaussian-Noise Optimization and Smoothing (COGNOS), a
universal, model-agnostic enhancement framework that tackles this issue at its
source. COGNOS introduces a novel Gaussian-White Noise Regularization strategy
during training, which directly constrains the model's output residuals to
conform to a Gaussian white noise distribution. This engineered statistical
property creates the ideal precondition for our second contribution: a Kalman
Smoothing Post-processor that provably operates as a statistically optimal
estimator to denoise the raw anomaly scores. The synergy between these two
components allows COGNOS to robustly separate the true anomaly signal from
random fluctuations. Extensive experiments demonstrate that COGNOS is highly
effective, delivering an average F-score uplift of 57.9% when applied to 12
diverse backbone models across multiple real-world benchmark datasets. Our work
reveals that directly regularizing output statistics is a powerful and
generalizable strategy for significantly improving anomaly detection systems.

</details>


### [160] [Dual-Pathway Fusion of EHRs and Knowledge Graphs for Predicting Unseen Drug-Drug Interactions](https://arxiv.org/abs/2511.06662)
*Franklin Lee,Tengfei Ma*

Main category: cs.LG

TL;DR: 本文提出了首个结合知识图谱和电子健康记录的系统，通过教师-学生模型实现DDI的零样本推理，能够在未见药物上产生可解释的机制特异性警报。


<details>
  <summary>Details</summary>
Motivation: 现有DDI检测模型要么依赖知识图谱（无法处理未见药物），要么依赖电子健康记录（噪声大、时间依赖性强且站点依赖），需要一种能够结合两者优势并实现零样本推理的方法。

Method: 使用融合教师模型学习知识图谱和EHR中的药物关系，然后蒸馏为仅使用EHR的学生模型进行零样本推理，共享药理学机制本体以产生可解释的警报。

Result: 在多机构EHR数据和DrugBank DDI图谱上训练，系统在保持精度的同时减少误报，在可比检测性能下漏报更少真实相互作用，并能零样本识别KG中缺失药物的临床认可机制。

Conclusion: 该系统支持临床决策支持和药物警戒的实际应用，能够为未见药物产生机制特异性、临床一致的预测，优于现有方法。

Abstract: Drug-drug interactions (DDIs) remain a major source of preventable harm, and
many clinically important mechanisms are still unknown. Existing models either
rely on pharmacologic knowledge graphs (KGs), which fail on unseen drugs, or on
electronic health records (EHRs), which are noisy, temporal, and
site-dependent. We introduce, to our knowledge, the first system that
conditions KG relation scoring on patient-level EHR context and distills that
reasoning into an EHR-only model for zero-shot inference. A fusion "Teacher"
learns mechanism-specific relations for drug pairs represented in both sources,
while a distilled "Student" generalizes to new or rarely used drugs without KG
access at inference. Both operate under a shared ontology (set) of
pharmacologic mechanisms (drug relations) to produce interpretable, auditable
alerts rather than opaque risk scores. Trained on a multi-institution EHR
corpus paired with a curated DrugBank DDI graph, and evaluated using a
clinically aligned, decision-focused protocol with leakage-safe negatives that
avoid artificially easy pairs, the system maintains precision across
multi-institutuion test data, produces mechanism-specific, clinically
consistent predictions, reduces false alerts (higher precision) at comparable
overall detection performance (F1), and misses fewer true interactions compared
to prior methods. Case studies further show zero-shot identification of
clinically recognized CYP-mediated and pharmacodynamic mechanisms for drugs
absent from the KG, supporting real-world use in clinical decision support and
pharmacovigilance.

</details>


### [161] [On The Presence of Double-Descent in Deep Reinforcement Learning](https://arxiv.org/abs/2511.06895)
*Viktor Veselý,Aleksandar Todorov,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 本文发现深度强化学习中存在双下降现象，过度参数化的模型在超过插值点后泛化能力反而提升，并通过策略熵度量发现这与策略不确定性的持续显著降低相关。


<details>
  <summary>Details</summary>
Motivation: 双下降现象在非平稳的深度强化学习领域尚未得到充分探索，本文旨在系统研究模型容量变化下的双下降现象及其与策略不确定性的关系。

Method: 使用Actor-Critic框架，通过信息论指标策略熵来测量训练过程中的策略不确定性，系统研究不同模型容量下的双下降现象。

Result: 初步结果显示存在明显的逐轮次双下降曲线，策略进入第二下降区域与策略熵的持续显著减少相关，表明过度参数化起到了隐式正则化作用。

Conclusion: 这些发现确立了双下降现象在深度强化学习中的存在，并为设计更通用、可迁移和鲁棒的智能体提供了基于信息的机制。

Abstract: The double descent (DD) paradox, where over-parameterized models see
generalization improve past the interpolation point, remains largely unexplored
in the non-stationary domain of Deep Reinforcement Learning (DRL). We present
preliminary evidence that DD exists in model-free DRL, investigating it
systematically across varying model capacity using the Actor-Critic framework.
We rely on an information-theoretic metric, Policy Entropy, to measure policy
uncertainty throughout training. Preliminary results show a clear epoch-wise DD
curve; the policy's entrance into the second descent region correlates with a
sustained, significant reduction in Policy Entropy. This entropic decay
suggests that over-parameterization acts as an implicit regularizer, guiding
the policy towards robust, flatter minima in the loss landscape. These findings
establish DD as a factor in DRL and provide an information-based mechanism for
designing agents that are more general, transferable, and robust.

</details>


### [162] [An Adaptive Machine Learning Triage Framework for Predicting Alzheimer's Disease Progression](https://arxiv.org/abs/2511.06681)
*Richard Hou,Shengpu Tang,Wei Jin*

Main category: cs.LG

TL;DR: 开发了一个两阶段机器学习框架，通过选择性获取昂贵的高级特征来预测轻度认知障碍向阿尔茨海默病的转化，在减少20%高级检测需求的同时保持了高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决认知测试和临床数据预测能力不足，而PET扫描和CSF生物标志物分析过于昂贵的成本-准确性困境，使早期可靠的阿尔茨海默病预测在现实世界中更易获得。

Method: 设计两阶段机器学习框架，基于预测的"信息价值"选择性获取昂贵的高级特征，应用于ADNI数据预测MCI向AD的进展。

Result: 框架将高级检测需求减少20%，测试AUROC达到0.929，与使用所有特征的模型相当（AUROC=0.915，p=0.1010），并提供可解释性分析解释分诊决策。

Conclusion: 该框架优化了AD诊断路径，平衡了准确性与成本，是向现实世界实践推广早期可靠AD预测的重要一步，未来应考虑更多类别的高级特征和大规模验证。

Abstract: Accurate predictions of conversion from mild cognitive impairment (MCI) to
Alzheimer's disease (AD) can enable effective personalized therapy. While
cognitive tests and clinical data are routinely collected, they lack the
predictive power of PET scans and CSF biomarker analysis, which are
prohibitively expensive to obtain for every patient. To address this
cost-accuracy dilemma, we design a two-stage machine learning framework that
selectively obtains advanced, costly features based on their predicted "value
of information". We apply our framework to predict AD progression for MCI
patients using data from the Alzheimer's Disease Neuroimaging Initiative
(ADNI). Our framework reduces the need for advanced testing by 20% while
achieving a test AUROC of 0.929, comparable to the model that uses both basic
and advanced features (AUROC=0.915, p=0.1010). We also provide an example
interpretability analysis showing how one may explain the triage decision. Our
work presents an interpretable, data-driven framework that optimizes AD
diagnostic pathways and balances accuracy with cost, representing a step
towards making early, reliable AD prediction more accessible in real-world
practice. Future work should consider multiple categories of advanced features
and larger-scale validation.

</details>


### [163] [Mitigating Modality Imbalance in Multi-modal Learning via Multi-objective Optimization](https://arxiv.org/abs/2511.06686)
*Heshan Fernando,Parikshit Ram,Yi Zhou,Soham Dan,Horst Samulowitz,Nathalie Baracaldo,Tianyi Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于多目标优化的多模态学习方法，通过将多模态学习重新表述为多目标优化问题，解决了模态间学习不平衡的问题，并在保持性能的同时大幅减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态学习方法存在模态间学习不平衡的问题，导致性能甚至不如单模态方法，且现有平衡方法通常需要计算密集的子程序。

Method: 将多模态学习问题重新表述为多目标优化问题，并提出梯度优化算法来解决修改后的多模态学习问题，提供了收敛性保证。

Result: 在流行的多模态学习基准测试中，该方法相比现有的平衡多模态学习和多目标优化基线方法表现出改进的性能，子程序计算时间减少了约20倍。

Conclusion: 该方法通过多目标优化框架有效解决了多模态学习中的不平衡问题，在提升性能的同时显著降低了计算成本。

Abstract: Multi-modal learning (MML) aims to integrate information from multiple
modalities, which is expected to lead to superior performance over
single-modality learning. However, recent studies have shown that MML can
underperform, even compared to single-modality approaches, due to imbalanced
learning across modalities. Methods have been proposed to alleviate this
imbalance issue using different heuristics, which often lead to computationally
intensive subroutines. In this paper, we reformulate the MML problem as a
multi-objective optimization (MOO) problem that overcomes the imbalanced
learning issue among modalities and propose a gradient-based algorithm to solve
the modified MML problem. We provide convergence guarantees for the proposed
method, and empirical evaluations on popular MML benchmarks showcasing the
improved performance of the proposed method over existing balanced MML and MOO
baselines, with up to ~20x reduction in subroutine computation time. Our code
is available at https://github.com/heshandevaka/MIMO.

</details>


### [164] [Counterfactual Explanation for Multivariate Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2511.06906)
*Keita Kinjo*

Main category: cs.LG

TL;DR: 该研究提出了一种使用外生变量为时间序列预测生成反事实解释的方法，解决了机器学习模型可解释性问题，并通过理论和实验验证了方法的准确性和实用性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在时间序列数据分析中广泛使用，但许多模型是黑盒，缺乏可解释性。反事实解释是解决这一问题的有效方法，但在时间序列预测领域的研究相对较少。

Method: 提出使用外生变量提取时间序列预测的反事实解释，包括分析各变量对整个时间序列的影响、通过改变特定变量生成反事实解释，以及评估生成的反事实解释质量。

Result: 通过理论分析和实证实验验证了所提方法的准确性，展示了其在实际应用中的可行性。

Conclusion: 该方法能够支持基于时间序列数据分析的实际决策制定，为商业和营销等领域的应用提供了有价值的工具。

Abstract: Currently, machine learning is widely used across various domains, including
time series data analysis. However, some machine learning models function as
black boxes, making interpretability a critical concern. One approach to
address this issue is counterfactual explanation (CE), which aims to provide
insights into model predictions. This study focuses on the relatively
underexplored problem of generating counterfactual explanations for time series
forecasting. We propose a method for extracting CEs in time series forecasting
using exogenous variables, which are frequently encountered in fields such as
business and marketing. In addition, we present methods for analyzing the
influence of each variable over an entire time series, generating CEs by
altering only specific variables, and evaluating the quality of the resulting
CEs. We validate the proposed method through theoretical analysis and empirical
experiments, showcasing its accuracy and practical applicability. These
contributions are expected to support real-world decision-making based on time
series data analysis.

</details>


### [165] [Peeling Context from Cause for Multimodal Molecular Property Prediction](https://arxiv.org/abs/2511.06692)
*Tao Li,Kaiyuan Hou,Tuan Vinh,Carl Yang,Monika Raj*

Main category: cs.LG

TL;DR: CLaP是一个用于分子性质预测的因果层剥离框架，通过逐层分离因果信号与上下文，整合多种分子图表示，提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在分子性质预测中难以解释，可能依赖虚假上下文而非因果结构，导致分布偏移下的可靠性降低和预测性能下降。

Method: 在每一层，因果块执行软分割到因果和非因果分支，融合跨模态的因果证据，逐步移除批次耦合的上下文以关注标签相关结构，限制捷径信号并稳定层间细化。

Result: 在四个分子基准测试中，CLaP在MAE、MSE和R²指标上持续优于竞争基线，并生成原子级因果显著性图谱，突出显示负责预测的子结构。

Conclusion: 通过逐层剥离上下文与因果关系，该模型产生了既准确又可解释的分子设计预测器，案例研究证实了图谱的准确性及其与化学直觉的一致性。

Abstract: Deep models are used for molecular property prediction, yet they are often
difficult to interpret and may rely on spurious context rather than causal
structure, which reduces reliability under distribution shift and harms
predictive performance. We introduce CLaP (Causal Layerwise Peeling), a
framework that separates causal signal from context in a layerwise manner and
integrates diverse graph representations of molecules. At each layer, a causal
block performs a soft split into causal and non-causal branches, fuses causal
evidence across modalities, and progressively removes batch-coupled context to
focus on label-relevant structure, thereby limiting shortcut signals and
stabilizing layerwise refinement. Across four molecular benchmarks, CLaP
consistently improves MAE, MSE, and $R^2$ over competitive baselines. The model
also produces atom-level causal saliency maps that highlight substructures
responsible for predictions, providing actionable guidance for targeted
molecular edits. Case studies confirm the accuracy of these maps and their
alignment with chemical intuition. By peeling context from cause at every
layer, the model yields predictors that are both accurate and interpretable for
molecular design.

</details>


### [166] [Learning to Focus: Prioritizing Informative Histories with Structured Attention Mechanisms in Partially Observable Reinforcement Learning](https://arxiv.org/abs/2511.06946)
*Daniel De Dios Allegue,Jinke He,Frans A. Oliehoek*

Main category: cs.LG

TL;DR: 本文提出在Transformer自注意力机制中引入结构化归纳先验，包括每头记忆长度先验和分布先验，以改善部分可观测环境下强化学习的动态建模效率。


<details>
  <summary>Details</summary>
Motivation: 标准自注意力机制在强化学习轨迹中效率低下，因为它均匀分配权重给所有历史标记，而不是强调对控制至关重要的少数关键转换。

Method: 在动态头部的自注意力机制中引入两种结构化归纳先验：(i) 每头记忆长度先验，将注意力限制在任务特定窗口内；(ii) 分布先验，学习对过去状态-动作对的平滑高斯加权。

Result: 在Atari 100k基准测试中，高斯先验实现了77%的相对改进，而记忆长度先验往往因过于严格的截断而削减有用信号。

Conclusion: 在具有非平稳时间依赖性的部分可观测RL领域，平滑分布先验比离散记忆窗口更灵活适应不同时间范围，产生更稳健的数据效率。

Abstract: Transformers have shown strong ability to model long-term dependencies and
are increasingly adopted as world models in model-based reinforcement learning
(RL) under partial observability. However, unlike natural language corpora, RL
trajectories are sparse and reward-driven, making standard self-attention
inefficient because it distributes weight uniformly across all past tokens
rather than emphasizing the few transitions critical for control. To address
this, we introduce structured inductive priors into the self-attention
mechanism of the dynamics head: (i) per-head memory-length priors that
constrain attention to task-specific windows, and (ii) distributional priors
that learn smooth Gaussian weightings over past state-action pairs. We
integrate these mechanisms into UniZero, a model-based RL agent with a
Transformer-based world model that supports planning under partial
observability. Experiments on the Atari 100k benchmark show that most
efficiency gains arise from the Gaussian prior, which smoothly allocates
attention to informative transitions, while memory-length priors often truncate
useful signals with overly restrictive cut-offs. In particular, Gaussian
Attention achieves a 77% relative improvement in mean human-normalized scores
over UniZero. These findings suggest that in partially observable RL domains
with non-stationary temporal dependencies, discrete memory windows are
difficult to learn reliably, whereas smooth distributional priors flexibly
adapt across horizons and yield more robust data efficiency. Overall, our
results demonstrate that encoding structured temporal priors directly into
self-attention improves the prioritization of informative histories for
dynamics modeling under partial observability.

</details>


### [167] [MobileLLM-Pro Technical Report](https://arxiv.org/abs/2511.06719)
*Patrick Huber,Ernie Chang,Wei Wen,Igor Fedorov,Tarek Elgamal,Hanxian Huang,Naveen Suda,Chinnadhurai Sankar,Vish Vogeti,Yanghan Wang,Alex Gladkov,Kai Sheng Tai,Abdelrahman Elogeel,Tarek Hefny,Vikas Chandra,Ahmed Aly,Anuj Kumar,Raghuraman Krishnamoorthi,Adithya Sagar*

Main category: cs.LG

TL;DR: MobileLLM-Pro是一个10亿参数的设备端语言模型，在11个标准基准测试中取得最先进结果，支持128,000个token的长上下文窗口，并在4位量化下仅显示轻微性能下降。


<details>
  <summary>Details</summary>
Motivation: 开发高效的设备端语言模型对于在移动和可穿戴设备上支持低延迟AI应用至关重要，但在保持强性能的同时支持长上下文窗口和实际部署仍然是一个重大挑战。

Method: 采用四项核心技术：1) 隐式位置蒸馏，通过知识蒸馏有效培养长上下文能力；2) 专家模型合并框架，将多个领域专家融合到紧凑模型中；3) 基于效用估计的模拟驱动数据混合；4) 4位量化感知训练与自蒸馏。

Result: 在11个标准基准测试中显著优于Gemma 3-1B和Llama 3.2-1B，支持128,000个token的长上下文窗口，4位量化下性能下降很小。

Conclusion: MobileLLM-Pro为设备端高效语言模型提供了有效的解决方案，通过创新的技术组合实现了在紧凑模型规模下的卓越性能，并发布了模型权重和代码以支持未来研究。

Abstract: Efficient on-device language models around 1 billion parameters are essential
for powering low-latency AI applications on mobile and wearable devices.
However, achieving strong performance in this model class, while supporting
long context windows and practical deployment remains a significant challenge.
We introduce MobileLLM-Pro, a 1-billion-parameter language model optimized for
on-device deployment. MobileLLM-Pro achieves state-of-the-art results across 11
standard benchmarks, significantly outperforming both Gemma 3-1B and Llama
3.2-1B, while supporting context windows of up to 128,000 tokens and showing
only minor performance regressions at 4-bit quantization. These improvements
are enabled by four core innovations: (1) implicit positional distillation, a
novel technique that effectively instills long-context capabilities through
knowledge distillation; (2) a specialist model merging framework that fuses
multiple domain experts into a compact model without parameter growth; (3)
simulation-driven data mixing using utility estimation; and (4) 4-bit
quantization-aware training with self-distillation. We release our model
weights and code to support future research in efficient on-device language
models.

</details>


### [168] [Multi-Modal Continual Learning via Cross-Modality Adapters and Representation Alignment with Knowledge Preservation](https://arxiv.org/abs/2511.06723)
*Evelyn Chee,Wynne Hsu,Mong Li Lee*

Main category: cs.LG

TL;DR: 提出了一种基于预训练模型的多模态持续学习框架，通过跨模态适配器和表示对齐损失来有效整合多模态信息并减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法主要关注单模态数据，而多模态学习能利用多样化感官输入，但多模态持续学习面临整合新信息和防止遗忘的额外挑战。

Method: 使用预训练模型框架，包含具有专家混合结构的跨模态适配器来促进多模态信息整合，引入表示对齐损失学习鲁棒表示，并正则化表示间关系以保留先前任务知识。

Result: 在多个多模态数据集上的实验表明，该方法在类增量和域增量学习中均优于基线方法，实现了更高的准确率和更低的遗忘率。

Conclusion: 所提出的多模态持续学习框架能有效整合多模态信息并减少灾难性遗忘，在多种增量学习场景下表现优异。

Abstract: Continual learning is essential for adapting models to new tasks while
retaining previously acquired knowledge. While existing approaches
predominantly focus on uni-modal data, multi-modal learning offers substantial
benefits by utilizing diverse sensory inputs, akin to human perception.
However, multi-modal continual learning presents additional challenges, as the
model must effectively integrate new information from various modalities while
preventing catastrophic forgetting. In this work, we propose a pre-trained
model-based framework for multi-modal continual learning. Our framework
includes a novel cross-modality adapter with a mixture-of-experts structure to
facilitate effective integration of multi-modal information across tasks. We
also introduce a representation alignment loss that fosters learning of robust
multi-modal representations, and regularize relationships between learned
representations to preserve knowledge from previous tasks. Experiments on
several multi-modal datasets demonstrate that our approach consistently
outperforms baselines in both class-incremental and domain-incremental
learning, achieving higher accuracy and reduced forgetting.

</details>


### [169] [On the Joint Minimization of Regularization Loss Functions in Deep Variational Bayesian Methods for Attribute-Controlled Symbolic Music Generation](https://arxiv.org/abs/2511.07118)
*Matteo Pettenó,Alessandro Ilic Mezza,Alberto Bernardini*

Main category: cs.LG

TL;DR: 该论文研究了显式潜变量模型中KLD和属性正则化损失之间的平衡问题，发现在符号音乐生成中，合适的属性变换可以同时实现可控性和潜空间正则化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡KLD和属性正则化损失时存在困难：KLD主导时模型缺乏可控性，属性正则化主导时编码器会违反标准正态先验。

Method: 探索了在符号音乐生成中通过属性变换来平衡KLD和属性正则化损失的方法。

Result: 现有方法难以同时最小化两个正则化目标，而合适的属性变换可以帮助同时实现可控性和目标潜维度的正则化。

Conclusion: 在显式潜变量模型中，通过适当的属性变换可以解决KLD和属性正则化之间的平衡问题，实现更好的可控性和潜空间正则化。

Abstract: Explicit latent variable models provide a flexible yet powerful framework for
data synthesis, enabling controlled manipulation of generative factors. With
latent variables drawn from a tractable probability density function that can
be further constrained, these models enable continuous and semantically rich
exploration of the output space by navigating their latent spaces. Structured
latent representations are typically obtained through the joint minimization of
regularization loss functions. In variational information bottleneck models,
reconstruction loss and Kullback-Leibler Divergence (KLD) are often linearly
combined with an auxiliary Attribute-Regularization (AR) loss. However,
balancing KLD and AR turns out to be a very delicate matter. When KLD dominates
over AR, generative models tend to lack controllability; when AR dominates over
KLD, the stochastic encoder is encouraged to violate the standard normal prior.
We explore this trade-off in the context of symbolic music generation with
explicit control over continuous musical attributes. We show that existing
approaches struggle to jointly minimize both regularization objectives, whereas
suitable attribute transformations can help achieve both controllability and
regularization of the target latent dimensions.

</details>


### [170] [Dual Mamba for Node-Specific Representation Learning: Tackling Over-Smoothing with Selective State Space Modeling](https://arxiv.org/abs/2511.06756)
*Xin He,Yili Wang,Yiwei Dai,Xin Wang*

Main category: cs.LG

TL;DR: 提出DMbaGCN框架，通过集成Mamba到GNN中从局部和全局角度解决过平滑问题，包含局部状态演化Mamba和全局上下文感知Mamba两个模块。


<details>
  <summary>Details</summary>
Motivation: 现有方法如残差连接和跳跃层未能显式建模节点表示在层间的节点特定渐进演化，且未考虑全局信息，这些对于缓解过平滑问题至关重要。

Method: DMbaGCN包含LSEMba模块用于局部邻域聚合和捕获节点特定表示动态，以及GCAMba模块利用Mamba的全局注意力能力为每个节点融入全局上下文。

Result: 在多个基准测试上的广泛实验证明了该方法的有效性和效率。

Conclusion: 通过结合局部和全局组件，DMbaGCN增强了深度GNN中节点的可区分性，从而缓解了过平滑问题。

Abstract: Over-smoothing remains a fundamental challenge in deep Graph Neural Networks
(GNNs), where repeated message passing causes node representations to become
indistinguishable. While existing solutions, such as residual connections and
skip layers, alleviate this issue to some extent, they fail to explicitly model
how node representations evolve in a node-specific and progressive manner
across layers. Moreover, these methods do not take global information into
account, which is also crucial for mitigating the over-smoothing problem. To
address the aforementioned issues, in this work, we propose a Dual
Mamba-enhanced Graph Convolutional Network (DMbaGCN), which is a novel
framework that integrates Mamba into GNNs to address over-smoothing from both
local and global perspectives. DMbaGCN consists of two modules: the Local
State-Evolution Mamba (LSEMba) for local neighborhood aggregation and utilizing
Mamba's selective state space modeling to capture node-specific representation
dynamics across layers, and the Global Context-Aware Mamba (GCAMba) that
leverages Mamba's global attention capabilities to incorporate global context
for each node. By combining these components, DMbaGCN enhances node
discriminability in deep GNNs, thereby mitigating over-smoothing. Extensive
experiments on multiple benchmarks demonstrate the effectiveness and efficiency
of our method.

</details>


### [171] [Fuzzy Label: From Concept to Its Application in Label Learning](https://arxiv.org/abs/2511.07165)
*Chenxi Luoa,Zhuangzhuang Zhaoa,Zhaohong Denga,Te Zhangb*

Main category: cs.LG

TL;DR: 本文提出基于模糊集理论的模糊标签概念，以更好地捕捉和表示标签不确定性，并开发了从原始数据挖掘生成模糊标签的方法，增强了标签学习模型的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统标签学习方法使用二元逻辑标签，但在实际应用中标签标注存在显著不确定性，简化表示会丢失有价值信息并限制模型表达能力。

Method: 提出模糊标签概念，开发高效的模糊标签生成方法，并基于此构建模糊标签增强的单标签和多标签学习算法，以KNN和多标签KNN为例进行说明。

Result: 实验结果表明，模糊标签能更有效地表征真实世界标注信息，显著提升标签学习模型的性能。

Conclusion: 模糊标签方法能够更好地处理标签不确定性，为标签学习提供了更丰富和细致的表示方式，有效提升了模型性能。

Abstract: Label learning is a fundamental task in machine learning that aims to
construct intelligent models using labeled data, encompassing traditional
single-label and multi-label classification models. Traditional methods
typically rely on logical labels, such as binary indicators (e.g., "yes/no")
that specify whether an instance belongs to a given category. However, in
practical applications, label annotations often involve significant uncertainty
due to factors such as data noise, inherent ambiguity in the observed entities,
and the subjectivity of human annotators. Therefore, representing labels using
simplistic binary logic can obscure valuable information and limit the
expressiveness of label learning models. To overcome this limitation, this
paper introduces the concept of fuzzy labels, grounded in fuzzy set theory, to
better capture and represent label uncertainty. We further propose an efficient
fuzzy labeling method that mines and generates fuzzy labels from the original
data, thereby enriching the label space with more informative and nuanced
representations. Based on this foundation, we present fuzzy-label-enhanced
algorithms for both single-label and multi-label learning, using the classical
K-Nearest Neighbors (KNN) and multi-label KNN algorithms as illustrative
examples. Experimental results indicate that fuzzy labels can more effectively
characterize the real-world labeling information and significantly enhance the
performance of label learning models.

</details>


### [172] [Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time Search](https://arxiv.org/abs/2511.07312)
*Samuel Sokota,Eugene Vinitsky,Hengyuan Hu,J. Zico Kolter,Gabriele Farina*

Main category: cs.LG

TL;DR: 该研究在Stratego游戏中实现了重大突破，不仅达到了顶级人类水平，还实现了超人类表现，且训练成本从数百万美元大幅降低至数千美元。


<details>
  <summary>Details</summary>
Motivation: Stratego作为需要处理大量隐藏信息的战略决策游戏，之前的研究投入数百万美元仍未能达到顶级人类水平，这促使研究者寻找更高效的方法。

Method: 开发了通用的自对弈强化学习方法和不完全信息下的测试时搜索技术。

Result: 成功实现了超人类水平的Stratego游戏表现，同时将训练成本从数百万美元降至数千美元。

Conclusion: 证明了通过创新的自对弈强化学习和测试时搜索方法，可以在不完全信息游戏中以极低成本实现超人类表现。

Abstract: Few classical games have been regarded as such significant benchmarks of
artificial intelligence as to have justified training costs in the millions of
dollars. Among these, Stratego -- a board wargame exemplifying the challenge of
strategic decision making under massive amounts of hidden information -- stands
apart as a case where such efforts failed to produce performance at the level
of top humans. This work establishes a step change in both performance and cost
for Stratego, showing that it is now possible not only to reach the level of
top humans, but to achieve vastly superhuman level -- and that doing so
requires not an industrial budget, but merely a few thousand dollars. We
achieved this result by developing general approaches for self-play
reinforcement learning and test-time search under imperfect information.

</details>


### [173] [TNT: Improving Chunkwise Training for Test-Time Memorization](https://arxiv.org/abs/2511.07343)
*Zeman Li,Ali Behrouz,Yuan Deng,Peilin Zhong,Praneeth Kacham,Mahdi Karami,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.LG

TL;DR: TNT是一种新的训练范式，通过两阶段过程解决RNN训练效率与推理性能的冲突，实现17倍训练加速同时提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 现有RNN并行化方法存在chunksize超参数的根本冲突：大chunk提升速度但降低性能，需要固定次优折衷。

Method: 两阶段训练：第一阶段效率导向预训练，使用分层内存和周期性重置局部内存状态实现大规模上下文并行化；第二阶段微调，仅调整局部内存模块到更小chunksize。

Result: 在Titans和TTT模型上评估，TNT实现高达17倍训练加速，同时提高模型准确性。

Conclusion: TNT消除了关键的可扩展性障碍，为开发表达性RNN建立了实用基础，有助于未来缩小与Transformers的性能差距。

Abstract: Recurrent neural networks (RNNs) with deep test-time memorization modules,
such as Titans and TTT, represent a promising, linearly-scaling paradigm
distinct from Transformers. While these expressive models do not yet match the
peak performance of state-of-the-art Transformers, their potential has been
largely untapped due to prohibitively slow training and low hardware
utilization. Existing parallelization methods force a fundamental conflict
governed by the chunksize hyperparameter: large chunks boost speed but degrade
performance, necessitating a fixed, suboptimal compromise. To solve this
challenge, we introduce TNT, a novel training paradigm that decouples training
efficiency from inference performance through a two-stage process. Stage one is
an efficiency-focused pre-training phase utilizing a hierarchical memory. A
global module processes large, hardware-friendly chunks for long-range context,
while multiple parallel local modules handle fine-grained details. Crucially,
by periodically resetting local memory states, we break sequential dependencies
to enable massive context parallelization. Stage two is a brief fine-tuning
phase where only the local memory modules are adapted to a smaller,
high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluated
on Titans and TTT models, TNT achieves a substantial acceleration in training
speed-up to 17 times faster than the most accurate baseline configuration -
while simultaneously improving model accuracy. This improvement removes a
critical scalability barrier, establishing a practical foundation for
developing expressive RNNs and facilitating future work to close the
performance gap with Transformers.

</details>


### [174] [Rethinking Parameter Sharing as Graph Coloring for Structured Compression](https://arxiv.org/abs/2511.06786)
*Boyang Zhang,Daning Cheng,Yunquan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于几何准则的参数共享方法Geo-Sharing，通过二阶泰勒展开和Hessian谱分析，系统性地确定跨层参数共享配置，解决了传统启发式方法局限于相邻层且缺乏系统性分析的问题。


<details>
  <summary>Details</summary>
Motivation: 现代深度模型参数规模庞大，导致推理时内存使用过高，限制了实际部署。参数共享作为一种结构化压缩方法能有效减少冗余，但现有方法仅限于相邻层共享且缺乏系统性分析，跨层共享面临指数级增长的配置空间问题。

Method: 从群论视角将参数共享重新表述为在模型参数空间中引入结构对称性，使用着色函数描述共享配置。提出基于泰勒展开和Hessian谱的二阶几何准则，通过将扰动投影到Hessian的低曲率特征子空间，为选择最小化性能影响的共享组提供解析规则。

Result: 在各种架构和任务中，Geo-Sharing持续优于最先进的启发式共享策略，在更高压缩比下实现更小的精度损失。

Conclusion: Geo-Sharing提供了一种原则性和可扩展的配置程序，通过几何准则系统性地解决了参数共享的配置瓶颈问题。

Abstract: Modern deep models have massive parameter sizes, leading to high
inference-time memory usage that limits practical deployment. Parameter
sharing, a form of structured compression, effectively reduces redundancy, but
existing approaches remain heuristic-restricted to adjacent layers and lacking
a systematic analysis for cross-layer sharing. However, extending sharing
across multiple layers leads to an exponentially expanding configuration space,
making exhaustive search computationally infeasible and forming a critical
bottleneck for parameter sharing. We recast parameter sharing from a
group-theoretic perspective as introducing structural symmetries in the model's
parameter space. A sharing configuration can be described by a coloring
function $\alpha:L\rightarrow C$ (L: layer indices and C: sharing classes),
which determines inter-layer sharing groups while preserving structural
symmetry. To determine the coloring function, we propose a second-order
geometric criterion based on Taylor expansion and the Hessian spectrum. By
projecting perturbations onto the Hessian's low-curvature eigensubspace, the
criterion provides an analytic rule for selecting sharing groups that minimize
performance impact, yielding a principled and scalable configuration procedure.
Across diverse architectures and tasks, Geo-Sharing consistently outperforms
state-of-the-art heuristic sharing strategies, achieving higher compression
ratios with smaller accuracy degradation.

</details>


### [175] [Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection](https://arxiv.org/abs/2511.07364)
*Vaibhav Mavi,Shubh Jaroria,Weiqi Sun*

Main category: cs.LG

TL;DR: 该研究将LLM自评估技术扩展到多步推理任务，比较了整体评分和逐步评分两种方法，发现逐步评估在错误检测方面表现更好，AUC-ROC相对提升达15%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多步推理任务中的可靠性和故障检测对其在高风险场景的部署至关重要，现有方法主要关注单步输出，忽视了多步推理的挑战。

Method: 在多步任务中测试两种直观方法：整体评分和逐步评分，使用两个多步基准数据集进行验证。

Result: 逐步评估在检测潜在错误方面普遍优于整体评分，AUC-ROC相对提升最高达15%。

Conclusion: 自评估LLM系统在复杂推理中能提供有意义的置信度估计，提高了可信度，并为故障检测提供了实用框架。

Abstract: Reliability and failure detection of large language models (LLMs) is critical
for their deployment in high-stakes, multi-step reasoning tasks. Prior work
explores confidence estimation for self-evaluating LLM-scorer systems, with
confidence scorers estimating the likelihood of errors in LLM responses.
However, most methods focus on single-step outputs and overlook the challenges
of multi-step reasoning. In this work, we extend self-evaluation techniques to
multi-step tasks, testing two intuitive approaches: holistic scoring and
step-by-step scoring. Using two multi-step benchmark datasets, we show that
stepwise evaluation generally outperforms holistic scoring in detecting
potential errors, with up to 15% relative increase in AUC-ROC. Our findings
demonstrate that self-evaluating LLM systems provide meaningful confidence
estimates in complex reasoning, improving their trustworthiness and providing a
practical framework for failure detection.

</details>


### [176] [Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning](https://arxiv.org/abs/2511.07368)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Bo Xue,Qingfu Zhang,Hau-San Wong,Taiji Suzuki*

Main category: cs.LG

TL;DR: 该论文通过树状马尔可夫链模型解释了基础模型后训练中的探索悖论：虽然RLVR和ORM/PRM主要强化现有推理路径而非扩展推理范围，但探索仍然至关重要，因为它保留了解决困难问题所需的稀有推理路径。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型后训练中的探索悖论：为什么探索在RLVR和ORM/PRM主要强化现有推理路径的情况下仍然重要。

Method: 采用多任务树状马尔可夫链（TMC）模型，将简单和困难推理步骤建模为低概率和高概率的马尔可夫转移，形式化后训练动态。

Result: 理论分析显示：RLVR产生挤压效应减少推理熵；ORM/PRM奖励鼓励一致性而非准确性；基础模型的稀有高不确定性推理路径对解决困难问题至关重要。

Conclusion: 探索之所以重要，是因为它保留了被RLVR挤压或被推理缩放忽略的稀有但关键的推理路径，这些路径对解决困难问题实例至关重要。

Abstract: Foundation models exhibit broad knowledge but limited task-specific
reasoning, motivating post-training strategies such as RLVR and inference
scaling with outcome or process reward models (ORM/PRM). While recent work
highlights the role of exploration and entropy stability in improving pass@K,
empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforce
existing tree-like reasoning paths rather than expanding the reasoning scope,
raising the question of why exploration helps at all if no new patterns emerge.
  To reconcile this paradox, we adopt the perspective of Kim et al. (2025),
viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering a
symmetry) reasoning steps as low- versus high-probability Markov transitions,
and formalize post-training dynamics through Multi-task Tree-structured Markov
Chains (TMC). In this tractable model, pretraining corresponds to tree
expansion, while post-training corresponds to chain-of-thought reweighting. We
show that several phenomena recently observed in empirical studies arise
naturally in this setting: (1) RLVR induces a squeezing effect, reducing
reasoning entropy and forgetting some correct paths; (2) population rewards of
ORM/PRM encourage consistency rather than accuracy, thereby favoring common
patterns; and (3) certain rare, high-uncertainty reasoning paths by the base
model are responsible for solving hard problem instances.
  Together, these explain why exploration -- even when confined to the base
model's reasoning scope -- remains essential: it preserves access to rare but
crucial reasoning traces needed for difficult cases, which are squeezed out by
RLVR or unfavored by inference scaling. Building on this, we further show that
exploration strategies such as rejecting easy instances and KL regularization
help preserve rare reasoning traces. Empirical simulations corroborate our
theoretical results.

</details>


### [177] [Coupling Agent-based Modeling and Life Cycle Assessment to Analyze Trade-offs in Resilient Energy Transitions](https://arxiv.org/abs/2511.06791)
*Beichen Zhang,Mohammed T. Zaki,Hanna Breunig,Newsha K. Ajami*

Main category: cs.LG

TL;DR: 提出了一个集成建模框架，结合基于主体的建模和生命周期评估，用于模拟能源转型路径与区域资源竞争、生态约束和社区层面负担的相互作用，应用于南加州案例研究。


<details>
  <summary>Details</summary>
Motivation: 向可持续和韧性能源系统转型需要在环境、社会和资源维度之间导航复杂的权衡关系，现有评估往往孤立评估新兴能源路径及其影响，忽略了区域资源竞争和累积影响等关键相互作用。

Method: 开发了一个集成建模框架，耦合基于主体的建模（ABM）和生命周期评估（LCA），模拟能源转型路径与区域资源竞争、生态约束和社区层面负担的相互作用。

Result: 在南加州案例研究中，该框架展示了集成和多尺度决策如何塑造能源路径部署，并在情景驱动约束下揭示空间明确的权衡关系。

Conclusion: 该建模框架可以在空间和制度尺度上进一步支持更具适应性和韧性的能源转型规划。

Abstract: Transitioning to sustainable and resilient energy systems requires navigating
complex and interdependent trade-offs across environmental, social, and
resource dimensions. Neglecting these trade-offs can lead to unintended
consequences across sectors. However, existing assessments often evaluate
emerging energy pathways and their impacts in silos, overlooking critical
interactions such as regional resource competition and cumulative impacts. We
present an integrated modeling framework that couples agent-based modeling and
Life Cycle Assessment (LCA) to simulate how energy transition pathways interact
with regional resource competition, ecological constraints, and community-level
burdens. We apply the model to a case study in Southern California. The results
demonstrate how integrated and multiscale decision making can shape energy
pathway deployment and reveal spatially explicit trade-offs under
scenario-driven constraints. This modeling framework can further support more
adaptive and resilient energy transition planning on spatial and institutional
scales.

</details>


### [178] [Transformers Provably Learn Chain-of-Thought Reasoning with Length Generalization](https://arxiv.org/abs/2511.07378)
*Yu Huang,Zixin Wen,Aarti Singh,Yuejie Chi,Yuxin Chen*

Main category: cs.LG

TL;DR: 本文对Transformer在状态跟踪任务中的长度泛化能力进行了理论分析，证明了注意力集中机制如何影响链式思维推理的泛化性能，并提出了递归自训练方案来扩展可解决问题的长度范围。


<details>
  <summary>Details</summary>
Motivation: 研究AI推理能力的关键问题：模型是否能够将学习到的推理模式外推到更难的、需要更长链式思维的任务中。

Method: 对Transformer在合成状态跟踪任务上的梯度下降学习进行理论分析，通过注意力集中机制和任务代数结构来研究长度泛化，并提出递归自训练方案。

Result: 理论证明了注意力层的检索鲁棒性与状态跟踪任务结构之间的联系，提供了首个优化保证，证明常数深度Transformer能够学习NC^1完全问题，超越了先前局限于TC^0的工作。

Conclusion: Transformer能够通过注意力集中机制实现链式思维推理的长度泛化，递归自训练可以渐进扩展可解决问题的长度范围，为AI推理能力提供了理论基础。

Abstract: The ability to reason lies at the core of artificial intelligence (AI), and
challenging problems usually call for deeper and longer reasoning to tackle. A
crucial question about AI reasoning is whether models can extrapolate learned
reasoning patterns to solve harder tasks with longer chain-of-thought (CoT). In
this work, we present a theoretical analysis of transformers learning on
synthetic state-tracking tasks with gradient descent. We mathematically prove
how the algebraic structure of state-tracking problems governs the degree of
extrapolation of the learned CoT. Specifically, our theory characterizes the
length generalization of transformers through the mechanism of attention
concentration, linking the retrieval robustness of the attention layer to the
state-tracking task structure of long-context reasoning. Moreover, for
transformers with limited reasoning length, we prove that a recursive
self-training scheme can progressively extend the range of solvable problem
lengths. To our knowledge, we provide the first optimization guarantee that
constant-depth transformers provably learn $\mathsf{NC}^1$-complete problems
with CoT, significantly going beyond prior art confined in $\mathsf{TC}^0$,
unless the widely held conjecture $\mathsf{TC}^0 \neq \mathsf{NC}^1$ fails.
Finally, we present a broad set of experiments supporting our theoretical
results, confirming the length generalization behaviors and the mechanism of
attention concentration.

</details>


### [179] [Beyond Uniform Deletion: A Data Value-Weighted Framework for Certified Machine Unlearning](https://arxiv.org/abs/2511.06794)
*Lisong He,Yi Yang,Xiangyu Chang*

Main category: cs.LG

TL;DR: 提出了数据价值加权遗忘(DVWU)框架，考虑数据价值异质性，在机器遗忘过程中对贡献不同的数据点进行差异化处理，提升更新后模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘算法忽视了不同数据点对模型性能贡献不均的异质性，平等处理会降低更新模型的性能，需要解决这一局限性。

Method: 设计基于数据价值的加权策略，将其整合到遗忘过程中，实现差异化遗忘。以一阶牛顿更新为例，开发输出和目标扰动算法实现认证遗忘。

Result: 在合成和真实数据集上的实验表明，相比传统遗忘方法，该方法获得了更优的预测性能和鲁棒性。

Conclusion: DVWU框架可广泛适配现有机器遗忘方法，展示了在梯度上升方法中的可扩展性，为更广泛的基于梯度的深度遗忘方法提供了适应性。

Abstract: As the right to be forgotten becomes legislated worldwide, machine unlearning
mechanisms have emerged to efficiently update models for data deletion and
enhance user privacy protection. However, existing machine unlearning
algorithms frequently neglect the fact that different data points may
contribute unequally to model performance (i.e., heterogeneous data values).
Treat them equally in machine unlearning procedure can potentially degrading
the performance of updated models. To address this limitation, we propose Data
Value-Weighted Unlearning (DVWU), a general unlearning framework that accounts
for data value heterogeneity into the unlearning process. Specifically, we
design a weighting strategy based on data values, which are then integrated
into the unlearning procedure to enable differentiated unlearning for data
points with varying utility to the model. The DVWU framework can be broadly
adapted to various existing machine unlearning methods. We use the one-step
Newton update as an example for implementation, developing both output and
objective perturbation algorithms to achieve certified unlearning. Experiments
on both synthetic and real-world datasets demonstrate that our methods achieve
superior predictive performance and robustness compared to conventional
unlearning approaches. We further show the extensibility of our framework on
gradient ascent method by incorporating the proposed weighting strategy into
the gradient terms, highlighting the adaptability of DVWU for broader
gradient-based deep unlearning methods.

</details>


### [180] [Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning](https://arxiv.org/abs/2511.06854)
*Jiexi Liu,Meng Cao,Songcan Chen*

Main category: cs.LG

TL;DR: iTimER是一个用于不规则采样时间序列表示学习的自监督预训练框架，通过建模重构误差分布和生成伪观测值来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖观测值来插补缺失值或推断潜在动态，但忽略了重构误差这一重要的学习信号，该误差反映了模型对数据结构的捕捉能力。

Method: iTimER通过建模观测值的重构误差分布，使用混合策略在采样误差和最后可用观测值之间生成伪观测值，将未观测时间戳转换为噪声感知训练目标。使用Wasserstein度量对齐观测区域和伪观测区域的误差分布，并结合对比学习目标增强表示的可区分性。

Result: 在分类、插值和预测任务上的大量实验表明，iTimER在不规则采样时间序列设置下持续优于最先进的方法。

Conclusion: iTimER通过利用重构误差作为学习信号，有效提升了不规则采样时间序列的表示学习性能，证明了重构误差在时间序列建模中的重要性。

Abstract: Irregularly sampled time series (ISTS), characterized by non-uniform time
intervals with natural missingness, are prevalent in real-world applications.
Existing approaches for ISTS modeling primarily rely on observed values to
impute unobserved ones or infer latent dynamics. However, these methods
overlook a critical source of learning signal: the reconstruction error
inherently produced during model training. Such error implicitly reflects how
well a model captures the underlying data structure and can serve as an
informative proxy for unobserved values. To exploit this insight, we propose
iTimER, a simple yet effective self-supervised pre-training framework for ISTS
representation learning. iTimER models the distribution of reconstruction
errors over observed values and generates pseudo-observations for unobserved
timestamps through a mixup strategy between sampled errors and the last
available observations. This transforms unobserved timestamps into noise-aware
training targets, enabling meaningful reconstruction signals. A Wasserstein
metric aligns reconstruction error distributions between observed and
pseudo-observed regions, while a contrastive learning objective enhances the
discriminability of learned representations. Extensive experiments on
classification, interpolation, and forecasting tasks demonstrate that iTimER
consistently outperforms state-of-the-art methods under the ISTS setting.

</details>


### [181] [Contact Wasserstein Geodesics for Non-Conservative Schrodinger Bridges](https://arxiv.org/abs/2511.06856)
*Andrea Testa,Soren Hauberg,Tamim Asfour,Leonel Rozo*

Main category: cs.LG

TL;DR: 本文提出了一种非保守广义薛定谔桥（NCGSB）方法，通过接触哈密顿力学允许能量随时间变化，从而能够建模更广泛的现实随机过程。该方法将桥问题提升到有限维空间中的可处理测地线计算，并通过ResNet架构实现非迭代求解。


<details>
  <summary>Details</summary>
Motivation: 传统薛定谔桥方法受限于能量守恒假设，无法建模能量变化的随机过程。为了克服这一限制，需要开发能够捕捉更丰富中间动力学的新方法。

Method: 基于接触哈密顿力学引入非保守广义薛定谔桥（NCGSB），通过参数化Wasserstein流形将桥问题转化为有限维空间中的测地线计算，使用接触Wasserstein测地线（CWG）和ResNet架构实现非迭代求解。

Result: 在流形导航、分子动力学预测和图像生成等任务上验证了该框架的有效性，展示了其实际优势和多功能性。CWG支持通过调节任务特定距离度量来实现引导生成。

Conclusion: NCGSB提供了一个更广泛的随机过程建模框架，能够捕捉更丰富和更真实的中间动力学，计算效率高且支持引导生成，在多个应用领域展现出实用价值。

Abstract: The Schr\"odinger Bridge provides a principled framework for modeling
stochastic processes between distributions; however, existing methods are
limited by energy-conservation assumptions, which constrains the bridge's shape
preventing it from model varying-energy phenomena. To overcome this, we
introduce the non-conservative generalized Schr\"odinger bridge (NCGSB), a
novel, energy-varying reformulation based on contact Hamiltonian mechanics. By
allowing energy to change over time, the NCGSB provides a broader class of
real-world stochastic processes, capturing richer and more faithful
intermediate dynamics. By parameterizing the Wasserstein manifold, we lift the
bridge problem to a tractable geodesic computation in a finite-dimensional
space. Unlike computationally expensive iterative solutions, our contact
Wasserstein geodesic (CWG) is naturally implemented via a ResNet architecture
and relies on a non-iterative solver with near-linear complexity. Furthermore,
CWG supports guided generation by modulating a task-specific distance metric.
We validate our framework on tasks including manifold navigation, molecular
dynamics predictions, and image generation, demonstrating its practical
benefits and versatility.

</details>


### [182] [Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery](https://arxiv.org/abs/2511.06973)
*Ananad Krishnakumar,Vengadesh Ravikumaran*

Main category: cs.LG

TL;DR: 提出了一种结合语义嵌入、数据类型和空间位置的混合距离度量方法，用于量化电子表格的结构相似性，相比传统方法在模板重建方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统方法无法捕捉定义模板的空间布局和类型模式，需要新的方法来量化电子表格相似性。

Method: 将电子表格转换为单元格级嵌入，然后使用Chamfer和Hausdorff距离等聚合技术计算相似性。

Result: 在FUSTE数据集上实现了完美的模板重建（调整兰德指数1.00 vs 0.90），相比基于图的Mondrian基线表现出更优的无监督聚类性能。

Conclusion: 该方法支持大规模自动化模板发现，为表格集合的检索增强生成、模型训练和批量数据清理等下游应用提供了可能。

Abstract: Traditional methods for identifying structurally similar spreadsheets fail to
capture the spatial layouts and type patterns defining templates. To quantify
spreadsheet similarity, we introduce a hybrid distance metric that combines
semantic embeddings, data type information, and spatial positioning. In order
to calculate spreadsheet similarity, our method converts spreadsheets into
cell-level embeddings and then uses aggregation techniques like Chamfer and
Hausdorff distances. Experiments across template families demonstrate superior
unsupervised clustering performance compared to the graph-based Mondrian
baseline, achieving perfect template reconstruction (Adjusted Rand Index of
1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale
automated template discovery, which in turn enables downstream applications
such as retrieval-augmented generation over tabular collections, model
training, and bulk data cleaning.

</details>


### [183] [Rethinking Crystal Symmetry Prediction: A Decoupled Perspective](https://arxiv.org/abs/2511.06976)
*Liheng Yu,Zhe Zhao,Xucong Wang,Di Wu,Pengkun Wang*

Main category: cs.LG

TL;DR: XRDecoupler框架通过解耦视角解决晶体对称性分析中的子属性混淆问题，结合多维晶体对称信息作为超类指导，设计分层PXRD模式学习模型和多目标优化方法，在多个主流数据库中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法盲目应用深度学习模型而忽略化学规则，面临严重的子属性混淆问题，需要开发更符合化学直觉的对称性分析方法。

Method: 从解耦视角出发，引入多维晶体对称信息作为超类指导，设计分层PXRD模式学习模型和多目标优化方法，确保模型预测过程符合化学直觉。

Result: 在CCDC、CoREMOF和InorganicData三个主流数据库上的综合评估表明，XRDecoupler在性能、可解释性和泛化能力方面表现卓越。

Conclusion: XRDecoupler框架有效解决了晶体对称性分析中的子属性混淆问题，为结构分析提供了高性能、可解释且泛化能力强的解决方案。

Abstract: Efficiently and accurately determining the symmetry is a crucial step in the
structural analysis of crystalline materials. Existing methods usually
mindlessly apply deep learning models while ignoring the underlying chemical
rules. More importantly, experiments show that they face a serious sub-property
confusion SPC problem. To address the above challenges, from a decoupled
perspective, we introduce the XRDecoupler framework, a problem-solving arsenal
specifically designed to tackle the SPC problem. Imitating the thinking process
of chemists, we innovatively incorporate multidimensional crystal symmetry
information as superclass guidance to ensure that the model's prediction
process aligns with chemical intuition. We further design a hierarchical PXRD
pattern learning model and a multi-objective optimization approach to achieve
high-quality representation and balanced optimization. Comprehensive
evaluations on three mainstream databases (e.g., CCDC, CoREMOF, and
InorganicData) demonstrate that XRDecoupler excels in performance,
interpretability, and generalization.

</details>


### [184] [Fast Bayesian Updates via Harmonic Representations](https://arxiv.org/abs/2511.06978)
*Di Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于谐波分析的快速贝叶斯更新统一框架，将贝叶斯更新转化为谱卷积，利用FFT实现O(N log N)复杂度的确定性算法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯推断方法如MCMC和变分推理存在计算可扩展性和效率限制，特别是证据积分的计算难题。

Method: 在正交基中表示先验和似然函数，将贝叶斯更新规则转化为谱卷积，引入谱截断方案实现有限维近似，利用FFT进行快速计算。

Result: 该方法将计算复杂度从O(N^2)降低到O(N log N)，对于平滑函数能获得高精度的有限维近似。

Conclusion: 该工作实现了贝叶斯计算的范式转变，将贝叶斯计算与信号处理联系起来，为广泛类别问题中的实时顺序推断开辟了新途径。

Abstract: Bayesian inference, while foundational to probabilistic reasoning, is often
hampered by the computational intractability of posterior distributions,
particularly through the challenging evidence integral. Conventional approaches
like Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) face
significant scalability and efficiency limitations. This paper introduces a
novel, unifying framework for fast Bayesian updates by leveraging harmonic
analysis. We demonstrate that representing the prior and likelihood in a
suitable orthogonal basis transforms the Bayesian update rule into a spectral
convolution. Specifically, the Fourier coefficients of the posterior are shown
to be the normalized convolution of the prior and likelihood coefficients. To
achieve computational feasibility, we introduce a spectral truncation scheme,
which, for smooth functions, yields an exceptionally accurate
finite-dimensional approximation and reduces the update to a circular
convolution. This formulation allows us to exploit the Fast Fourier Transform
(FFT), resulting in a deterministic algorithm with O(N log N) complexity -- a
substantial improvement over the O(N^2) cost of naive methods. We establish
rigorous mathematical criteria for the applicability of our method, linking its
efficiency to the smoothness and spectral decay of the involved distributions.
The presented work offers a paradigm shift, connecting Bayesian computation to
signal processing and opening avenues for real-time, sequential inference in a
wide class of problems.

</details>


### [185] [Correcting False Alarms from Unseen: Adapting Graph Anomaly Detectors at Test Time](https://arxiv.org/abs/2511.07023)
*Junjun Pan,Yixin Liu,Chuan Zhou,Fei Xiong,Alan Wee-Chung Liew,Shirui Pan*

Main category: cs.LG

TL;DR: 提出了一种轻量级即插即用的测试时适应框架TUNE，用于在图异常检测中纠正未见过的正常模式，解决训练和测试分布不一致导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中图异常检测模型在部署时会遇到未见过的正常样本，导致分布偏移和性能下降，而重新训练或微调模型成本高昂且不切实际。

Method: 使用图对齐器在属性级别对齐偏移数据，并通过最小化表示级偏移作为监督信号来训练对齐器，利用聚合污染作为正态性偏移的关键指标。

Result: 在10个真实世界数据集上的广泛实验表明，TUNE显著提升了预训练图异常检测模型对合成和真实未见正常模式的泛化能力。

Conclusion: TUNE框架有效解决了图异常检测中的分布偏移问题，通过轻量级的测试时适应方法提高了模型在实际部署中的鲁棒性。

Abstract: Graph anomaly detection (GAD), which aims to detect outliers in
graph-structured data, has received increasing research attention recently.
However, existing GAD methods assume identical training and testing
distributions, which is rarely valid in practice. In real-world scenarios,
unseen but normal samples may emerge during deployment, leading to a normality
shift that degrades the performance of GAD models trained on the original data.
Through empirical analysis, we reveal that the degradation arises from (1)
semantic confusion, where unseen normal samples are misinterpreted as anomalies
due to their novel patterns, and (2) aggregation contamination, where the
representations of seen normal nodes are distorted by unseen normals through
message aggregation. While retraining or fine-tuning GAD models could be a
potential solution to the above challenges, the high cost of model retraining
and the difficulty of obtaining labeled data often render this approach
impractical in real-world applications. To bridge the gap, we proposed a
lightweight and plug-and-play Test-time adaptation framework for correcting
Unseen Normal pattErns (TUNE) in GAD. To address semantic confusion, a graph
aligner is employed to align the shifted data to the original one at the graph
attribute level. Moreover, we utilize the minimization of representation-level
shift as a supervision signal to train the aligner, which leverages the
estimated aggregation contamination as a key indicator of normality shift.
Extensive experiments on 10 real-world datasets demonstrate that TUNE
significantly enhances the generalizability of pre-trained GAD models to both
synthetic and real unseen normal patterns.

</details>


### [186] [Fair Bayesian Data Selection via Generalized Discrepancy Measures](https://arxiv.org/abs/2511.07032)
*Yixuan Zhang,Jiabin Luo,Zhenggang Wang,Feng Zhou,Quyu Kong*

Main category: cs.LG

TL;DR: 提出了一种贝叶斯数据选择框架，通过对齐组特定的模型参数后验分布和样本权重与共享中心分布来确保公平性，支持多种分布差异度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法通常在模型层面干预，存在计算成本高、可扩展性差和泛化能力弱的问题，需要更有效的数据中心化公平性解决方案。

Method: 采用贝叶斯数据选择框架，使用Wasserstein距离、最大均值差异和f-散度等分布差异度量来对齐组特定后验分布与共享中心分布，无需显式公平性约束。

Result: 在基准数据集上的实验表明，该方法在公平性和准确性方面均优于现有的数据选择和基于模型的公平性方法。

Conclusion: 该数据中心化方法能够有效缓解训练数据中的组特定偏差，提高下游任务的公平性，并具有理论保证。

Abstract: Fairness concerns are increasingly critical as machine learning models are
deployed in high-stakes applications. While existing fairness-aware methods
typically intervene at the model level, they often suffer from high
computational costs, limited scalability, and poor generalization. To address
these challenges, we propose a Bayesian data selection framework that ensures
fairness by aligning group-specific posterior distributions of model parameters
and sample weights with a shared central distribution. Our framework supports
flexible alignment via various distributional discrepancy measures, including
Wasserstein distance, maximum mean discrepancy, and $f$-divergence, allowing
geometry-aware control without imposing explicit fairness constraints. This
data-centric approach mitigates group-specific biases in training data and
improves fairness in downstream tasks, with theoretical guarantees. Experiments
on benchmark datasets show that our method consistently outperforms existing
data selection and model-based fairness methods in both fairness and accuracy.

</details>


### [187] [Breaking Privacy in Federated Clustering: Perfect Input Reconstruction via Temporal Correlations](https://arxiv.org/abs/2511.07073)
*Guang Yang,Lixia Luo,Qiongxiu Li*

Main category: cs.LG

TL;DR: 本文重新审视联邦聚类中发布中间质心是否安全的问题，发现k-means迭代中的时间规律性会产生可利用结构，使得完美输入重建成为可能。作者提出了轨迹感知重建(TAR)攻击，结合时间分配信息和代数分析来恢复原始输入，证明质心发布会显著损害隐私。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类允许多方在不共享原始样本的情况下发现分布式数据中的模式。为了减少开销，许多协议在训练期间公开中间质心。虽然通常被认为对效率无害，但这种披露是否会损害隐私仍然是一个悬而未决的问题。先前的研究将该问题建模为所谓的隐藏子集和问题(HSSP)，并认为质心发布可能是安全的，因为经典的HSSP攻击无法恢复输入。

Method: 作者提出了轨迹感知重建(TAR)攻击，该攻击结合了k-means迭代中的时间分配信息和代数分析。通过利用k-means迭代过程中产生的时间规律性结构，该攻击能够从公开的质心轨迹中恢复精确的原始输入。

Result: 研究结果表明，质心披露在联邦聚类中显著损害了隐私。TAR攻击能够完美地重建原始输入，这为质心发布的安全性提供了第一个严格的证据，并揭示了一个实际的攻击方法。

Conclusion: 本文的发现揭示了联邦聚类中隐私与效率之间的基本张力。通过证明质心披露会导致严重的隐私泄露，作者强调了在联邦学习系统中需要更严格的隐私保护措施，特别是在涉及聚类任务时。

Abstract: Federated clustering allows multiple parties to discover patterns in
distributed data without sharing raw samples. To reduce overhead, many
protocols disclose intermediate centroids during training. While often treated
as harmless for efficiency, whether such disclosure compromises privacy remains
an open question. Prior analyses modeled the problem as a so-called Hidden
Subset Sum Problem (HSSP) and argued that centroid release may be safe, since
classical HSSP attacks fail to recover inputs.
  We revisit this question and uncover a new leakage mechanism: temporal
regularities in $k$-means iterations create exploitable structure that enables
perfect input reconstruction. Building on this insight, we propose
Trajectory-Aware Reconstruction (TAR), an attack that combines temporal
assignment information with algebraic analysis to recover exact original
inputs. Our findings provide the first rigorous evidence, supported by a
practical attack, that centroid disclosure in federated clustering
significantly compromises privacy, exposing a fundamental tension between
privacy and efficiency.

</details>


### [188] [REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks](https://arxiv.org/abs/2511.07127)
*Linna Wang,Zhixuan You,Qihui Zhang,Jiunan Wen,Ji Shi,Yimin Chen,Yusen Wang,Fanqi Ding,Ziliang Feng,Li Lu*

Main category: cs.LG

TL;DR: REACT-LLM是一个评估LLMs与因果特征结合在临床风险预测中性能的基准测试，发现LLMs在临床预后方面表现合理但尚未超越传统ML模型，因果特征的整合带来的性能提升有限。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏系统评估LLMs与因果学习在临床风险预测中整合的基准，且临床决策需要识别对结果具有因果影响的特征以获得可操作和可信的预测。

Method: 引入REACT-LLM基准，在2个真实世界数据集上评估7个临床结果，比较15个主要LLMs、6个传统ML模型和3个因果发现算法。

Result: LLMs在临床预后中表现合理但尚未超越传统ML模型；将因果发现算法得出的因果特征整合到LLMs中仅带来有限的性能提升，主要由于许多因果发现方法的严格假设在复杂临床数据中常被违反。

Conclusion: 虽然直接整合带来的改进有限，但基准揭示了一个更有前景的协同作用，表明LLMs与因果学习在临床决策中的结合潜力需要进一步探索。

Abstract: Large Language Models (LLMs) and causal learning each hold strong potential
for clinical decision making (CDM). However, their synergy remains poorly
understood, largely due to the lack of systematic benchmarks evaluating their
integration in clinical risk prediction. In real-world healthcare, identifying
features with causal influence on outcomes is crucial for actionable and
trustworthy predictions. While recent work highlights LLMs' emerging causal
reasoning abilities, there lacks comprehensive benchmarks to assess their
causal learning and performance informed by causal features in clinical risk
prediction. To address this, we introduce REACT-LLM, a benchmark designed to
evaluate whether combining LLMs with causal features can enhance clinical
prognostic performance and potentially outperform traditional machine learning
(ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a
limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2
real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and
3 causal discovery (CD) algorithms. Our findings indicate that while LLMs
perform reasonably in clinical prognostics, they have not yet outperformed
traditional ML models. Integrating causal features derived from CD algorithms
into LLMs offers limited performance gains, primarily due to the strict
assumptions of many CD methods, which are often violated in complex clinical
data. While the direct integration yields limited improvement, our benchmark
reveals a more promising synergy.

</details>


### [189] [Guiding Generative Models to Uncover Diverse and Novel Crystals via Reinforcement Learning](https://arxiv.org/abs/2511.07158)
*Hyunsoo Park,Aron Walsh*

Main category: cs.LG

TL;DR: 该论文提出了一种强化学习框架，指导潜在去噪扩散模型生成多样化、新颖且热力学可行的晶体化合物，解决了生成建模中基于似然采样与目标导向探索之间的不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前生成人工智能在晶体材料发现中存在根本挑战：生成建模中的基于似然采样与针对未探索区域（新化合物所在位置）的目标导向之间存在客观不对齐。

Method: 引入强化学习框架，结合组相对策略优化和可验证的多目标奖励，共同平衡创造性、稳定性和多样性，指导潜在去噪扩散模型。

Result: 该方法不仅实现了从头生成，还展示了增强的属性引导设计，在保持化学有效性的同时针对所需功能特性。

Conclusion: 该框架为可控AI驱动逆向设计建立了模块化基础，解决了生成模型在科学发现应用中的新颖性与有效性权衡问题。

Abstract: Discovering functional crystalline materials entails navigating an immense
combinatorial design space. While recent advances in generative artificial
intelligence have enabled the sampling of chemically plausible compositions and
structures, a fundamental challenge remains: the objective misalignment between
likelihood-based sampling in generative modelling and targeted focus on
underexplored regions where novel compounds reside. Here, we introduce a
reinforcement learning framework that guides latent denoising diffusion models
toward diverse and novel, yet thermodynamically viable crystalline compounds.
Our approach integrates group relative policy optimisation with verifiable,
multi-objective rewards that jointly balance creativity, stability, and
diversity. Beyond de novo generation, we demonstrate enhanced property-guided
design that preserves chemical validity, while targeting desired functional
properties. This approach establishes a modular foundation for controllable
AI-driven inverse design that addresses the novelty-validity trade-off across
scientific discovery applications of generative models.

</details>


### [190] [Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning](https://arxiv.org/abs/2511.07198)
*Hua Ye,Siyuan Chen,Haoliang Zhang,Weihao Luo,Yanbin Li,Xuan Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于分区的多阶段微调框架，通过平衡领域差异、协同效应和模型容量约束，将领域划分为子集，以利用领域间协同作用并最小化负迁移。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在跨多个异构领域时存在领域间干扰问题，需要有效的方法来利用领域间协同作用同时避免负迁移。

Method: 采用基于分区的多阶段微调框架，通过理论分析推导出泛化边界来指导领域划分策略。

Result: 在各种语言理解任务上的广泛实证评估表明，该方法始终优于最先进的基线方法。

Conclusion: 提出的分区多阶段微调框架能够有效解决跨领域适应中的干扰问题，并在多个任务上取得优异性能。

Abstract: Large language models (LLMs) demonstrate impressive generalization abilities,
yet adapting them effectively across multiple heterogeneous domains remains
challenging due to inter-domain interference. To overcome this challenge, we
propose a partition-based multi-stage fine-tuning framework designed to exploit
inter-domain synergies while minimizing negative transfer. Our approach
strategically partitions domains into subsets (stages) by balancing domain
discrepancy, synergy, and model capacity constraints. We theoretically analyze
the proposed framework and derive novel generalization bounds that justify our
partitioning strategy. Extensive empirical evaluations on various language
understanding tasks show that our method consistently outperforms
state-of-the-art baselines.

</details>


### [191] [DETECT: Data-Driven Evaluation of Treatments Enabled by Classification Transformers](https://arxiv.org/abs/2511.07213)
*Yuanheng Mao,Lillian Yang,Stephen Yang,Ethan Shao,Zihan Li*

Main category: cs.LG

TL;DR: DETECT是一个数据驱动的框架，通过比较治疗前后患者日常生活活动来评估治疗效果，使用智能手机传感器数据提供客观评估。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛是全球健康挑战，传统的主观评估方法（如数字评分量表）存在局限性，需要客观可靠的方法来测量临床治疗的功能影响。

Method: 提出DETECT框架，使用公共基准数据集和智能手机传感器模拟的患者数据，通过分类变换器比较治疗前后患者日常生活活动。

Result: DETECT被证明是客观且轻量级的，能够显著改进临床决策，为治疗影响提供更好的理解。

Conclusion: DETECT可以作为独立工具或与其他自我报告指标结合使用，帮助医生实现更个性化和响应性的患者护理。

Abstract: Chronic pain is a global health challenge affecting millions of individuals,
making it essential for physicians to have reliable and objective methods to
measure the functional impact of clinical treatments. Traditionally used
methods, like the numeric rating scale, while personalized and easy to use, are
subjective due to their self-reported nature. Thus, this paper proposes DETECT
(Data-Driven Evaluation of Treatments Enabled by Classification Transformers),
a data-driven framework that assesses treatment success by comparing patient
activities of daily life before and after treatment. We use DETECT on public
benchmark datasets and simulated patient data from smartphone sensors. Our
results demonstrate that DETECT is objective yet lightweight, making it a
significant and novel contribution to clinical decision-making. By using
DETECT, independently or together with other self-reported metrics, physicians
can improve their understanding of their treatment impacts, ultimately leading
to more personalized and responsive patient care.

</details>


### [192] [Does TabPFN Understand Causal Structures?](https://arxiv.org/abs/2511.07236)
*Omar Swelam,Lennart Purucker,Jake Robertson,Hanne Raum,Joschka Boedecker,Frank Hutter*

Main category: cs.LG

TL;DR: 本文研究了TabPFN（一种基于transformer的表格基础模型）是否在其内部表示中编码了因果信息，并开发了一个适配器框架来提取这些因果信号用于因果发现。


<details>
  <summary>Details</summary>
Motivation: 因果发现对多个科学领域至关重要，但从真实世界数据中提取因果信息仍然是一个重大挑战。鉴于TabPFN在真实数据上的成功表现，研究者希望探索这个预训练模型是否在其内部表示中编码了因果信息。

Method: 开发了一个适配器框架，使用可学习的解码器和因果标记，从TabPFN的冻结嵌入中提取因果信号，并将其解码为用于因果发现的邻接矩阵。

Result: 评估表明TabPFN的嵌入包含因果信息，其性能优于几种传统的因果发现算法，且这些因果信息主要集中在中间层。

Conclusion: 这些发现为基础模型的可解释性和适应性开辟了新方向，并展示了利用预训练表格模型进行因果发现的潜力。

Abstract: Causal discovery is fundamental for multiple scientific domains, yet
extracting causal information from real world data remains a significant
challenge. Given the recent success on real data, we investigate whether
TabPFN, a transformer-based tabular foundation model pre-trained on synthetic
datasets generated from structural causal models, encodes causal information in
its internal representations. We develop an adapter framework using a learnable
decoder and causal tokens that extract causal signals from TabPFN's frozen
embeddings and decode them into adjacency matrices for causal discovery. Our
evaluations demonstrate that TabPFN's embeddings contain causal information,
outperforming several traditional causal discovery algorithms, with such causal
information being concentrated in mid-range layers. These findings establish a
new direction for interpretable and adaptable foundation models and demonstrate
the potential for leveraging pre-trained tabular models for causal discovery.

</details>


### [193] [The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models](https://arxiv.org/abs/2511.07237)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 本文发现时间序列预测模型存在缩放悖论：模型规模增大但性能不提升。通过分析发现只有少数层真正有效，大多数层冗余甚至干扰训练。提出保留关键层的方法，仅用21%参数就能提升12%准确率并加速2.7倍。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型普遍认为扩大模型规模和训练数据量能提升性能，但作者观察到缩放悖论现象：更大的模型并不总是带来更好的性能。

Method: 通过分析模型内部表示，识别出少数层主导现象，提出自动识别和保留关键层的方法，大幅减少冗余参数。

Result: 在4种规模（1亿到17亿参数）和多种数据（最多60亿观测值）上的实验证实缩放悖论普遍存在。仅保留21%参数就能提升12%准确率，推理速度提升2.7倍。在8个SOTA模型上验证，保留不到30%层就能在95%任务中达到相当或更好的准确率。

Conclusion: 时间序列预测模型存在严重的层冗余问题，通过识别和保留关键层可以显著提升模型效率和性能，这对大规模时间序列模型的开发具有重要指导意义。

Abstract: Large-scale models are at the forefront of time series (TS) forecasting,
dominated by two paradigms: fine-tuning text-based Large Language Models
(LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both
approaches share a foundational assumption that scaling up model capacity and
data volume leads to improved performance. However, we observe a
\textit{\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon
that larger models do \emph{NOT} achieve better performance. Through extensive
experiments on two model families across four scales (100M to 1.7B parameters)
and diverse data (up to 6B observations), we rigorously confirm that the
scaling paradox is a pervasive issue. We then diagnose its root cause by
analyzing internal representations, identifying a phenomenon we call
\textit{few-layer dominance}: only a small subset of layers are functionally
important, while the majority are redundant, under-utilized, and can even
distract training. Based on this discovery, we propose a practical method to
automatically identify and retain only these dominant layers. In our models,
retaining only 21\% of the parameters achieves up to a 12\% accuracy
improvement and a 2.7$\times$ inference speedup. We validate the universality
of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing
that retaining less than 30\% of layers achieves comparable or superior
accuracy in over 95\% of tasks.

</details>


### [194] [RobustA: Robust Anomaly Detection in Multimodal Data](https://arxiv.org/abs/2511.07276)
*Salem AlMarri,Muhammad Irzam Liaqat,Muhammad Zaigham Zaheer,Shah Nawaz,Karthik Nandakumar,Markus Schedl*

Main category: cs.LG

TL;DR: 本文提出了首个系统研究多模态异常检测中模态损坏问题的框架，包括创建包含音频和视觉损坏的评估数据集RobustA，并提出了一种对损坏模态具有鲁棒性的多模态异常检测方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的多模态数据经常因环境失真而损坏，但目前缺乏对损坏模态如何影响多模态异常检测性能的系统研究。

Method: 提出了一种多模态异常检测方法，学习不同模态的共享表示空间，并在推理时基于估计的损坏程度采用动态加权方案。

Result: 创建了RobustA评估数据集，系统观察了音频和视觉损坏对异常检测系统的影响，提出的方法在损坏模态情况下表现出显著鲁棒性。

Conclusion: 这项工作推动了多模态异常检测在现实世界中的应用，解决了模态损坏可能发生的情况，相关数据集和特征将公开提供。

Abstract: In recent years, multimodal anomaly detection methods have demonstrated
remarkable performance improvements over video-only models. However, real-world
multimodal data is often corrupted due to unforeseen environmental distortions.
In this paper, we present the first-of-its-kind work that comprehensively
investigates the adverse effects of corrupted modalities on multimodal anomaly
detection task. To streamline this work, we propose RobustA, a carefully
curated evaluation dataset to systematically observe the impacts of audio and
visual corruptions on the overall effectiveness of anomaly detection systems.
Furthermore, we propose a multimodal anomaly detection method, which shows
notable resilience against corrupted modalities. The proposed method learns a
shared representation space for different modalities and employs a dynamic
weighting scheme during inference based on the estimated level of corruption.
Our work represents a significant step forward in enabling the real-world
application of multimodal anomaly detection, addressing situations where the
likely events of modality corruptions occur. The proposed evaluation dataset
with corrupted modalities and respective extracted features will be made
publicly available.

</details>


### [195] [Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training](https://arxiv.org/abs/2511.07328)
*Artyom Sorokin,Nazar Buzun,Alexander Anokhin,Oleg Inozemcev,Egor Vedernikov,Petr Anokhin,Mikhail Burtsev,Trushkov Alexey,Yin Wenshuai,Evgeny Burnaev*

Main category: cs.LG

TL;DR: Q-RAG是一种新的检索增强生成方法，通过强化学习微调嵌入模型实现多步检索，在长上下文基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法主要关注单步检索，无法有效回答需要多步搜索的复杂问题。现有的多步检索方法需要微调小LLM，资源消耗大且无法使用大模型。

Method: 提出Q-RAG方法，使用强化学习微调嵌入模型进行多步检索，提供资源高效的替代方案。

Result: 在开放域问答任务中表现优异，在Babilong和RULER长上下文基准测试中达到最先进结果，支持高达1000万token的上下文。

Conclusion: Q-RAG为多步检索提供了竞争性强且资源高效的解决方案，在长上下文处理方面具有显著优势。

Abstract: Retrieval-Augmented Generation (RAG) methods enhance LLM performance by
efficiently filtering relevant context for LLMs, reducing hallucinations and
inference cost. However, most existing RAG methods focus on single-step
retrieval, which is often insufficient for answering complex questions that
require multi-step search. Recently, multi-step retrieval approaches have
emerged, typically involving the fine-tuning of small LLMs to perform
multi-step retrieval. This type of fine-tuning is highly resource-intensive and
does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel
approach that fine-tunes the Embedder model for multi-step retrieval using
reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient
alternative to existing multi-step retrieval methods for open-domain question
answering and achieves state-of-the-art results on the popular long-context
benchmarks Babilong and RULER for contexts up to 10M tokens.

</details>


### [196] [Private Sketches for Linear Regression](https://arxiv.org/abs/2511.07365)
*Shrutimoy Das,Debanuj Nayak,Anirban Dasgupta*

Main category: cs.LG

TL;DR: 本文提出了一种差分隐私的线性回归方法，通过发布数据集的私有草图而非参数向量，解决了敏感数据隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 线性回归广泛应用于各领域，但敏感数据需要隐私保护。现有差分隐私方法通常估计带噪声的参数向量，我们希望提供更灵活的私有草图方法。

Method: 提出差分私有草图方法，针对最小二乘回归和最小绝对偏差回归问题，生成私有数据集草图而非直接估计参数。

Result: 开发了私有草图技术，使得常用回归求解器可以在不泄露隐私的情况下应用。

Conclusion: 私有草图方法为线性回归提供了有效的隐私保护解决方案，支持现有求解器的安全使用。

Abstract: Linear regression is frequently applied in a variety of domains. In order to
improve the efficiency of these methods, various methods have been developed
that compute summaries or \emph{sketches} of the datasets. Certain domains,
however, contain sensitive data which necessitates that the application of
these statistical methods does not reveal private information. Differentially
private (DP) linear regression methods have been developed for mitigating this
problem. These techniques typically involve estimating a noisy version of the
parameter vector. Instead, we propose releasing private sketches of the
datasets. We present differentially private sketches for the problems of least
squares regression, as well as least absolute deviations regression. The
availability of these private sketches facilitates the application of commonly
available solvers for regression, without the risk of privacy leakage.

</details>


### [197] [Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training](https://arxiv.org/abs/2511.07372)
*Dake Bu,Wei Huang,Andi Han,Atsushi Nitanda,Hau-San Wong,Qingfu Zhang,Taiji Suzuki*

Main category: cs.LG

TL;DR: 本文提出了一个理论框架来解释课程学习在LLM后训练阶段提升推理性能的原因，证明通过渐进式学习可避免指数级复杂度瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有课程学习技术被广泛观察到能提升LLM推理性能，但缺乏对其工作原理和有效范围的系统性理论理解。

Method: 基于CoT解决数学问题的洞察，将CoT生成建模为状态条件自回归推理树，定义均匀分支基础模型，并将课程阶段形式化为深度增加或提示减少的子任务。

Result: 在仅结果奖励信号下，强化学习微调以多项式样本复杂度实现高准确率，而直接学习则遭受指数级瓶颈。课程感知查询将奖励oracle调用和采样成本从指数级降至多项式级。

Conclusion: 课程后训练在满足连续阶段复杂度条件下可避免指数级复杂度瓶颈，为课程学习在LLM推理中的有效性提供了理论保证。

Abstract: Recent curriculum techniques in the post-training stage of LLMs have been
widely observed to outperform non-curriculum approaches in enhancing reasoning
performance, yet a principled understanding of why and to what extent they work
remains elusive. To address this gap, we develop a theoretical framework
grounded in the intuition that progressively learning through manageable steps
is more efficient than directly tackling a hard reasoning task, provided each
stage stays within the model's effective competence. Under mild complexity
conditions linking consecutive curriculum stages, we show that curriculum
post-training avoids the exponential complexity bottleneck.
  To substantiate this result, drawing insights from the Chain-of-Thoughts
(CoTs) solving mathematical problems such as Countdown and parity, we model CoT
generation as a states-conditioned autoregressive reasoning tree, define a
uniform-branching base model to capture pretrained behavior, and formalize
curriculum stages as either depth-increasing (longer reasoning chains) or
hint-decreasing (shorter prefixes) subtasks. Our analysis shows that, under
outcome-only reward signals, reinforcement learning finetuning achieves high
accuracy with polynomial sample complexity, whereas direct learning suffers
from an exponential bottleneck. We further establish analogous guarantees for
test-time scaling, where curriculum-aware querying reduces both reward oracle
calls and sampling cost from exponential to polynomial order.

</details>


### [198] [A Diffusion Model to Shrink Proteins While Maintaining Their Function](https://arxiv.org/abs/2511.07390)
*Ethan Baron,Alan N. Amin,Ruben Weitzman,Debora Marks,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: SCISOR是一种新颖的离散扩散模型，通过删除序列中的字母来生成类似自然蛋白质的样本，在蛋白质缩短任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 许多在医学和生物工程中有用的蛋白质由于序列过长而难以在实验室中制造、与细胞中其他蛋白质融合或递送到组织。缩短这些序列通常需要昂贵耗时的实验，需要开发能够高效搜索删除组合空间的方法。

Method: 提出SCISOR模型，训练一个去噪器来逆转向自然序列添加随机插入的前向噪声过程，通过离散扩散方法学习如何删除序列。

Result: SCISOR在进化序列数据拟合方面与先前大型模型竞争，在ProteinGym上实现了对删除功能效应的最先进预测，其建议的删除能产生更真实的蛋白质并更常保留功能基序。

Conclusion: SCISOR模型为蛋白质缩短提供了一种有效的方法，能够生成更接近自然序列的缩短蛋白质，并更好地保留功能特性。

Abstract: Many proteins useful in modern medicine or bioengineering are challenging to
make in the lab, fuse with other proteins in cells, or deliver to tissues in
the body, because their sequences are too long. Shortening these sequences
typically involves costly, time-consuming experimental campaigns. Ideally, we
could instead use modern models of massive databases of sequences from nature
to learn how to propose shrunken proteins that resemble sequences found in
nature. Unfortunately, these models struggle to efficiently search the
combinatorial space of all deletions, and are not trained with inductive biases
to learn how to delete. To address this gap, we propose SCISOR, a novel
discrete diffusion model that deletes letters from sequences to generate
protein samples that resemble those found in nature. To do so, SCISOR trains a
de-noiser to reverse a forward noising process that adds random insertions to
natural sequences. As a generative model, SCISOR fits evolutionary sequence
data competitively with previous large models. In evaluation, SCISOR achieves
state-of-the-art predictions of the functional effects of deletions on
ProteinGym. Finally, we use the SCISOR de-noiser to shrink long protein
sequences, and show that its suggested deletions result in significantly more
realistic proteins and more often preserve functional motifs than previous
models of evolutionary sequences.

</details>


### [199] [Entangled Schrödinger Bridge Matching](https://arxiv.org/abs/2511.07406)
*Sophia Tang,Yinuo Zhang,Pranam Chatterjee*

Main category: cs.LG

TL;DR: 提出了Entangled Schr"odinger Bridge Matching (EntangledSBM)框架，用于学习多粒子系统中相互作用的随机动力学，其中每个粒子的路径方向和大小动态依赖于其他粒子的路径。


<details>
  <summary>Details</summary>
Motivation: 模拟复杂能量景观上的多粒子系统轨迹是分子动力学和药物发现中的核心任务，但由于计算昂贵和长时间模拟而具有挑战性。现有方法无法捕捉动态演化的相互作用。

Method: 引入纠缠薛定谔桥匹配框架，通过求解耦合的偏置力系统来纠缠粒子速度，学习相互作用多粒子系统的一阶和二阶随机动力学。

Result: 该框架能够准确模拟扰动下的异质细胞群体和高维生物分子系统中的罕见转变。

Conclusion: EntangledSBM为解决多粒子系统中动态相互作用的模拟问题提供了有效框架。

Abstract: Simulating trajectories of multi-particle systems on complex energy
landscapes is a central task in molecular dynamics (MD) and drug discovery, but
remains challenging at scale due to computationally expensive and long
simulations. Previous approaches leverage techniques such as flow or
Schr\"odinger bridge matching to implicitly learn joint trajectories through
data snapshots. However, many systems, including biomolecular systems and
heterogeneous cell populations, undergo dynamic interactions that evolve over
their trajectory and cannot be captured through static snapshots. To close this
gap, we introduce Entangled Schr\"odinger Bridge Matching (EntangledSBM), a
framework that learns the first- and second-order stochastic dynamics of
interacting, multi-particle systems where the direction and magnitude of each
particle's path depend dynamically on the paths of the other particles. We
define the Entangled Schr\"odinger Bridge (EntangledSB) problem as solving a
coupled system of bias forces that entangle particle velocities. We show that
our framework accurately simulates heterogeneous cell populations under
perturbations and rare transitions in high-dimensional biomolecular systems.

</details>


### [200] [Routing Manifold Alignment Improves Generalization of Mixture-of-Experts LLMs](https://arxiv.org/abs/2511.07419)
*Zhongyang Li,Ziyue Li,Tianyi Zhou*

Main category: cs.LG

TL;DR: 提出RoMA方法，通过路由流形对齐来改进稀疏混合专家模型的路由器性能，仅需轻量级微调即可显著提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有MoE LLMs中的路由器在广泛下游任务中表现次优，与最优路由存在10-20%的性能差距，需要改进路由器以提升模型泛化性能。

Method: RoMA方法在训练后目标中引入额外的流形正则化项，仅微调路由器参数。该正则化鼓励每个样本的路由权重在任务嵌入空间中接近其成功邻居的路由权重。

Result: 在OLMoE、DeepSeekMoE和Qwen3-MoE模型上进行实验，RoMA在多样化基准测试中相比基线方法带来了显著改进。

Conclusion: RoMA展示了将任务理解与解决方案生成统一起来的优势，通过构建任务与专家之间的绑定关系实现了更好的泛化性能。

Abstract: Sparse Mixture-of-Experts (MoE) have been widely adopted in recent large
language models since it can efficiently scale up the model capability without
increasing the inference cost. However, evaluations on broad downstream tasks
reveal a consistent suboptimality of the routers in existing MoE LLMs, which
results in a severe performance gap (e.g., 10-20% in accuracy) to the optimal
routing. In this paper, we show that aligning the manifold of routing weights
with that of task embedding can effectively reduce the gap and improve MoE
LLMs' generalization performance. Our method, "Routing Manifold Alignment
(RoMA)", introduces an additional manifold regularization term in the
post-training objective and only requires lightweight finetuning of routers
(with other parameters frozen). Specifically, the regularization encourages the
routing weights of each sample to be close to those of its successful neighbors
(whose routing weights lead to correct answers) in a task embedding space.
Consequently, samples targeting similar tasks will share similar expert choices
across layers. Building such bindings between tasks and experts over different
samples is essential to achieve better generalization. Moreover, RoMA
demonstrates the advantage of unifying the task understanding (by embedding
models) with solution generation (by MoE LLMs). In experiments, we finetune
routers in OLMoE, DeepSeekMoE, and Qwen3-MoE using RoMA. Evaluations on diverse
benchmarks and extensive comparisons with baselines show the substantial
improvement brought by RoMA.

</details>
